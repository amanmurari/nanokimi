# nanoKimi: A Minimal, Efficient, and Scalable Implementation of the Kimi-K2 Architecture for Next-Generation Language Modeling

Recent advancements in large language models (LLMs) demand architectures that are efficient, scalable, and easy to fine-tune. The Kimi-K2 architecture introduces a set of innovations — notably the Muon optimizer, Mixture of Experts, and Latent Attention — that offer substantial improvements in convergence speed, model sparsity, and contextual reasoning.

## Prerequisites
- Python 3.10 or higher
- CUDA
- pytorch

## Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/amanmurari/nanokimi

   cd nanokimi
   ```

2. Install dependencies:
   ```bash
   pip install -r req.txt
   ```

## Dataset
go  `data/data.txt`
## Training

Run the main script:
```bash
python pipeline.py
```

## Usage

Run the main script:
```bash
python main.py
```





## Contributing

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

## License

This project is licensed under the MIT License - see the LICENSE file for details. 
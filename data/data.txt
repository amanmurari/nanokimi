Pretrained Transformers for Text Ranking:
BERT and Beyond
Jimmy Lin,
1 Rodrigo Nogueira,
1 and Andrew Yates
2
,
3
1 David R. Cheriton School of Computer Science, University of Waterloo 2 University of Amsterdam 3 Max Planck Institute for Informatics
Version 0.99 — August 20, 2021
Abstract
The goal of text ranking is to generate an ordered list of texts retrieved from a
corpus in response to a query for a particular task. Although the most common
formulation of text ranking is search, instances of the task can also be found
in many text processing applications. This survey provides an overview of text
ranking with neural network architectures known as transformers, of which BERT
is the best-known example. The combination of transformers and self-supervised
pretraining has been responsible for a paradigm shift in natural language processing
(NLP), information retrieval (IR), and beyond. For text ranking, transformer-based
models produce high quality results across many domains, tasks, and settings.
This survey provides a synthesis of existing work as a single point of entry for
practitioners who wish to deploy transformers for text ranking and researchers who
wish to pursue work in this area. We cover a wide range of techniques, grouped
into two categories: transformer models that perform reranking in multi-stage
architectures and dense retrieval techniques that perform ranking directly. Examples
in the first category include approaches based on relevance classification, evidence
aggregation from multiple segments of text, and document and query expansion.
The second category involves using transformers to learn dense representations of
texts, where ranking is formulated as comparisons between query and document
representations that take advantage of nearest neighbor search.
At a high level, there are two themes that pervade our survey: techniques for
handling long documents, beyond typical sentence-by-sentence processing in NLP,
and techniques for addressing the tradeoff between effectiveness (i.e., result quality)
and efficiency (e.g., query latency, model and index size). Much effort has been
devoted to developing ranking models that address the mismatch between document
lengths and the length limitations of existing transformers. The computational
costs of inference with transformers has led to alternatives and variants that aim
for different tradeoffs, both within multi-stage architectures as well as with dense
learned representations.
Although transformer architectures and pretraining techniques are recent innovations, many aspects of how they are applied to text ranking are relatively well
understood and represent mature techniques. However, there remain many open
research questions, and thus in addition to laying out the foundations of pretrained
transformers for text ranking, this survey also attempts to prognosticate where the
field is heading.

1 Introduction
The goal of text ranking is to generate an ordered list of texts retrieved from a corpus in response to a
query for a particular task. The most common formulation of text ranking is search, where the search
engine (also called the retrieval system) produces a ranked list of texts (web pages, scientific papers,
news articles, tweets, etc.) ordered by estimated relevance with respect to the user’s query. In this
context, relevant texts are those that are “about” the topic of the user’s request and address the user’s
information need. Information retrieval (IR) researchers call this the ad hoc retrieval problem.1
With keyword search, also called keyword querying (for example, on the web), the user typically
types a few query terms into a search box (for example, in a browser) and gets back results containing
representations of the ranked texts. These results are called ranked lists, hit lists, hits, “ten blue
links”,2 or search engine results pages (SERPs). The representations of the ranked texts typically
comprise the title, associated metadata, “snippets” extracted from the texts themselves (for example,
an extractive keyword-in-context summary where the user’s query terms are highlighted), as well
as links to the original sources. While there are plenty of examples of text ranking problems (see
Section 1.1), this particular scenario is ubiquitous and undoubtedly familiar to all readers.
This survey provides an overview of text ranking with a family of neural network models known as
transformers, of which BERT (Bidirectional Encoder Representations from Transformers) [Devlin
et al., 2019], an invention of Google, is the best-known example. These models have been responsible
for a paradigm shift in the fields of natural language processing (NLP) and information retrieval (IR),
and more broadly, human language technologies (HLT), a catch-all term that includes technologies
to process, analyze, and otherwise manipulate (human) language data. There are few endeavors
involving the automatic processing of natural language that remain untouched by BERT.3
In the
context of text ranking, BERT provides results that are undoubtedly superior in quality than what
came before. This is a robust and widely replicated empirical result, across many text ranking tasks,
domains, and problem formulations.
A casual skim through paper titles in recent proceedings from NLP and IR conferences will leave the
reader without a doubt as to the extent of the “BERT craze” and how much it has come to dominate the
current research landscape. However, the impact of BERT, and more generally, transformers, has not
been limited to academic research. In October 2019, a Google blog post4
confirmed that the company
had improved search “by applying BERT models to both ranking and featured snippets”. Ranking
refers to “ten blue links” and corresponds to most users’ understanding of web search; “feature
snippets” represent examples of question answering5
(see additional discussion in Section 1.1). Not to
be outdone, in November 2019, a Microsoft blog post6
reported that “starting from April of this year,
we used large transformer models to deliver the largest quality improvements to our Bing customers
in the past year”.
As a specific instance of transformer architectures, BERT has no doubt improved how users find
relevant information. Beyond search, other instances of the model have left their marks as well. For
example, transformers dominate approaches to machine translation, which is the automatic translation
of natural language text7
from one human language to another, for example, from English to French.
1There are many footnotes in this survey. Since nobody reads footnotes, we wanted to take one opportunity to
inform the reader here that we’ve hidden lots of interesting details in the footnotes. But this message is likely to
be ignored anyway.
2Here’s the first interesting tidbit: The phrase “ten blue links” is sometimes used to refer to web search and has a
fascinating history. Fernando Diaz helped us trace the origin of this phrase to a BBC article in 2004 [BBC,
2004], where Tony Macklin, director of product at Ask UK, was quoted saying “searching is going to be about
more than just 10 blue links”. Google agreed: in 2010, Jon Wiley, Senior User Experience Designer for Google,
said, “Google is no longer just ten blue links on a page, those days are long gone” [ReadWrite, 2010].
3And indeed, programming languages as well [Alon et al., 2020, Feng et al., 2020]!
4
https://www.blog.google/products/search/search-language-understanding-bert/
5
https://support.google.com/websearch/answer/9351707
6
https://azure.microsoft.com/en-us/blog/bing-delivers-its-largest-improvement-insearch-experience-using-azure-gpus/
7A machine translation system can be coupled with an automatic speech recognition system and a speech
synthesis system to perform speech-to-speech translation—like a primitive form of the universal translator from
Star Trek or (a less annoying version of) C-3PO from Star Wars!
4
Blog posts by both Facebook8
and Google9
tout the effectiveness of transformer-based architectures.
Of course, these are just the high-profile announcements. No doubt many organizations—from
startups to Fortune 500 companies, from those in the technology sector to those in financial services
and beyond—have already or are planning to deploy BERT (or one of its siblings or intellectual
decedents) in production.
Transformers were first presented in June 2017 [Vaswani et al., 2017] and BERT was unveiled in
October 2018.10 Although both are relatively recent inventions, we believe that there is a sufficient
body of research such that the broad contours of how to apply transformers effectively for text ranking
have begun to emerge, from high-level design choices to low-level implementation details. The “core”
aspects of how BERT is used—for example, as a relevance classifier—is relatively mature. Many of
the techniques we present in this survey have been applied in many domains, tasks, and settings, and
the improvements brought about by BERT (and related models) are usually substantial and robust. It
is our goal to provide a synthesis of existing work as a single point of entry for practitioners who
wish to gain a better understanding of how to apply BERT to text ranking problems and researchers
who wish to pursue further advances in this area.
Like nearly all scientific advances, BERT was not developed in a vacuum, but built on several
previous innovations, most notably the transformer architecture itself [Vaswani et al., 2017] and the
idea of self-supervised pretraining based on language modeling objectives, previously explored by
ULMFiT [Howard and Ruder, 2018] and ELMo (Embeddings from Language Models) [Peters et al.,
2018]. Both ideas initially came together in GPT (Generative Pretrained Transformer) [Radford et al.,
2018], and the additional innovation of bidirectional training culminated in BERT (see additional
discussions about the history of these developments in Section 3.1). While it is important to
recognize previous work, BERT is distinguished in bringing together many crucial ingredients
to yield tremendous leaps in effectiveness on a broad range of natural language processing tasks.
Typically, “training” BERT (and in general, pretrained models) to perform a downstream task involves
starting with a publicly available pretrained model (often called a “model checkpoint”) and then
further fine-tuning the model using task-specific labeled data. In general, the computational and
human effort involved in fine-tuning is far less than pretraining. The commendable decision by
Google to open-source BERT and to release pretrained models supported widespread replication
of the impressive results reported by the authors and additional applications to other tasks, settings,
and domains. The rapid proliferation of these BERT applications was in part due to the relatively
lightweight fine-tuning process. BERT supercharged subsequent innovations by providing a solid
foundation to build on.
The germinal model, in turn, spawned a stampede of other models differing to various extents in architecture, but nevertheless can be viewed as variations on its main themes. These include ERNIE [Sun
et al., 2019b], RoBERTa [Liu et al., 2019c], Megatron-LM [Shoeybi et al., 2019], XLNet [Yang
et al., 2019f], DistilBERT [Sanh et al., 2019], ALBERT [Lan et al., 2020], ELECTRA [Clark et al.,
2020b], Reformer [Kitaev et al., 2020], DeBERTa [He et al., 2020], Big Bird [Zaheer et al., 2020],
and many more. Additional pretrained sequence-to-sequence transformer models inspired by BERT
8
https://engineering.fb.com/ai-research/scaling-neural-machine-translation
9
https://ai.googleblog.com/2020/06/recent-advances-in-google-translate.html
10The nature of academic publishing today means that preprints are often available (e.g., on arXiv) several
months before the formal publication of the work in a peer-reviewed venue (which is increasingly becoming a
formality). For example, the BERT paper was first posted on arXiv in October 2018, but did not appear in a
peer-reviewed venue until June 2019, at NAACL 2019 (a top conference in NLP) . Throughout this survey,
we attribute innovations to their earliest known preprint publication dates, since that is the date when a work
becomes “public” and available for other researchers to examine, critique, and extend. For example, the earliest
use of BERT for text ranking was reported in January 2019 [Nogueira and Cho, 2019], a scant three months
after the appearance of the original BERT preprint and well before the peer-reviewed NAACL publication.
The rapid pace of progress in NLP, IR, and other areas of computer science today means that by the time an
innovation formally appears in a peer-reviewed venue, the work is often already “old news”, and in some
cases, as with BERT, the innovation had already become widely adopted. In general, we make an effort to cite
the peer-reviewed version of a publication unless there is some specific reason otherwise, e.g., to establish
precedence. At the risk of bloating this already somewhat convoluted footnote even more, there’s the additional
complication of a conference’s submission deadline. Clearly, if a paper got accepted at a conference, then the
work must have existed at the submission deadline, even if it did not appear on arXiv. So how do we take
this into account when establishing precedence? Here, we just throw up our hands and shrug; at this point,
“contemporaneous” would be a fair characterization.
5
include T5 [Raffel et al., 2020], UniLM [Dong et al., 2019], PEGASUS [Zhang et al., 2020c], and
BART [Lewis et al., 2020b].
Although a major focus of this survey is BERT, many of the same techniques we describe can
(and have been) applied to its descendants and relatives as well, and BERT is often incorporated as
part of a larger neural ranking model (as Section 3 discusses in detail). While BERT is no doubt
the “star of the show”, there are many exciting developments beyond BERT being explored right
now: the application of sequence-to-sequence transformers, transformer variants that yield more
efficient inference, ground-up redesigns of transformer architectures, and representation learning with
transformers—just to name a few (all of which we will cover). The diversity of research directions
being actively pursued by the research community explains our choice for the subtitle of this survey
(“BERT and Beyond”). While many aspects of the application of BERT and transformers to text
ranking can be considered “mature”, there remain gaps in our knowledge and open research questions
yet to be answered. Thus, in addition to synthesizing the current state of knowledge, we discuss
interesting unresolved issues and highlight where we think the field is going.
Let us begin!
1.1 Text Ranking Problems
While our survey opens with search (specifically, what information retrieval researchers call ad hoc
retrieval) as the motivating scenario due to the ubiquity of search engines, text ranking appears in
many other guises. Beyond typing keywords into a search box and getting back “ten blue links”,
examples of text ranking abound in scenarios where users desire access to relevant textual information,
in a broader sense.
Consider the following examples:
Question Answering (QA). Although there are many forms question answering, the capability that
most users have experience with today appears in search engines as so-called “infoboxes” or what
Google calls “featured snippets”11 that appear before (or sometimes to the right of) the main search
results. In the context of a voice-capable intelligent agent such as Siri or Alexa, answers to user
questions are directly synthesized using text-to-speech technology. The goal is for the system to
identify (or extract) a span of text that directly answers the user’s question, instead of returning a
list of documents that the user must then manually peruse. In “factoid” question answering, systems
primarily focus on questions that can be answered with short phrases or named entities such as dates,
locations, organizations, etc.
Although the history of question answering systems dates back to the 1960s [Simmons, 1965], modern
extractive approaches (i.e., that is, techniques focused on extracting spans of text from documents)
trace their roots to work that began in the late 1990s [Voorhees, 2001]. Most architectures that adopt
an extractive approach break the QA challenge into two steps: First, select passages of text from
a potentially large corpus that are likely to contain answers, and second, apply answer extraction
techniques to identify the answer spans. In the modern neural context, Chen et al. [2017a] called this
the retriever–reader framework. The first stage (i.e., the “retriever”) is responsible for tackling the text
ranking problem. Although question answering encompasses more than just extractive approaches or
a focus on factoid questions, in many cases methods for approaching these challenges still rely on
retrieving texts from a corpus as a component.
Community Question Answering (CQA). Users sometimes search for answers not by attempting
to find relevant information directly, but by locating another user who has asked the same or similar
question, for example, in a frequently-asked questions (FAQ) list or in an online forum such as Quora
or Stack Overflow. Answers to those questions usually address the user’s information need. This
mode of searching, which dates back to the late 1990s [Burke et al., 1997], is known as community
question answering (CQA) [Srba and Bielikova, 2016]. Although it differs from traditional keywordbased querying, CQA is nevertheless a text ranking problem. One standard approach formulates the
problem as estimating semantic similarity between two pieces of texts—more specifically, if two
natural language questions are paraphrases of each other. A candidate list of questions (for example,
based on keyword search) is sorted by the estimated degree of “paraphrase similarity” (for example,
the output of a machine-learned model) and the top-k results are returned to the user.
11https://blog.google/products/search/reintroduction-googles-featured-snippets/
6
Information Filtering. In search, queries are posed against a (mostly) static collection of texts.
Filtering considers the opposite scenario where a (mostly) static query is posed against a stream of
texts. Two examples of this mode of information seeking might be familiar to many readers: push
notifications that are sent to a user’s mobile device whenever some content of interest is published
(could be a news story or a social media post); and, in a scholarly context, email digests that are sent
to users whenever a paper that matches the user’s interest is published (a feature available in Google
Scholar today). Not surprisingly, information filtering has a long history, dating back to the 1960s,
when it was called “selective dissemination of information” (SDI); see Housman and Kaskela [1970]
for a survey of early systems. The most recent incarnation of this idea is “real-time summarization”
in the context of social media posts on Twitter, with several community-wide evaluations focused on
notification systems that inform users in real time about relevant content as it is being generated [Lin
et al., 2016]. Before that, document filtering was explored in the context of the TREC Filtering
Tracks, which ran from 1995 [Lewis, 1995] to 2002 [Robertson and Soboroff, 2002], and the general
research area of topic detection and tracking, also known as TDT [Allan, 2002]. The relationship
between search and filtering has been noted for decades: Belkin and Croft [1992] famously argued
that they represented “two sides of the same coin”. Models that attempt to capture relevance for ad
hoc retrieval can also be adapted for information filtering.
Text Recommendation. When a search system is displaying a search result, it might suggest other
texts that may be of interest to the user, for example, to assist in browsing [Smucker and Allan, 2006].
This is frequently encountered on news sites, where related articles of interest might offer background
knowledge or pointers to related news stories [Soboroff et al., 2018]. In the context of searching the
scientific literature, the system might suggest papers that are similar in content: An example of this
feature is implemented in the PubMed search engine, which provides access to the scientific literature
in the life sciences [Lin and Wilbur, 2007]. Citation recommendation [Ren et al., 2014, Bhagavatula
et al., 2018] is another good example of text recommendation in the scholarly context. All of these
challenges involve text ranking.
Text Ranking as Input to Downstream Modules. The output of text ranking may not be intended
for direct user consumption, but may rather be meant to feed downstream components: for example,
an information extraction module to identify key entities and relations [Gaizauskas and Robertson,
1997], a summarization module that attempts to synthesize information from multiple sources with
respect to an information need [Dang, 2005], a clustering module that organizes texts based on content
similarity [Vadrevu et al., 2011], or a browsing interface for exploration and discovery [Sadler, 2009].
Even in cases where a ranked list of results is not directly presented to the user, text ranking may still
form an important component technology in a larger system.
We can broadly characterize ad hoc retrieval, question answering, and the different tasks described
above as “information access”—a term we use to refer to these technologies collectively. Text ranking
is without a doubt an important component of information access.
However, beyond information access, examples of text ranking abound in natural language processing.
For example:
Semantic Similarity Comparisons. The question of whether two texts “mean the same thing” is a
fundamental problem in natural language processing and closely related to the question of whether
a text is relevant to a query. While there are some obvious differences, researchers have explored
similar approaches and have often even adopted the same models to tackle both problems. In the
context of learned dense representations for ranking, the connections between these two problems
have become even more intertwined, bringing the NLP and IR communities closer and further erasing
the boundaries between text ranking, question answering, paraphrase detection, and many related
problems. Since Section 5 explores these connections in detail, we will not further elaborate here.
Distant Supervision and Data Augmentation. Training data form a crucial ingredient in NLP
approaches based on supervised machine learning. All things being equal, the more data the better,12
and so there is a never-ending quest for practitioners and researchers to acquire more, more, and
more! Supervised learning requires training examples that have been annotated for the specific
12A well-known observation dating back at least decades; see, for example, Banko and Brill [2001].
7
task, typically by humans, which is a labor-intensive process. For example, to train a sentiment
classifier, we must somehow acquire a corpus of texts in which each instance has been labeled with
its sentiment (e.g., positive or negative). There are natural limits to the amount of data that can be
acquired via human annotation: in the sentiment analysis example, we can automatically harvest
various online sources that have “star ratings” associated with texts (e.g., reviews), but even these
labels are ultimately generated by humans. This is a form of crowdsourcing, and merely shifts the
source of the labeling effort, but does not change the fundamental need for human annotation.
Researchers have extensively explored many techniques to overcome the data bottleneck in supervised
machine learning. At a high level, distant supervision and data augmentation represent two successful
approaches, although in practice they are closely related. Distant supervision involves training models
using low-quality “weakly” labeled examples that are gathered using heuristics and other simple but
noisy techniques. One simple example is to assume that all emails mentioning Viagra are spam for
training a spam classifier; obviously, there are “legitimate” non-spam emails (called “ham”) that
use the term, but the heuristic may be a reasonable way to build an initial classifier [Cormack et al.,
2011]. We give this example because it is easy to convey, but the general idea of using heuristics to
automatically gather training examples to train a classifier in NLP dates back to Yarowsky [1995], in
the context of word sense disambiguation.13
Data augmentation refers to techniques that exploit a set of training examples to gather or create
additional training examples. For example, given a corpus of English sentences, we could translate
them automatically using a machine translation (MT) system, say, into French, and then translate
those sentences back into English (this is called back-translation).14 With a good MT system, the
resulting sentences are likely paraphrases of the original sentence, and using this technique we can
automatically increase the quantity and diversity of the training examples that a model is exposed to.
Text ranking lies at the heart of many distant supervision and data augmentation techniques for
natural language processing. We illustrate with relation extraction, which is the task of identifying
and extracting relationships in natural language text. For example, from the sentence “Albert Einstein
was born in Ulm, in the Kingdom of Württemberg in the German Empire, on 14 March 1879”, a
system could automatically extract the relation birthdate(Albert Einstein, 1879/03/14); these
are referred to as “tuples” or extracted facts. Relations usually draw from a relatively constrained
vocabulary (dozens at most), but can be domain specific, for example, indicating that a gene regulates
a protein (in the biomedical domain).
One simple technique for distant supervision is to search for specific patterns or “cue phrases” such as
“was born in” and take the tokens occurring to the left and to the right of the phrase as participating in
the relation (i.e., they form the tuple). These tuples, together with the source documents, can serve as
noisy training data. One simple technique for data augmentation is to take already known tuples, e.g.,
Albert Einstein and his birthdate, and search a corpus for sentences that contain those tokens (e.g., by
exact or approximate string matching). Furthermore, we can combine the two techniques iteratively:
search with a pattern, identify tuples, find texts with those tuples, and from those learn more patterns,
going around and around.15 Proposals along these lines date back to the late 1990s [Riloff, 1996, Brin,
1998, Agichtein and Gravano, 2000].16 Obviously, training data and extracted tuples gathered in this
manner are noisy, but studies have empirically shown that such approaches are cheap when used alone
and effective in combination with supervised techniques. See Smirnova and Cudré-Mauroux [2018]
13Note that the term “distant supervision” was coined in the early 2000s, so it would be easy to miss these early
papers by keyword search alone; Yarowsky calls his approach “unsupervised”.
14The “trick” of translating a sentence from one language into another and then back again is nearly old as
machine translation systems themselves. An apocryphal story from the 1960s goes that with an early English–
Russian MT system, the phrase “The spirit is willing, but the flesh is weak” translated into Russian and back
into English again became “The whisky is strong, but the meat is rotten” [Hutchins, 1995] (in some accounts,
whisky is replaced with vodka). The earliest example we could find of using this trick to generate synthetic
training data is Alshawi et al. [1997]. Bannard and Callison-Burch [2005] is often cited for using “pivot
languages” (the other language we translate into and back) as anchors for automatically extracting paraphrases
from word alignments.
15The general idea of training a machine learning model on its own output, called self-training, dates back to at
least the 1960s [Scudder, 1965].
16Although, once again, they did specifically use the modern terminology of distant supervision and data
augmentation.
8
for a survey of distant supervision techniques applied to relation extraction, and Snorkel [Ratner et al.,
2017] for a modern implementation of these ideas.
Wrapped inside these distant supervision and data augmentation techniques are usually variants
of text ranking problems, centered around the question of “is this a good training example?” For
example, given a collection of sentences that match a particular pattern, or when considering multiple
patterns, which ones are “good”? Answering this question requires ranking texts with respect to
the quality of the evidence, and many scoring techniques proposed in the above-cited papers share
similarities with the probabilistic framework for relevance [Robertson and Zaragoza, 2009].
An entirely different example comes from machine translation: In modern systems, such as those
built by Facebook and Google referenced in the introduction, translation models are learned from
a parallel corpus (also called bitext), comprised of pairs of sentences in two languages that are
translations of each other [Tiedemann, 2011]. Some parallel corpora can be found “naturally” as the
byproduct of an organization’s deliberate effort to disseminate information in multiple languages, for
example, proceedings of the Canadian Parliament in French and English [Brown et al., 1990], and
texts produced by the United Nations in many different languages. In modern data-driven approaches
to machine translation, these pairs serve as the input for training translation models.
Since there are limits to the amount of parallel corpora available, researchers have long explored
techniques that can exploit comparable data, or texts in different languages that are topically similar
(i.e., “talk about the same thing”) but are not necessarily translations of each other [Resnik and
Smith, 2003, Munteanu and Marcu, 2005, Smith et al., 2010]. Techniques that can take advantage of
comparable corpora expand the scope and volume of data that can be thrown at the machine translation
problem, since the restriction for semantic equivalence is relaxed. Furthermore, researchers have
developed techniques for mining comparable corpora automatically at scale [Uszkoreit et al., 2010,
Ture and Lin, 2012]. These can be viewed as a cross-lingual text ranking problem [Ture et al., 2011]
where the task is to estimate the semantic similarity between sentences in different languages, i.e., if
they are mutual translations.
Selecting from Competing Hypotheses. Many natural language tasks that involve selecting from
competing hypotheses can be formulated as text ranking problems, albeit on shorter segments of text,
possibly integrated with additional features. The larger the hypothesis space, the more crucial text
ranking becomes as a method to first reduce the number of candidates under consideration.
There are instances of text ranking problems in “core” NLP tasks that at first glance have nothing
to do with text ranking. Consider the semantic role labeling problem [Gildea and Jurafsky, 2001,
Palmer et al., 2010], where the system’s task is to populate “slots” in a conceptual “frame” with
entities that fill the “semantic roles” defined by the frame. For example, the sentence “John sold his
violin to Mary” depicts a COMMERCIALTRANSACTION frame, where “John” is the SELLER, Mary
is the BUYER, and the violin is the GOODS transacted. One strategy for semantic role labeling is
to identify all entities in the sentence, and for each slot, rank the entities by the likelihood that each
plays that role. For example, is “John”, “Mary”, or “the violin” most likely to be the SELLER? This
ranking formulation can be augmented by attempts to perform joint inference to resolve cases where
the same entity is identified as the most likely filler of more than one slot; for example, resolving
the case where a model (independently) identifies “John” erroneously as both the most likely buyer
and the most likely seller (which is semantically incoherent). Although the candidate entities are
short natural language phrases, they can be augmented with a number of features, in which case the
problem begins to share characteristics with ranking in a vector space model. While the number of
entities to be ranked is not usually very big, what’s important is the amount of evidence (i.e., different
features) used to estimate the probability that an entity fills a role, which isn’t very different from
relevance classification (see Section 3.2).
Another problem that lends itself naturally to a ranking formulation is entity linking, where the task
is to resolve an entity with respect to an external knowledge source such as Wikidata [Vrandeciˇ c´
and Krötzsch, 2014]. For example, in a passage of text that mentions Adam Smith, which exact
person is being referenced? Is it the famous 18th century Scottish economist and moral philosopher,
or one of the lessor-known individuals that share the same name? An entity linking system “links”
the instance of the entity mention (in a piece of text) to a unique id in the knowledge source: the
Scottish economist has the unique id Q9381,17 while the other individuals have different ids. Entity
17https://www.wikidata.org/wiki/Q9381
9
linking can be formulated as a ranking problem, where candidates from the knowledge source are
ranked in terms of their likelihood of being the actual referent of a particular mention [Shen et al.,
2015]. This is an instance of text ranking because these candidates are usually associated with textual
descriptions—for example, a short biography of the individual—which forms crucial evidence. Here,
the “query” is the entity to be linked, represented not only by its surface form (i.e., the mention
string), but also the context in which the entity appears. For example, if the text discusses the Wealth
of Nations, it’s likely referencing the famous Scot.
Yet another example of text ranking in a natural language task that involves selecting from competing
hypotheses is the problem of fact verification [Thorne et al., 2018], for example, to combat the
spread of misinformation online. Verifying the veracity of a claim requires fetching supporting
evidence from a possibly large corpus and assessing the credibility of those sources. The first step
of gathering possible supporting evidence is a text ranking problem. Here, the hypothesis space
is quite large (passages from an arbitrarily large corpus), and thus text ranking plays a critical
role. In the same vein, for systems that engage in or assist in human dialogue, such as intelligent
agents or “chatbots”, one common approach to generating responses (beyond question answering and
information access discussed above) is to retrieve possible responses from a corpus (and then perhaps
modifying them) [Henderson et al., 2017, Dinan et al., 2019, Roller et al., 2020]. Here, the task is to
rank possible responses with respect to their appropriateness.
The point of this discussion is that while search is perhaps the most visible instance of the text
ranking problem, there are manifestations everywhere—not only in information retrieval but also
natural language processing. This exposition also explains our rationale in intentionally using the
term “text ranking” throughout this survey, as opposed to the more popular term “document ranking”.
In many applications, the “atomic unit” of text to be ranked is not a document, but rather a sentence,
a paragraph, or even a tweet; see Section 2.1 and Section 2.9 for more discussions.
To better appreciate how BERT and transformers have revolutionized text ranking, it is first necessary
to understand “how we got here”. We turn our attention to this next in a brief exposition of important
developments in information retrieval over the past three quarters of a century.
1.2 A Brief History
The vision of exploiting computing machines for information access is nearly as old as the invention
of computing machines themselves, long before computer science emerged as a coherent discipline.
The earliest motivation for developing information access technologies was to cope with the explosion
of scientific publications in the years immediately following World War II.18 Vannevar Bush’s oftencited essay in The Atlantic in July 1945, titled “As We May Think” [Bush, 1945], described a
hypothetical machine called the “memex” that performs associative indexing to connect arbitrary
items of content stored on microfilm, as a way to capture insights and to augment the memory of
scientists. The article describes technologies that we might recognize today as capturing aspects of
personal computers, hypertext, the Semantic Web, and online encyclopedias.19 A clearer description
of what we might more easily identify today as a search engine was provided by Holmstrom [1948],
although discussed in terms of punch-card technology!
1.2.1 The Beginnings of Text Ranking
Although the need for machines to improve information access was identified as early as the mid1940s, interestingly, the conception of text ranking was still a decade away. Libraries, of course, have
existed for millennia, and the earliest formulations of search were dominated by the automation of
what human librarians had been doing for centuries: matching based on human-extracted descriptors
18Scholars have been complaining about there being more information than can be consumed since shortly after
the invention of the printing press. “Is there anywhere on earth exempt from these swarms of new books?
Even if, taken out one at a time, they offered something worth knowing, the very mass of them would be an
impediment to learning from satiety if nothing else”, the philosopher Erasmus complained in the 16th century.
19Bush talks about naming “trails”, which are associations between content items. Today, we might call these
subject–verb–object triples. Viewed from this perspective, the memex is essentially a graph store! Furthermore,
he envisioned sharing these annotations, such that individuals can build on each others’ insights. Quite
remarkably, the article mentions text-to-speech technology and speech recognition, and even speculates on
brain–computer interfaces!
10
of content stored on physical punch-card representations of the texts to be searched (books, scientific
articles, etc.). These descriptors (also known as “index terms”) were usually assigned by human
subject matter experts (or at least trained human indexers) and typically drawn from thesauri, “subject
headings”, or “controlled vocabularies”—that is, a predefined vocabulary. This process was known
as “indexing”—the original sense of the activity involved humans, and is quite foreign to modern
notions that imply automated processing—or is sometimes referred to as “abstracting”.20 Issuing
queries to search content required librarians (or at least trained individuals) to translate the searcher’s
information need into these same descriptors; search occurs by matching these descriptors in a
boolean fashion (hence, no ranking).
As a (radical at the time) departure from this human-indexing approach, Luhn [1958] proposed
considering “statistical information derived from word frequency and distribution . . . to compute
a relative measure of significance”, thus leading to “auto-abstracts”. He described a precursor of
what we would recognize today as tf–idf weighting (that is, term weights based on term frequency
and inverse document frequency). However, Luhn neither implemented nor evaluated any of the
techniques he proposed.
A clearer articulation of text ranking was presented by Maron and Kuhns [1960], who characterized
the information retrieval problem (although they didn’t use these words) as receiving requests from
the user and “to provide as an output an ordered list of those documents which most probably satisfy
the information needs of the user”. They proposed that index terms (“tags”) be weighted according to
the probability that a user desiring information contained in a particular document would use that
term in a query. Today, we might call this query likelihood [Ponte and Croft, 1998]. The paper also
described the idea of a “relevance number” for each document, “which is a measure of the probability
that the document will satisfy the given request”. Today, we would call these retrieval scores. Beyond
laying out these foundational concepts, Maron and Kuhns described experiments to test their ideas.
We might take for granted today the idea that automatically extracted terms from a document can
serve as descriptors or index terms for describing the contents of those documents, but this was an
important conceptual leap in the development of information retrieval.
Throughout the 1960s and 1970s, researchers and practitioners debated the merits of “automatic
content analysis” (see, for example, Salton [1968]) vs. “traditional” human-based indexing. Salton
[1972] described a notable evaluation comparing the SMART retrieval system based on the vector
space model with human-based indexing in the context of MEDLARS (Medical Literature Analysis
and Retrieval System), which was a computerized version of the Index Medicus, a comprehensive print
bibliographic index of medical articles that the U.S. National Library of Medicine (NLM) had been
publishing since 1879. SMART was shown to produce higher-quality results, and Salton concluded
“that no technical justification exists for maintaining controlled, manual indexing in operational
retrieval environments”. This thread of research has had significant impact, as MEDLARS evolved
into MEDLINE (short for MEDLARS onLINE). In the internet era, MEDLINE became publicly
accessible via the PubMed search engine, which today remains the authoritative bibliographic
database for the life sciences literature.
The mode of information access we take for granted today—based on ranking automatically constructed representations of documents and queries—gradually gained acceptance, although the history
of information retrieval showed this to be an uphill battle. Writing about the early history of information retrieval, Harman [2019] goes as far as to call these “indexing wars”: the battle between
human-derived and automatically-generated index terms. This is somewhat reminiscent of the rulebased vs. statistical NLP “wars” that raged beginning in the late 1980s and into the 1990s, and goes
to show how foundational shifts in thinking are often initially met with resistance. Thomas Kuhn
would surely find both these two cases to be great examples supporting his views on the structure of
scientific revolutions [Kuhn, 1962].
Bringing all the major ideas together, Salton et al. [1975] is frequently cited for the proposal of the
vector space model, in which documents and queries are both represented as “bags of words” using
sparse vectors according to some term weighting scheme (tf–idf in this case), where document–query
similarity is computed in terms of cosine similarity (or, more generally, inner products). However, this
development did not happen all at once, but represented innovations that gradually accumulated over
20Thus, an indexer is a human who performs indexing, not unlike the earliest uses of computers to refer to
humans who performed computations by hand.
11
the two preceding decades. For additional details about early historical developments in information
retrieval, we refer the reader to Harman [2019].
1.2.2 The Challenges of Exact Match
For the purposes of establishing a clear contrast with neural network models, the most salient feature
of all approaches up to this point in history is their reliance exclusively on what we would call today
exact term matching—that is, terms from documents and terms from queries had to match exactly to
contribute to a relevance score. Since systems typically perform stemming—that is, the elimination of
suffixes (in English)—matching occurs after terms have been normalized to some extent (for example,
stemming would ensure that “dog” matches “dogs”).
Nevertheless, with techniques based on exact term matching, a scoring function between a query q
and a document d could be written as:
S(q, d) = X
t∈q∩d
f(t) (1)
where f is some function of a term and its associated statistics, the three most important of which are
term frequency (how many times a term occurs in a document), document frequency (the number
of documents that contain at least once instance of the term), and document length (the length of
the document that the term occurs in). It is from the first two statistics that we derive the ubiquitous
scoring function tf–idf, which stands for term frequency, inverse document frequency. In the vector
space model, cosine similarity has a length normalization component that implicitly handles issues
related to document length.
A major thread of research in the 1980s and into the 1990s was the exploration of different term
weighting schemes in the vector space model [Salton and Buckley, 1988a], based on easily computed
term-based statistics such as those described above. One of the most successful of these methods,
Okapi BM25 [Robertson et al., 1994, Crestani et al., 1999, Robertson and Zaragoza, 2009], still
provides the starting point of many text ranking approaches today, both in academic research as well
as commercial systems.21
Given the importance of BM25, the exact scoring function is worth repeating to illustrate what a
ranking model based on exact term matching looks like. The relevance score of a document d with
respect to a query q is defined as:
BM25(q, d) = X
t∈q∩d
log N − df(t) + 0.5
df(t) + 0.5
·
tf(t, d) · (k1 + 1)
tf(t, d) + k1 ·

1 − b + b ·
ld
L
 (2)
As BM25 is based on exact term matching, the score is derived from a sum of contributions from
each query term that appears in the document. In more detail:
• The first component of the summation (the log term) is the idf (inverse document frequency)
component: N is the total number of documents in the corpus, and df(t) is the number of
documents that contain term t (i.e., its document frequency).
• In the second component of the summation, tf(t, d) represents the number of times term t
appears in document d (i.e., its term frequency). The expression in the denominator involving b
is responsible for performing length normalization, since collections usually have documents
that differ in length: ld is the length of document d while L is the average document length
across all documents in the collection.
Finally, k1 and b are free parameters. Note that the original formulation by Robertson et al. [1994]
includes additional scoring components with parameters k2 and k3, but they are rarely used and
are often omitted from modern implementations. In addition to the original scoring function described above, there are several variants that have been discussed in the literature, including the one
implemented in the popular open-source Lucene search library; see Section 2.8 for more details.
21Strictly speaking, BM25 derives from the probabilistic retrieval framework, but its ultimate realization is
a weighting scheme based on a probabilistic interpretation of how terms contribute to document relevance.
Retrieval is formulated in terms of inner products on sparse bag-of-words vectors, which is operationally
identical to the vector space model; see, for example, Crestani et al. [1999].
1
While term weighting schemes can model term importance (sometimes called “salience”) based on
statistical properties of the texts, exact match techniques are fundamentally powerless in cases where
terms in queries and documents don’t match at all. This happens quite frequently, when searchers
use different terms to describe their information needs than what authors of the relevant documents
used. One way of thinking about search is that an information seeker is trying to guess the terms
(i.e., posed as the query) that authors of relevant texts would have used when they wrote the text
(see additional discussion in Section 2.2). We’re looking for a “tragic love story” but Shakespeare
wrote about “star-crossed lovers”. To provide a less poetic, but more practical example, what we
call “information filtering” today was known as “selective dissemination of information (SDI)” in the
1960s (see Section 1.1). Imagine the difficulty we would face trying to conduct a thorough literature
review without knowing the relationship between these key terms. Yet another example, also from
Section 1.1: early implementations of distant supervision did not use the term “distant supervision”.
In both these cases, it would be easy to (falsely) conclude that no prior work exists beyond recent
papers that use contemporary terminology!
These are just two examples of the “vocabulary mismatch problem” [Furnas et al., 1987], which
represents a fundamental challenge in information retrieval. There are three general approaches to
tackling this challenge: enrich query representations to better match document representations, enrich
document representations to better match query representations, and attempts to go beyond exact
term matching:
• Enriching query representations. One obvious approach to bridge the gap between query and
document terms is to enrich query representations with query expansion techniques [Carpineto
and Romano, 2012]. In relevance feedback, the representation of the user’s query is augmented
with terms derived from documents that are known to be relevant (for example, documents
that have been presented to the user and that the user has indicated is relevant): two popular
formulations are based on the vector space model [Rocchio, 1971] and the probabilistic retrieval
framework [Robertson and Spark Jones, 1976]. In pseudo-relevance feedback [Croft and Harper,
1979], also called “blind” relevance feedback, top-ranking documents are simply assumed to
be relevant, thus providing a source for additional query terms. Query expansion techniques,
however, do not need to involve relevance feedback: examples include Xu and Croft [2000], who
introduced global techniques that identify word relations from the entire collection as possible
expansion terms (this occurs in a corpus preprocessing step, independent of any queries),
and Voorhees [1994], who experimented with query expansion using lexical-semantic relations
from WordNet [Miller, 1995]. A useful distinction when discussing query expansion techniques
is the dichotomy between pre-retrieval techniques, where expansion terms can be computed
without examining any documents from the collection, and post-retrieval techniques, which are
based on analyses of documents from an initial retrieval. Section 4 discusses query expansion
techniques in the context of transformers.
• Enriching document representations. Another obvious approach to bridge the gap between
query and document terms is to enrich document representations. This strategy works well for
noisy transcriptions of speech [Singhal and Pereira, 1999] and short texts such as tweets [Efron
et al., 2012]. Although not as popular as query expansion techniques, researchers nevertheless
explored this approach throughout the 1980s and 1990s [Salton and Buckley, 1988b, Voorhees
and Hou, 1993]. The origins of document expansion trace even earlier to Kwok [1975], who
took advantage of bibliographic metadata for expansion, and finally, Brauen et al. [1968], who
used previously issued user queries to modify the vector representation of a relevant document.
Historically, document expansion techniques have not been as popular as query expansion
techniques, but we have recently witnessed a resurgence of interest in document expansion in
the context of transformers, which we cover in Section 4.
• Beyond exact term matching. Researchers have investigated models that attempt to address the
vocabulary mismatch problem without explicitly enriching query or document representations.
A notable attempt is the statistical translation approach of Berger and Lafferty [1999], who
modeled retrieval as the translation of a document into a query in a noisy channel model.
Their approach learns translation probabilities between query and document terms, but these
nevertheless represent mappings between terms in the vocabulary space of the documents. Other
examples of attempts to go beyond exact match include techniques that attempt to perform
matching in some semantic space induced from data, for example, based on latent semantic
analysis [Deerwester et al., 1990] or latent Dirichlet allocation [Wei and Croft, 2006]. However,
13
neither approach has gained widespread adoption as serious competition to keyword-based
querying. Nevertheless, there are clear connections between this thread of work and learned
dense representations for ranking, which we detail in Section 5.
At a high level, retrieval models up until this time contrast with “soft” or semantic matching enabled
by continuous representations in neural networks, where query terms do not have to match document
terms exactly in order to contribute to relevance. Semantic matching refers to techniques and attempts
to address a variety of linguistic phenomena, including synonymy, paraphrase, term variation, and
different expressions of similar intents, specifically in the context of information access [Li and Xu,
2014]. Following this usage, “relevance matching” is often used to describe the correspondences
between queries and texts that account for a text being relevant to a query (see Section 2.2). Thus,
relevance matching is generally understood to comprise both exact match and semantic match
components. However, there is another major phase in the development of ranking techniques before
we get to semantic matching and how neural networks accomplish it.
1.2.3 The Rise of Learning to Rank
BM25 and other term weighting schemes are typically characterized as unsupervised, although
they contain free parameters (e.g., k1 and b) that can be tuned given training data. The next major
development in text ranking, beginning in the late 1980s, is the application of supervised machinelearning techniques to learn ranking models: early examples include Fuhr [1989], Wong et al. [1993],
and Gey [1994]. This approach, known as “learning to rank”, makes extensive use of hand-crafted,
manually-engineered features, based primarily on statistical properties of terms contained in the texts
as well as intrinsic properties of the texts:
• Statistical properties of terms include functions of term frequencies, document frequencies,
document lengths, etc., the same components that appear in a scoring function such as BM25.
In fact, BM25 scores between the query and various document fields (as well as scores based
on other exact match scoring functions) are typically included as features in a learning-to-rank
setup. Often, features incorporate proximity constraints, such as the frequency of a term pair
co-occurring within five positions. Proximity constraints can be localized to a specific field in
the text, for example, the co-occurrence of terms in the title of a web page or in anchor texts.
• Intrinsic properties of texts, ranging from very simple statistics, such as the amount of JavaScript
code on a web page or the ratio between HTML tags and content, to more sophisticated measures,
such as the editorial quality or spam score as determined by a classifier. In the web context,
features of the hyperlink graph, such as the count of inbound and outgoing links and PageRank
scores, are common as well.
A real-world search engine can have hundreds of features (or even more).22 For systems with a
sufficiently larger user base, features based on user behavior—for example, how many times users
issued a particular query or clicked on a particular link (in different contexts)—are very valuable
relevance signals and are thoroughly integrated into learning-to-rank methods.
This rise of learning to rank was driven largely by the growth in importance of search engines as
indispensable tools for navigating the web, as earlier approaches based on human-curated directories
(e.g., Yahoo!) became quickly untenable with the explosion of available content. Log data capturing
behavioral traces of users (e.g., queries and clicks) could be used to improve machine-learned ranking
models. A better search experience led to user growth, which yielded even more log data and
behavior-based features to further improve ranking quality—thus closing a self-reinforcing virtuous
cycle (what Jeff Bezos calls “the flywheel”). Noteworthy innovations that played an important role
in enabling this growth included the development and refinement of techniques for interpreting
noisy user clicks and converting them into training examples that could be fed into machine-learning
algorithms [Joachims, 2002, Radlinski and Joachims, 2005].
As we lack the space for a detailed treatment of learning to rank, we refer interested readers to two
surveys [Liu, 2009, Li, 2011] and focus here on highlights that are most directly relevant for text
ranking with transformers. At a high-level, learning-to-rank methods can be divided into three basic
types, based on the general form of their loss functions:
22https://googleblog.blogspot.com/2008/03/why-data-matters.html
14
• A pointwise approach only considers losses on individual documents, transforming the ranking
problem into classification or regression.
• A pairwise approach considers losses on pairs of documents, and thus focuses on preferences,
that is, the property wherein A is more relevant than (or preferred over) B.
• A listwise approach considers losses on entire lists of documents, for example, directly optimizing a ranking metric such as normalized discounted cumulative gain (see Section 2.5 for a
discussion of metrics).
Since this basic classification focuses on the form of the loss function, it can also be used to describe
ranking techniques with transformers.
Learning to rank reached its zenith in the early 2010s, on the eve of the deep learning revolution,
with the development of models based on tree ensembles [Burges, 2010].23 At that time, there was an
emerging consensus that tree-based models, and gradient-boosted decision trees [Ganjisaffar et al.,
2011] in particular, represented the most effective solution to learning to rank. By that time, tree
ensembles had been deployed to solve a wide range of problems; one notable success story is their
important role in winning the Netflix Prize, a high-profile competition that aimed to improve the
quality of movie recommendations.24
Note that “learning to rank” should not be understood as being synonymous with “supervised
machine-learning approaches to ranking”. Rather, learning to rank refers to techniques that emerged
during a specific period in the history of information retrieval. Transformers for text ranking can be
characterized as a supervised machine-learning approach, but would not generally be regarded as a
learning-to-rank method. In particular, there is one key characteristic that distinguishes learning to
rank from the deep learning approaches that came after. What’s important is not the specific supervised
machine-learning model: in fact, neural networks have been used since the early 1990s [Wong et al.,
1993], and RankNet [Burges et al., 2005], one of the most influential and well-known learning-to-rank
models, adopted a basic feedforward neural architecture. Instead, learning to rank is characterized
by its use of numerous sparse, usually hand-crafted features. However, to muddle the waters a bit,
the phrase “deep learning to rank” has recently emerged in the discourse to describe deep learning
approaches that also incorporate sparse features [Pasumarthi et al., 2019].
1.2.4 The Advent of Deep Learning
For text ranking, after learning to rank came deep learning, following initial excitement in the computer vision and then the natural language processing communities. In the context of information
retrieval, deep learning approaches were exciting for two reasons: First, continuous vector representations freed text retrieval from the bounds of exact term matching (as already mentioned above,
we’ll see exactly how below). Second, neural networks promised to obviate the need for laboriously
hand-crafted features (addressing a major difficulty with building systems using learning to rank).
In the space of deep learning approaches to text ranking, it makes sense to further distinguish “preBERT” models from BERT-based models (and more generally, transformer models). After all, the
“BERT revolution” is the motivation for this survey to begin with. In the Deep Learning Track at
TREC 2019,25 the first large-scale evaluation of retrieval techniques following the introduction of
BERT, its impact, and more generally, the impact of pretrained neural language models, was clear
from the effectiveness of the submissions [Craswell et al., 2020]. Analysis of the results showed
that, taken as a family of techniques, BERT-based models achieved substantially higher effectiveness
than pre-BERT models, across implementations by different teams. The organizers of the evaluation
recognized this as a meaningful distinction that separated two different “eras” in the development of
deep neural approaches to text ranking.
This section provides a high-level overview of pre-BERT models. Needless to say, we do not have
sufficient space to thoroughly detail roughly half a dozen years of model progression, and therefore
refer the reader to existing surveys devoted to the topic [Onal et al., 2018, Mitra and Craswell, 2019a,
Xu et al., 2020]. Note that here we focus specifically on models designed for document ranking and
23Although a specific thread of work in the learning-to-rank tradition, called “counterfactual learning to
rank” [Agarwal et al., 2019] remains active today.
24https://www.netflixprize.com/
25See Section 2.6 for an overview of what TREC is.
15
E1
q1
E2
q2
E3
q3
F1
d1
F2
d2
Fm
… dm
s
(a) a generic representation-based neural ranking model
E
q 1
1
E
q 2
2
E
q 3
3
F1
d1
F2
d2
Fm
… dm
s
…
…
…
(b) a generic interaction-based neural ranking model
Figure 1: Two classes of pre-BERT neural ranking models. Representation-based models (left) learn
vector representations of queries and documents that are compared using simple metrics such as
cosine similarity to compute relevance scores. Interaction-based models (right) explicitly model term
interactions in a similarity matrix that is further processed to compute relevance scores.
leave aside another vast body of literature, mostly from the NLP community, on the closely related
problem of computing the semantic similarity between two sentences (for example, to detect if two
sentences are paraphrases of each other). Models for these tasks share many architectural similarities,
and indeed there has been cross-fertilization between the NLP and IR communities in this regard.
However, there is one major difference: inputs to a model for computing semantic similarity are
symmetric, i.e., Rel(s1, s2) = Rel(s2, s1), whereas queries and documents are obviously different
and cannot be swapped as model inputs. The practical effect is that architectures for computing
semantic similarity are usually symmetric, but may not be for modeling query–document relevance.
Interestingly, recent developments in learned dense representations for ranking are erasing the
distinction between these two threads of work, as we will see in Section 5.
Pre-BERT neural ranking models are generally classified into two classes: representation-based
models and interaction-based models. Their high-level architectures are illustrated in Figure 1.
Representation-based models (left) focus on independently learning dense vector representations of
queries and documents that can be compared to compute relevance via a simple metric such as cosine
similarity or inner products. Interaction-based models (right) compare the representations of terms in
the query with terms in a document to produce a similarity matrix that captures term interactions.
This matrix then undergoes further analysis to arrive at a relevance score. In both cases, models can
incorporate many different neural components (e.g., convolutional neural networks and recurrent
neural networks) to extract relevance signals.
Both representation-based and interaction-based models are usually trained end-to-end with relevance
judgments (see Section 2.4), using only the embeddings of query and document terms as input.
Notably, additional features (hand-crafted or otherwise) are typically not used, which is a major
departure from learning to rank. Below, we provide more details, with illustrative examples:
Representation-based models. This class of models (Figure 1, left) learns vector representations of
queries and documents that can be compared at ranking time to compute query–document relevance
scores. Since the query and document “arms” of the network are independent, this approach allows
document representations to be computed offline. One of the earliest neural ranking models in the
deep learning era, the Deep Structure Semantic Model (DSSM) [Huang et al., 2013] constructs
character n-grams from an input (i.e., query or document) and passes the results to a series of
fully-connected layers to produce a vector representation. At retrieval time, query and document
representations can then be compared with cosine similarity. Shen et al. [2014] improved upon DSSM
by using CNNs to capture context. Rather than learning text representations as part of the model, the
Dual Embedding Space Model (DESM) [Mitra et al., 2016, Nalisnick et al., 2016] represents texts
16
using pre-trained word2vec embeddings [Le and Mikolov, 2014] and computes relevance scores by
aggregating cosine similarities across all query–document term pairs. Language models based on
word embeddings [Ganguly et al., 2015] can also be categorized as representation-based models.
Interestingly, we are witnessing a resurgence of interest in representation-based approaches, albeit
using transformer architectures. The entirety of Section 5 is devoted to this topic.
Interaction-based models. This class of models (Figure 1, right) explicitly captures “interactions”
between terms from the query and terms from the document. These interactions are typically
operationalized using a similarity matrix with rows corresponding to query terms and columns
corresponding to document terms. Each entry mi,j in the matrix is usually populated with the cosine
similarity between the embedding of the i-th query term and the embedding of the j-th document
term.26 At a high level, these models operate in two steps: feature extraction and relevance scoring.
• In the feature extraction step, the model extracts relevance signals from the similarity matrix. By
exploiting continuous vector representations of terms, these models can potentially overcome
the vocabulary mismatch problem. Unigram models like DRMM [Guo et al., 2016] and
KNRM [Xiong et al., 2017] aggregate the similarities between each query term and each
document term, which can be viewed as histograms. DRMM creates explicit histograms,
while KNRM uses Gaussian kernels to create differentiable “soft histograms” that allow the
embeddings to be learned during training. Position-aware models like MatchPyramid [Pang
et al., 2016], PACRR [Hui et al., 2017], Co-PACRR [Hui et al., 2018], and ConvKNRM [Dai
et al., 2018] use additional architectural components to identify matches between sequences of
query and document terms.27
• In the relevance scoring step, features extracted from above are combined and processed to
produce a query–document relevance score. This step often consists of applying pooling operations, concatenating extracted features together, and then passing the resulting representation to
a feedforward network that computes the relevance score.
While interaction-based models generally follow this high-level approach, many variants have been
proposed that incorporate additional components. For example, POSIT-DRMM [McDonald et al.,
2018] uses an LSTM to contextualize static embeddings before comparing them. EDRM [Liu et al.,
2018b] extends ConvKNRM by incorporating entity embeddings. HiNT [Fan et al., 2018b] splits
the document into passages, creates a similarity matrix for each, and then combines passage-level
signals to predict a single document-level relevance score. The NPRF [Li et al., 2018] framework
incorporates feedback documents by using a neural ranking method like KNRM to predict their
similarity to a target document being ranked.
In general, studies have shown pre-BERT interaction-based models to be more effective but slower
than pre-BERT representation-based models. The latter reduces text ranking to simple similarity
comparisons between query vectors and precomputed document vectors, which can be performed
quickly on large corpora using nearest neighbor search techniques (see Section 5.2). In contrast,
interaction-based models are typically deployed as rerankers over a candidate set of results retrieved
by keyword search. Interaction-based models also preserve the ability to explicitly capture exact
match signals, which remain important in relevance matching (see discussion in Section 3.2.3).
Hybrid models. Finally, representation-based and interaction-based approaches are not mutually
exclusive. A well-known hybrid is the DUET model [Mitra et al., 2017, Mitra and Craswell,
2019b], which augments a representation-learning component with an interaction-based component
responsible for identifying exact term matches.
26Although other distance metrics can be used as well, for example, see He and Lin [2016], Pang et al. [2016].
27One might argue that, with this class of models, we have simply replaced feature engineering (from learning
to rank) with network engineering, since in some cases there are pretty clear analogies between features in
learning to rank and the relevance signals that different neural architectural components are designed to identify.
While this is not an unfair criticism, it can be argued that different network components more compactly
capture the intuitions of what makes a document relevant to a query. For example, bigram relations can be
compactly expressed as convolutions, whereas in learning to rank distinct bigram features would need to be
enumerated explicitly.
17
MS MARCO Passage
Development Test
Method MRR@10 MRR@10
BM25 (Microsoft Baseline) 0.167 0.165
IRNet (Deep CNN/IR Hybrid Network) January 2nd, 2019 0.278 0.281
BERT [Nogueira and Cho, 2019] January 7th, 2019 0.365 0.359
Table 1: The state of the leaderboard for the MS MARCO passage ranking task in January 2019,
showing the introduction of BERT and the best model (IRNet) just prior to it. This large gain in
effectiveness kicked off the “BERT revolution” in text ranking.
There has undeniably been significant research activity throughout the 2010s exploring a wide
range of neural architectures for document ranking, but how far has the field concretely advanced,
particularly since approaches based on deep learning require large amounts of training data? Lin
[2018] posed the provocative question, asking if neural ranking models were actually better than
“traditional” keyword-matching techniques in the absence of vast quantities of training data available
from behavior logs (i.e., queries and clickthroughs). This is an important question because academic
researchers have faced a perennial challenge in obtaining access to such data, which are available to
only researchers in industry (with rare exceptions). To what extent do neural ranking models “work”
on the limited amounts of training data that are publicly available?
Yang et al. [2019b] answered this question by comparing several prominent interaction-based and
representation-based neural ranking models to a well-engineered implementation of bag-of-words
search with well-tuned query expansion on the dataset from the TREC 2004 Robust Track [Voorhees,
2004]. Under this limited data condition, most of the neural ranking methods were unable to beat
the keyword search baseline. Yates et al. [2020] replicated the same finding for an expanded set of
neural ranking methods with completely different implementations, thus increasing the veracity of the
original findings. While many of the papers cited above report significant improvements when trained
on large, proprietary datasets (many of which include behavioral signals), the results are difficult to
validate and the benefits of the proposed methods are not broadly accessible to the community.
With BERT, though, everything changed, nearly overnight.
1.2.5 The Arrival of BERT
BERT [Devlin et al., 2019] arrived on the scene in October 2018. The first application of BERT to
text ranking was reported by Nogueira and Cho [2019] in January 2019 on the MS MARCO passage
ranking test collection [Bajaj et al., 2018], where the task is to rank passages (paragraph-length
extracts) from web pages with respect to users’ natural language queries, taken from Bing query logs
(see more details in Section 2.7). The relevant portion of the leaderboard at the time is presented in
Table 1, showing Microsoft’s BM25 baseline and the effectiveness of IRNet, the best system right
before the introduction of BERT (see Section 2.5 for the exact definition of the metric). Within less
than a week, effectiveness shot up by around eight points28 absolute, which corresponds to a ∼30%
relative gain.
Such a big jump in effectiveness that can be directly attributed to an individual model is rarely
seen in either academia or industry, which led to immediate excitement in the community. The
simplicity of the model led to rapid widespread replication of the results. Within a few weeks, at least
two other teams had confirmed the effectiveness of BERT for passage ranking, and exploration of
model variants built on the original insights of Nogueira and Cho [2019] had already begun.29 The
skepticism expressed by Lin [2018] was retracted in short order [Lin, 2019], as many researchers
quickly demonstrated that with pretrained transformer models, large amounts of relevance judgments
were not necessary to build effective models for text ranking. The availability of the MS MARCO
passage ranking test collection further mitigated data availability issues. The combination of these
factors meant that, nearly overnight, exploration at the forefront of neural models for text ranking
was within reach of academic research groups, and was no longer limited to researchers in industry
who had the luxury of access to query logs.
28A change of 0.01 is often referred to as a “point”; see Section 2.5.
29https://twitter.com/MSMarcoAI/status/1095035433375821824
18
Nogueira and Cho [2019] kicked off the “BERT revolution” for text ranking, and the research
community quickly set forth to build on their results—addressing limitations and expanding the work
in various ways. Looking at the leaderboard today, the dominance of BERT remains evident, just by
looking at the names of the submissions.
The rest, as they say, is history. The remainder of this survey is about that history.
1.3 Roadmap, Assumptions, and Omissions
The target audience for this survey is a first-year graduate student or perhaps an advanced undergraduate. As this is not intended to be a general introduction to natural language processing or
information retrieval, we assume that the reader has basic background in both. For example, we
discuss sequence-to-sequence formulations of text processing problems (to take an example from
NLP) and query evaluation with inverted indexes (to take an example from IR) assuming that the
reader has already encountered these concepts before.
Furthermore, we expect that the reader is already familiar with neural networks and deep learning,
particularly pre-BERT models (for example, CNNs and RNNs). Although we do provide an overview
of BERT and transformer architectures, that material is not designed to be tutorial in nature, but
merely intended to provide the setup of how to apply transformers to text ranking problems.
This survey is organized as follows:
• Setting the Stage (Section 2). We begin with a more precise characterization of the problem
we are tackling in the specific context of information retrieval. This requires an overview of
modern evaluation methodology, involving discussions about information needs, notions of
relevance, ranking metrics, and the construction of test collections.
• Multi-Stage Architectures for Reranking (Section 3). The most straightforward application
of transformers to text ranking is as reranking models to improve the output quality of candidates
generated by keyword search. This section details various ways this basic idea can be realized
in the context of multi-stage ranking architectures.
• Refining Query and Document Representations (Section 4). One fundamental challenge in
ranking is overcoming the vocabulary mismatch problem, where users’ queries and documents
use different words to describe the same concepts. This section describes expansion techniques
for query and document representations that bring them into closer “alignment”.
• Learned Dense Representations for Ranking (Section 5). Text ranking can be cast as a
representation learning problem in terms of efficient comparisons between dense vectors that
capture the “meaning” of documents and queries. This section covers different architectures as
well as training methods for accomplishing this.
• Future Directions and Conclusions (Section 6). We have only begun to scratch the surface
in applications of transformers to text ranking. This survey concludes with discussions of
interesting open problems and our attempts to prognosticate where the field is heading.
Given limits in both time and space, it is impossible to achieve comprehensive coverage, even in a
narrowly circumscribed topic, both due to the speed at which research is progressing and the wealth
of connections to related topics.
This survey focuses on what might be characterized as “core” text ranking. Noteworthy intentional
omissions include other aspects of information access such as question answering, summarization,
and recommendation, despite their close relationship to the material we cover. Adequate treatments
of each of these topics would occupy an equally lengthy survey! Our focus on “core” text ranking
means that we do not elaborate on how ranked results might be used to directly supply answers (as
in typical formulations of question answering), how multiple results might be synthesized (as in
summarization), and how systems might suggest related texts based on more than just content (as in
recommendations).
19
2 Setting the Stage
This section begins by more formally characterizing the text ranking problem, explicitly enumerating
our assumptions about characteristics of the input and output, and more precisely circumscribing
the scope of this survey. In this exposition, we will adopt the perspective of information access,
focusing specifically on the problem of ranking texts with respect to their relevance to a particular
query—what we have characterized as the “core” text ranking problem (and what information retrieval
researchers would refer to as ad hoc retrieval). However, most of our definitions and discussions carry
straightforwardly to other ranking tasks, such as the diverse applications discussed in Section 1.1.
From the evaluation perspective, this survey focuses on what is commonly known as the Cranfield
paradigm, an approach to systems-oriented evaluation of information retrieval (IR) systems based
on a series of experiments by Cyril Cleverdon and his colleagues in the 1960s. For the interested
reader, Harman [2011] provides an overview of the early history of IR evaluation. Also known as
“batch evaluations”, the Cranfield paradigm has come to dominate the IR research landscape over
the last half a century. Nevertheless, there are other evaluation paradigms worth noting: interactive
evaluations place humans “in the loop” and are necessary to understand the important role of user
behavior in information seeking [Kelly, 2009]. Online services with substantial numbers of users can
engage in experimentation using an approach known as A/B testing [Kohavi et al., 2007]. Despite
our focus on the Cranfield paradigm, primarily due to its accessibility to the intended audience
of our survey, evaluations from multiple perspectives are necessary to accurately characterize the
effectiveness of a particular technique.
2.1 Texts
The formulation of text ranking assumes the existence of a collection of texts or a corpus C = {di}
comprised of mostly unstructured natural language text. We say “mostly unstructured” because texts
are, of course, typically broken into paragraphs, with section headings and other discourse markers—
these can be considered a form of “structure”. This stands in contrast to, for example, tabular data or
semi-structured logs (e.g., in JSON), which are comprised of text as well. We specifically consider
such types of textual data out of scope in this survey.
Our collection C can be arbitrarily large (but finite)—in the case of the web, countless billions
of pages. This means that issues related to computational efficiency, for example the latency and
throughput of text ranking, are important considerations, especially in production systems. We mostly
set aside issues related to multilinguality and focus on English, although there are straightforward
extensions to some of the material discussed in this survey to other languages that serve as reasonable
baselines and starting points for multilingual IR.30
It is further assumed that the corpus is provided “ahead of time” to the system, prior to the arrival of
queries, and that a “reasonable” amount of offline processing may be conducted on the corpus. This
constraint implies that the corpus is mostly static, in the sense that additions, deletions, or modifications to texts happen in batch or at a pace that is slow compared to the amount of preprocessing
required by the system for proper operation.31 This assumption becomes important in the context of
document expansion techniques we discuss in Section 4.
Texts can vary in length, ranging from sentences (e.g., searching for related questions in a community
question answering application) to entire books, although the organization of the source texts, how
they are processed, and the final granularity of ranking can be independent. To illustrate: in a
collection of full-text scientific articles, we might choose to only search the article titles and abstracts.
That is, the ranking model only considers selected portions of the articles; experiments along these
lines date back to at least the 1960s [Salton and Lesk, 1968]. An alternative might be to segment
full-text articles into paragraphs and consider each paragraph as the unit of retrieval, i.e., the system
30With respect to multilinguality, IR researchers have explored two distinct problem formulations: mono-lingual
retrieval in languages other than English (where one major challenge is mitigating the paucity of training data),
and cross-lingual retrieval, where queries are in a different language than the corpus (for example, searching
Telugu documents with English queries). A worthy treatment of multilinguality in IR would occupy a separate
survey, and thus we consider these issues mostly out of scope. See additional discussions in Section 6.2.
31For example, daily updates to the corpus would likely meet this characterization, but not streams of tweets that
require real-time processing. See, for example, Busch et al. [2012] for an overview techniques for real-time
indexing and search.
20
returns a list of paragraphs as results. Yet another alternative might be to rank articles by aggregating
evidence across paragraphs—that is, the system treats paragraphs as the atomic unit of analysis,
but for the goal of producing a ranking of the articles those paragraphs are drawn from. Zhang
et al. [2020a] provided a recent example of these different schemes in the context of the biomedical
literature. Approaches to segmenting documents into passages for ranking purposes and integrating
evidence from multiple document granularities—commonly referred to as passage retrieval—was
an active area of research in the 1990s [Salton et al., 1993, Hearst and Plaunt, 1993, Callan, 1994,
Wilkinson, 1994, Kaszkiel and Zobel, 1997, Clarke et al., 2000]. Note that for certain types of text,
the “right level” of granularity may not be immediately obvious: For example, when searching email,
should the system results be comprised of individual emails or email threads? What about when
searching (potentially long) podcasts based on their textual transcripts? What about chat logs or
transcriptions of phone calls?
In this survey, we have little to say about the internal structure of texts other than applying the most
generic treatments (e.g., segmenting by paragraphs or overlapping windows). Specific techniques are
often domain-specific (e.g., reconstructing and segmenting email threads) and thus orthogonal to our
focus. However, the issue of text length is an important consideration in applications of transformer
architectures to text ranking (see Section 3.3). There are two related issues: transformers are typically
pretrained with input sequences up to a certain maximum length, making it difficult to meaningfully
encode longer sequences, and feeding long texts into transformers results in excessive memory usage
and inference latency. These limitations have necessitated the development of techniques to handle
ranking long texts. In fact, many of these techniques draw from work in passage retrieval referenced
above, dating back nearly three decades (see Section 3.3.2).
2.2 Information Needs
Having sufficiently characterized the corpus, we now turn our attention to queries. In the web context,
short keyword queries that a user types into a search box are merely the external manifestations of
an information need, which is the motivation that compelled the user to seek information in the first
place. Belkin [1980] calls this an “anomalous state of knowledge” (ASK), where searchers perceive
gaps in their cognitive states with respect to some task or problem; see also Belkin et al. [1982a,b].
Strictly speaking, queries are not synonymous with information needs [Taylor, 1962]. The same
information need might give rise to different manifestations with different systems: for example, a
few keywords are typed into the search box of a web search engine, but a fluent, well-formed natural
language question is spoken to a voice assistant.32
In this survey, we are not concerned with the cognitive processes underlying information seeking, and
focus on the workings of text ranking models only after they have received a tangible signal to process.
Thus, we somewhat abuse the terminology and refer to the query as “the thing” that the ranking
is computed with respect to (i.e., the input to the ranking model), and use it as a metonym for the
underlying information need. In other words, although the query is not the same as the information
need, we only care about what is fed to the ranking model (for the purposes of this survey), in which
case this distinction is not particularly important.33 We only consider queries that are expressed in
text, although in principle queries can be presented in different modalities, for example, speech34 or
images, or even “query by humming” [Ghias et al., 1995].
Nevertheless, to enable automated processing, information needs must be encoded in some representation. In the Text Retrieval Conferences (TRECs), an influential series of community evaluations in
information retrieval (see Section 2.6), information needs are operationalized as “topics”.35 Figure 2
provides an example from the TREC 2004 Robust Track.
A TREC topic for ad hoc retrieval is comprised of three fields:
32In the latter case, researchers might refer to these as voice queries, but it is clear that spoken utterances are
very different from typed queries, even if the underlying information needs are the same.
33Note, however, that this distinction may be important from the perspective of relevance judgments; see more
discussion in Section 2.3.
34Spoken queries can be transcribed into text with the aid of automatic speech recognition (ASR) systems.
35Even within TREC, topic formats have evolved over time, but the structure we describe here has been stable
since TREC-7 in 1998 [Voorhees and Harman, 1998].
21
<top>
<num> Number: 336
<title> Black Bear Attacks
<desc> Description:
A relevant document would discuss the frequency of vicious black bear
attacks worldwide and the possible causes for this savage behavior.
<narr> Narrative:
It has been reported that food or cosmetics sometimes attract hungry black
bears, causing them to viciously attack humans. Relevant documents would
include the aforementioned causes as well as speculation preferably from the
scientific community as to other possible causes of vicious attacks by black
bears. A relevant document would also detail steps taken or new methods
devised by wildlife officials to control and/or modify the savageness of the
black bear.
</top>
Figure 2: An example ad hoc retrieval “topic” (i.e., representation of an information need) from the
TREC 2004 Robust Track, comprised of “title”, “description”, and “narrative” fields.
• the “title”, which consists of a few keywords that describe the information need, close to a query
that a user would type into a search engine;
• the “description”, typically a well-formed natural language sentence that describes the desired
information; and,
• the “narrative”, a paragraph of prose that details the characteristics of the desired information,
particularly nuances that are not articulated in the title or description.
In most information retrieval evaluations, the title serves as the query that is fed to the system to
generate a ranked list of results (that are then evaluated). Some papers explicitly state “title queries”
or something to that effect, but many papers omit this detail, in which case it is usually safe to assume
that the topic titles were used as queries.
Although in actuality the narrative is a more faithful description of the information need, i.e., what
the user really wants, in most cases feeding the narrative into a ranking model leads to poor results
because the narrative often contains terms that are not important to the topic. These extraneous
terms serve as distractors to a ranking model based on exact term matches, since such a model will
try to match all query terms.36 Although results vary by domain and the specific set of topics used
for evaluation, one common finding is that either the title or the title and description concatenated
together yields the best results with bag-of-words queries; see, for example, Walker et al. [1997].
However, the differences in effectiveness between the two conditions are usually small. Nevertheless,
the key takeaway here is that the expression of the information need that is fed to a ranking model
often has a substantive effect on retrieval effectiveness. We will see that this is particularly the case
for BERT (see Section 3.3.2).
Having more precisely described the inputs, we can now formally define the text ranking problem:
Given an information need expressed as a query q, the text ranking task is to return
a ranked list of k texts {d1, d2 . . . dk} from an arbitrarily large but finite collection
of texts C = {di} that maximizes a metric of interest, for example, nDCG, AP, etc.
Descriptions of a few common metrics are presented in Section 2.5, but at a high level they all aim to
quantify the “goodness” of the results with respect to the information need. The ranking task is also
called top-k retrieval (or ranking), where k is the length of the ranked list (also known as the ranking
or retrieval depth).
The “thing” that performs the ranking is referred to using different terms in the literature: {ranking, retrieval, scoring} × {function, model, method, technique . . . }, or even just “the system”
36Prior to the advent of neural networks, researchers have attempted to extract “key terms” or “key phrases” from
so-called “verbose” queries, e.g., Bendersky and Croft [2008], though these usually refer to sentence-length
descriptions of information needs as opposed to paragraph-length narratives.
22
when discussed in an end-to-end context. In this survey, we tend to use the term “ranking model”,
but consider all these variations roughly interchangeable. Typically, the ranked texts are associated with scores, and thus the output of a ranking model can be more explicitly characterized as
{(d1, s1),(d2, s2). . .(dk, sk)} with the constraint that s1 > s2 > . . . sk.
37
A distinction worth introducing here: ranking usually refers to the task of constructing a ranked
list of texts selected from the corpus C. As we will see in Section 3.2, it is impractical to apply
transformer-based models to directly rank all texts in a (potentially large) corpus to produce the top k.
Instead, models are often used to rerank a candidate list of documents, typically produced by keyword
search. More formally, in reranking, the model takes as input a list of texts R = {d1, d2 . . . dk}
and produces another list of texts R0 = {d
0
1
, d0
2
. . . d0
k
}, where R0
is a permutation of R. Ranking
becomes conceptually equivalent to reranking if we feed a reranker the entire corpus, but in practice
they involve very different techniques: Section 3 and Section 4 primarily focus on reranking with
transformer-based models, while Section 5 covers nearest neighbor search techniques for directly
ranking dense representations generated by transformer-based models. Nevertheless, in this survey
we adopt the expository convention of referring to both as ranking unless the distinction is important.
Similarly, we refer to ranking models even though a particular model may, in fact, be performing
reranking. We believe this way of writing improves clarity by eliminating a distinction that is usually
clear from context.
Finally, as information retrieval has a rich history dating back well over half a century, the parlance
can be confusing and inconsistent, especially in cases where concepts overlap with neighboring
sub-disciplines of computer science such as natural language processing or data mining. An example
here is the usage of “retrieval” and “ranking” in an interchangeable fashion. These issues are for the
most part not critical to the material presented in this survey, but we devote Section 2.9 to untangling
terminological nuances.
2.3 Relevance
There is one final concept necessary to connect the query, as an expression of the information need,
to the “goodness” of the ranked texts according to some metric: Ultimately, the foundation of all
ranking metrics rests on the notion of relevance,
38 which is a relation between a text and a particular
information need. A text is said to be relevant if it addresses the information need, otherwise it is not
relevant. However, this binary treatment of relevance is a simplification, as it is more accurate, for
example, to characterize relevance using ordinal scales in multiple dimensions [Spink and Greisdorf,
2001]. Discussions and debates about the nature of relevance are almost as old as the quest for
building automated search systems itself (see Section 1.2), since relevance figures into discussions of
what such systems should return and how to evaluate the quality of their outputs. Countless pages have
been written about relevance, from different perspectives ranging from operational considerations
(i.e., for designing search systems) to purely cognitive and psychological studies (i.e., how humans
assimilate and use information acquired from search systems). We refer the reader to Saracevic
[2017] for a survey that compiles accumulated wisdom on the topic of relevance spanning many
decades [Saracevic, 1975].
While seemingly intuitive, relevance is surprisingly difficult to precisely define. Furthermore, the
information science literature discusses many types of relevance; for the purposes of measuring
search quality, information retrieval researchers are generally concerned with topical relevance, or
the “aboutness” of the document—does the topic or subject of the text match the information need?
There are other possible considerations as well: for example, cognitive relevance, e.g., whether the
text is understandable by the user, or situational relevance, e.g., whether the text is useful for solving
the problem at hand.
To illustrate these nuances: A text might be topically relevant, but is written for experts whereas the
searcher desires an accessible introduction; thus, it may not be relevant from the cognitive perspective.
A text might be topically relevant, but the user is searching for information to aid in making a specific
decision—for example, whether to send a child to public or private school—and while the text
provides helpful background information, it offers no actionable advice. In this case, we might say
37A minor complication is that ranking models might produce score ties, which need to be resolved at evaluation
time since many metrics assume monotonically increasing ranks; see Section 2.5 for more details.
38“Relevancy” is sometimes used, often by industry practitioners. However, information retrieval researchers
nearly always use the term “relevance” in the academic literature.
23
that the document is topically relevant but not useful, i.e., from the perspective of situational relevance.
Although it has been well understood for decades that relevance is a complex phenomenon, there
remains a wide gap between studies that examine these nuances and the design of search systems and
ranking models, as it is not clear how such insights can be operationalized.
More to the task at hand: in terms of developing ranking models, the most important lesson from
many decades of information retrieval research is that relevance is in the eye of the beholder, that it is
a user-specific judgment about a text that involves complex cognitive processes. To put more simply:
for my information need, I am the ultimate arbiter of what’s relevant or not; nobody else’s opinion
counts or matters. Thus, relevance judgments represent a specific person’s assessment of what’s
relevant or not—this person is called the assessor (or sometimes the annotator). In short, all relevance
judgments are opinions, and thus are subjective. Relevance is not a “truth” (in a platonic sense) or an
“inherent property” of a piece of text (with respect to an information need) that the assessor attempts
to “unlock”. Put differently, unlike facts and reality, everyone can have different notions of relevance,
and they are all “correct”.
In this way, relevance differs quite a bit from human annotations in NLP applications, where
(arguably), there is, for example, the true part-of-speech tag of a word or dependency relation
between two words. Trained annotators can agree on a word’s part of speech nearly all the time, and
disagreements are interpreted as the result of a failure to properly define the subject of annotation
(i.e., what a part of speech is). It would be odd to speak of an annotator’s opinion of a word’s part of
speech, but that is exactly what relevance is: an assessor’s opinion concerning the relation between a
text and an information need.
With this understanding, it shouldn’t be a surprise then that assessor agreement on relevance judgments
is quite low: 60% overlap is a commonly cited figure [Voorhees, 2000], but the range of values
reported in the literature vary quite a bit (from around 30% to greater than 70%), depending on
the study design, the information needs, and the exact agreement metric; see [Harman, 2011] for a
discussion of this issue across studies spanning many decades. The important takeaway message is
that assessor agreement is far lower than values an NLP researcher would be comfortable with for a
human annotation task (κ > 0.9 is sometimes used as a reference point for what “good” agreement
means). The reaction from an NLP researcher would be, “we need better annotation guidelines”.
This, however, is fundamentally not possible, as we explain below.
Why is agreement so low among relevance judgments provided by different assessors? First, it is
important to understand the setup of such experiments. Ultimately, all information needs arise from
a single individual. In TREC, a human assessor develops the topic, which represents a best effort
articulation of the information need relatively early in the information seeking process. Topics are
formulated after some initial exploratory searches, but before in-depth perusal of texts from the
corpus. The topics are then released to teams participating in the evaluation, and the same individual
who created the topic then assesses system outputs (see Section 2.6 for more details).
Thus, if we ask another assessor to produce an independent set of relevance judgments (for example,
in the same way we might ask multiple annotators to assign part-of-speech tags to a corpus in an NLP
setting in order to compute inter-annotator agreement), such a task is based on a particular external
representation of that information need (e.g., a TREC topic, as in Figure 2).39 Thus, the second
individual is judging relevance with respect to an interpretation of that representation. Remember,
the actual characteristics of the desired information is a cognitive state that lies in the user’s head,
i.e., Belkin’s anomalous state of knowledge. Furthermore, in some cases, the topic statements aren’t
even faithful representations of the true information need to begin with: details may be missing and
inconsistencies may be present in the representations themselves. The paradox of relevance is that if
a user were able to fully and exhaustively articulate the parameters of relevance, there may likely be
no need to search in the first place—for the user would already know the information desired.
We can illustrate with a concrete example based on the TREC topic shown in Figure 2 about “black
bears attacks”: consider, would documents about brown (grizzly) bears be relevant?40 It could be
the case that the user is actually interested in attacks by bears (in general), and just happens to have
referenced black bears as a starting point. It could also be the case that the user specifically wants
39As far as we know, assessors cannot Vulcan mind meld with each other.
40In TREC “lore”, this was a serious debate that was had “back in the day”. The other memorable debate along
similar lines involved Trump and the Taj Mahal in the context of question answering.
24
only attacks by black bears, perhaps to contrast with the behavior of brown bears. Or, it could be
the case that the user isn’t familiar with the distinction, started off by referencing black bears, and
only during the process of reading initial results is a decision made about different types of bears.
All three scenarios are plausible based on the topic statement, and it can be seen now how different
interpretations might give rise to very different judgments.
Beyond these fundamental issues, which center around representational deficiencies of cognitive
states, there are issues related to human performance. Humans forget how they interpreted a previously
encountered text and may judge two similar texts inconsistently. There may be learning effects that
carry across multiple texts: for example, one text uses terminology that the assessor does not recognize
as being relevant until a second text is encountered (later) that explains the terminology. In this case,
the presentation order of the texts matters, and the assessor may or may not reexamine previous texts
to adjust the judgments. There are also more mundane factors: Assessors may get tired and misread
the material presented. Sometimes, they just make mistakes (e.g., clicked on the wrong button in an
assessment interface). All of these factors further contribute to low agreement.
One obvious question that arises from this discussion is: With such low inter-annotator agreement,
how are information retrieval researchers able to reliably evaluate systems at all? Given the critical
role that evaluation methodology plays in any empirical discipline, it should come as no surprise that
researchers have examined this issue in detail. In studies where we have multiple sets of relevance
judgments (i.e., from different assessors), it is easy to verify that the score of a system does indeed
vary (often, quite a bit) depending on which set of relevance judgments the system is evaluated with
(i.e., whose opinion of relevance). However, the ranking of a group of systems is usually stable with
respect to assessor variations [Voorhees, 2000].41 How stable? Exact values depend on the setting,
but measured in terms of Kendall’s τ , a standard rank correlation metric, values consistently above
0.9 are observed. That is, if system A is better than system B, then the score of system A will likely
be higher than the score of system B, regardless of the relevance judgments used for evaluation.42
This is a widely replicated and robust finding, and these conclusions have been shown to hold across
many different retrieval settings [Sormunen, 2002, Trotman and Jenkinson, 2007, Bailey et al., 2008,
Wang et al., 2015].
This means that while the absolute value of an evaluation metric must be interpreted cautiously,
comparisons between systems are generally reliable given a well-constructed test collection; see
more discussions in Section 2.6. The inability to quantify system effectiveness in absolute terms is
not a limitation outside of the ability to make marketing claims.43 As most research is focused on
the effectiveness of a particular proposed innovation, the desired comparison is typically between a
ranking model with and without that innovation, for which a reusable test collection can serve as an
evaluation instrument.
2.4 Relevance Judgments
Formally, relevance judgments, also called qrels, comprise a set of (q, d, r) triples, where the relevance
judgment r is a (human-provided) annotation on (q, d) pairs. Relevance judgments are also called
relevance labels or human judgments. Practically speaking, they are contained in text files that can be
downloaded as part of a test collection and can be treated like “ground truth”.44 In Section 2.6, we
describe a common way in which test collections are created via community evaluations, but for now
it suffices to view them as the product of (potentially large-scale) human annotation efforts.
In the simplest case, r is a binary variable—either document d is relevant to query q, or it is not
relevant. A three-way scale of not relevant, relevant, and highly-relevant is one common alternative,
41Note that while studies of assessor agreement predated this paper by several decades at least, for example, Lesk
and Salton [1968], the work of Voorhees is generally acknowledged as establishing these findings in the context
of modern test collections.
42Conflated with this high-level summary is the effect size, i.e., the “true” difference between the effectiveness
of systems, or an inferred estimate thereof. With small effect sizes, system A vs. system B comparisons are
less likely to be consistent across different assessors. Not surprisingly, Voorhees [2000] studied this as well;
see Wang et al. [2015] for a more recent examination in a different context.
43Occasionally on the web, one stumbles upon a statement like “our search engine achieves 90% accuracy”
without references to the corpus, information needs, or users. Such marketing slogans are utterly meaningless.
44However, IR researchers tend to avoid the term “ground truth” because relevance judgments are opinions, as
we discussed in Section 2.2.
25
and in web search, a five-point scale is often used—perfect, excellent, good, fair, and bad—which even
has an acronym: PEGFB.45 Non-binary relevance judgments are called graded relevance judgments:
“graded” is used in the sense of “grade”, defined as “a position in a scale of ranks or qualities” (from
the Merriam–Webster Dictionary).
Relevance judgments serve two purposes: they can be used to train ranking models in a supervised
setting and they can also be used to evaluate ranking models. To a modern researcher or practitioner
of applied machine learning, this distinction might seem odd, since these are just the roles of the
training, development, and test split of a dataset, but historically, information retrieval test collections
have not been large enough to meaningfully train ranking models (with the exception of simple
parameter tuning). However, with the release of the MS MARCO datasets, which we introduced in
Section 1.2.5 and will further discuss in Section 2.7, the community has gained public access to a
sufficiently large collection of relevance judgments for training models in a supervised setting. Thus,
throughout this survey, we use the terms relevance judgments, test collections, and training data
roughly interchangeably.
Researchers describe datasets for supervised learning of ranking models in different ways, but they are
equivalent. It makes sense to explicitly discuss some of these variations to reduce possible confusion:
Our view of relevance judgments as (q, d, r) triples, where r is a relevance label on query–document
pairs, is perhaps the most general formulation. However, documents may in fact refer to paragraphs,
passages, or some other unit of retrieval (see discussion in Section 2.9). Most often, d refers to
the unique id of a text from the corpus, but in some cases (for example, some question answering
datasets), the “document” may be just a span of text, without any direct association to the contents of
a corpus.
When the relevance judgments are binary, i.e., r is either relevant or non-relevant, researchers often
refer to the training data as comprising (query, relevant document) pairs. In some papers, the training
data are described as (query, relevant document, non-relevant document) triples, but this is merely
a different organization of (q, d, r) triples. It is important to note that non-relevant documents are
often qualitatively different from relevant documents. Relevant documents are nearly always judged
by a human assessor as being so. Non-relevant documents, however, may either come from explicit
human judgments or they may be heuristically constructed. For example, in the MS MARCO passage
ranking test collection, non-relevant documents are sampled from BM25 results not otherwise marked
as relevant (see Section 2.7 for details). Here, we have a divergence in data preparation for training
versus evaluation: heuristically sampling non-relevant documents is a common technique when
training a model. However, such sampling is almost never used during evaluation. Thus, there arises
the distinction between documents that have been explicitly judged as non-relevant and “unjudged”
documents, which we discuss in the context of ranking metrics below.
2.5 Ranking Metrics
Ranking metrics quantify the quality of a ranking of texts and are computed from relevance judgments
(qrels), described in the previous section. The ranked lists produced by a system (using a particular
approach) for a set of queries (in TREC, topics) is called a “run”, or sometimes a “submission”,
in that files containing these results represent the artifacts submitted for evaluation, for example,
in TREC evaluations (more below). The qrels and the run file are fed into an evaluation program
such as trec_eval, the most commonly used program by information retrieval researchers, which
automatically computes a litany of metrics. These metrics define the hill to climb in the quest for
effectiveness improvements.
Below, we describe a number of common metrics that are used throughout this survey. To be
consistent with the literature, we largely follow the notation and convention of Mitra and Craswell
[2019a]. We rewrite a ranked list R = {(di
, si)}
l
i=1 of length l as {(i, di)}
l
i=1, retaining only the
rank i induced by the score si’s. Many metrics are computed at a particular cutoff (or have variants
that do so), which means that the ranked list R is truncated to a particular length k, {(di
, si)}
k
i=1,
where k ≤ l: this is notated as Metric@k. The primary difference between l and k is that the system
decides l (i.e., how many results to return), whereas k is a property of the evaluation metric, typically
set by the organizers of an evaluation or the authors of a paper. Sometimes, l and k are left unspecified,
in which case it is usually the case that l = k = 1000. In most TREC evaluations, runs contain up
45Yes, there are those who actually try to pronounce this jumble of letters.
26
to 1000 results per topic, and the metrics evaluate the entirety of the ranked lists (unless an explicit
cutoff is specified).
From a ranked list R, we can compute the following metrics:
Precision is defined as the fraction of documents in ranked list R that are relevant, or:
Precision(R, q) =
P
(i,d)∈R rel(q, d)
|R|
, (3)
where rel(q, d) indicates whether document d is relevant to query q, assuming binary relevance.
Graded relevance judgments are binarized with some relevance threshold, e.g., in a three-grade
scale, we might set rel(q, d) = 1 for “relevant” and “highly relevant” judgments. Often, precision is
evaluated at a cutoff k, notated as Precision@k or abbreviated as P@k. If the cutoff is defined in
terms of the number of relevant documents for a particular topic (i.e., a topic-specific cutoff), the
metric is known as R-precision.
Precision has the advantage that it is easy to interpret: of the top k results, what fraction are relevant?46
There are two main downsides: First, precision does not take into account graded relevance judgments,
and for example, cannot separate “relevant” from “highly relevant” results since the distinction is
erased in rel(q, d). Second, precision does not take into account rank positions (beyond the cutoff
k). For example, consider P@10: relevant documents appearing at ranks one and two (with no other
relevant documents) would receive a precision of 0.2; P@10 would be exactly the same if those
two relevant documents appeared at ranks nine and ten. Yet, clearly, the first ranked list would be
preferred by a user.
Recall is defined as the fraction of relevant documents (in the entire collection C) for q that are
retrieved in ranked list R, or:
Recall(R, q) =
P
(i,d)∈R rel(q, d)
P
d∈C rel(q, d)
, (4)
where rel(q, d) indicates whether document d is relevant to query q, assuming binary relevance.
Graded relevance judgments are binarized in the same manner as precision.
Mirroring precision, recall is often evaluated at a cutoff k, notated as Recall@k or abbreviated R@k.
This metric has the same advantages and disadvantages as precision: it is easy to interpret, but does
not take into account relevance grades or the rank positions in which relevant documents appear.47
Reciprocal rank (RR) is defined as:
RR(R, q) = 1
ranki
, (5)
where ranki
is the smallest rank number of a relevant document. That is, if a relevant document
appears in the first position, reciprocal rank = 1, 1/2 if it appears in the second position, 1/3 if it
appears in the third position, etc. If a relevant document does not appear in the top k, then that query
receives a score of zero. Like precision and recall, RR is computed with respect to binary judgments.
Although RR has an intuitive interpretation, it only captures the appearance of the first relevant result.
For question answering or tasks in which the user may be satisfied with a single answer, this may be
an appropriate metric, but reciprocal rank is usually a poor choice for ad hoc retrieval because users
46There is a corner case here if l < k: for example, what is P@10 for a ranked list that only has five results? One
possibility is to always use k in the denominator, in which case the maximum possible score is 0.5; this has
the downside of averaging per-topic scores that have different ranges when summarizing effectiveness across
a set of topics. The alternative is to use l as the denominator. Unfortunately, treatment is inconsistent in the
literature.
47Note that since the denominator in the recall equation is the total number of relevant documents, the symmetric
situation of what happens when l < k does not exist as it does with precision. However, a different issue arises
when k is smaller than the total number of relevant documents, in which case perfect recall is not possible.
Therefore, it is inadvisable to set k to a value smaller than the smallest total number of relevant documents
for a topic across all topics in a test collection. While in most formulations, k is fixed for all topics in a test
collection, there exist variant metrics (though less commonly used) where k varies per topic, for example, as a
function of the number of (known) relevant documents for that topic.
27
usually desire more than one relevant document. As with precision and recall, reciprocal rank can be
computed at a particular rank cutoff, denoted with the same @k convention.
Average Precision (AP) is defined as:
AP(R, q) =
P
(i,d)∈R Precision@i(R, q) · rel(q, d)
P
d∈C rel(q, d)
, (6)
where all notation used have already been defined. The intuitive way to understand average precision
is that it is the average of precision scores at cutoffs corresponding to the appearance of every
relevant document; rel(q, d) can be understood as a binary indicator variable, where non-relevant
documents contribute nothing. Since the denominator is the total number of relevant documents,
relevant documents that don’t appear in the ranked list at all contribute zero to the average. Once
again, relevance is assumed to be binary.
Typically, average precision is measured without an explicit cutoff, over the entirety of the ranked
list; since the default length of l used in most evaluations is 1000, the practical effect is that AP is
computed at a cutoff of rank 1000, although it is almost never written as AP@1000. Since the metric
factors in retrieval of all relevant documents, a cutoff would artificially reduce the score (i.e., it has
the effect of including a bunch of zeros in the average for relevant documents that do not appear in the
ranked list). Evaluations use average precision when the task requires taking into account recall, so
imposing a cutoff usually doesn’t make sense. The implied cutoff of 1000 is a compromise between
accurate measurement and practicality: in practice, relevant documents appearing below rank 1000
contribute negligibly to the final score (which is usually reported to four digits after the decimal
point), and run submissions with 1000 hits per topic are still manageable in size.
Average precision is more difficult to interpret, but it is a single summary statistic that captures
aspects of both precision and recall, while favoring appearance of relevant documents towards the top
of the ranked list. The downside of average precision is that it does not distinguish between relevance
grades; that is, “marginally” relevant and “highly” relevant documents make equal contributions to
the score.
Normalized Discounted Cumulative Gain (nDCG) is a metric that is most frequently used to
measure the quality of web search results. Unlike the other metrics above, nDCG was specifically
designed for graded relevance judgments. For example, if relevance were measured on a five-point
scale, rel(q, d) would return r ∈ {0, 1, 2, 3, 4}. First, we define Discounted Cumulative Gain (DCG):
DCG(R, q) = X
(i,d)∈R
2
rel(q,d) − 1
log2
(i + 1). (7)
Gain is used here in the sense of utility, i.e., how much value does a user derive from a particular
result. There are two factors that go into this calculation: (1) the relevance grade (i.e., highly relevant
results are “worth” more than relevant results) and (2) the rank at which the result appears (relevant
results near the top of the ranked list are “worth” more). The discounting refers to the decay in the
gain (utility) as the user consumes results lower and lower in the ranked list, i.e., factor (2). Finally,
we introduce normalization:
nDCG(R, q) = DCG(R, q)
IDCG(R, q)
, (8)
where IDCG represents the DCG of an “ideal” ranked list: this would be a ranked list that begins
with all of the documents of the highest relevance grade, then the documents with the next highest
relevance grade, etc. Thus, nDCG represents DCG normalized to a range of [0, 1] with respect to
the best possible ranked list. Typically, nDCG is associated with a rank cutoff; a value of 10 or 20 is
common. Since most commercial web search engines present ten results on a page (on the desktop,
at least), these two settings represent nDCG with respect to the first or first two pages of results. For
similar reasons, nDCG@3 or nDCG@5 are often used in the context of mobile search, given the
much smaller screen sizes of phones.
This metric is popular for evaluating the results of web search for a number of reasons: First, nDCG
can take advantage of graded relevance judgments, which provide finer distinctions on output quality.
Second, the discounting and cutoff represent a reasonably accurate (albeit simplified) model of
real-world user behavior, as revealed through eye-tracking studies; see, for example, Joachims et al.
28
[2007]. Users do tend to scan results linearly, with increasing probability of “giving up” and “losing
interest” as they consume more and more results (i.e., proceed further down the ranked list). This is
modeled in the discounting, and there are variants of nDCG that apply different discounting schemes
to model this aspect of user behavior. The cutoff value models a hard stop when users stop reading
(i.e., give up). For example, nDCG@10 quantifies the result quality of the first page of search results
in a browser, assuming the user never clicks “next page” (which is frequently the case).
All of the metrics we have discussed above quantify the quality of a single ranked list with respect
to a specific topic (query). Typically, the arithmetic mean across all topics in a test collection is
used as a single summary statistic to denote the quality of a run for those topics.
48 We emphasize
that it is entirely meaningless to compare effectiveness scores from different test collections (since
scores do not control for differences due to corpora, topic difficulty, and many other issues), and even
comparing a run that participated in a particular evaluation with a run that did not can be fraught with
challenges (see next section).
A few additional words of caution: aggregation can hide potentially big differences in per-topic scores.
Some topics are “easy” and some topics are “difficult”, and it is certainly possible that a particular
ranking model has an affinity towards certain types of information needs. These nuances are all lost
in a simple arithmetic mean across per-topic scores.
There is one frequently unwritten detail that is critical to the interpretation of metrics worth discussing.
What happens if the ranked list R contains a document for which no relevance judgment exists, i.e.,
the document does not appear in the qrels file for that topic? This is called an “unjudged document”,
and the standard treatment (by most evaluation programs) is to consider unjudged documents not
relevant. Unjudged documents are quite common because it is impractical to exhaustively assess the
relevance of every document in a collection with respect to every information need; the question of
how to select documents for assessment is discussed in the next section, but for now let’s just take
this observation as a given.
The issue of unjudged documents is important because of the assumption that unjudged documents
are not relevant. Thus, a run may score poorly not because the ranking model is poor, but because
the ranking model produces many results that are unjudged (again, assume this as a given for now;
we discuss why this may be the case in the next section). The simplest way to diagnose potential
issues is to compute the fraction of judged documents at cutoff k (Judged@k or J@k). For example,
if we find that 80% of the results in the top 10 hits are unjudged, Precision@10 is capped at 0.2.
There is no easy fix to this issue beyond diagnosing and noting it: assuming that unjudged documents
are not relevant is perhaps too pessimistic, but the alternative of assuming that unjudged documents
are relevant is also suspect. While information retrieval researchers have developed metrics that
explicitly account for unjudged documents, e.g., bpref [Buckley and Voorhees, 2004], the condensed
list approach [Sakai, 2007], and rank-based precision (RBP) [Moffat and Zobel, 2008], in our opinion
these metrics have yet to reach widespread adoption by the community.
There is a final detail worth explicitly mentioning. All of the above metrics assume that document
scores are strictly decreasing, and that there are no score ties. Otherwise, the evaluation program
must arbitrarily make some decision to map identical scores to different ranks (necessary because
metrics are defined in terms of rank order). For example, trec_eval breaks ties based on the reverse
lexicographical order of the document ids. These arbitrary decisions introduce potential differences
across alternative implementations of the same metric. Most recently, Lin and Yang [2019] quantified
the effects of scoring ties from the perspective of experimental repeatability and found that score
ties can be responsible for metric differences up to the third place after the decimal point. While the
overall effects are small and not statistically significant, to eliminate this experimental confound, they
advocated that systems should explicitly ensure that there are no score ties in the ranked lists they
produce, rather than let the evaluation program make arbitrary decisions.49 Of course, Lin and Yang
were not the first to examine this issue, see for example, Cabanac et al. [2010], Ferro and Silvello
[2015] for additional discussions.
48Although other approaches for aggregation have been explored, such as the geometric and harmonic means [Ravana and Moffat, 2009].
49This can be accomplished by first defining a consistent tie-breaking procedure and then subtracting a small  to
the tied scores to induce the updated rank ordering.
29
We conclude this section with a number of remarks, some of which represent conventions and tacit
knowledge by the community that are rarely explicitly communicated:
• Naming metrics. Mean average precision, abbreviated MAP, represents the mean of average
precision scores across many topics. Similarly, mean reciprocal rank, abbreviated MRR,
represents the mean of reciprocal rank scores across topics.50 In some papers, the phrase
“early-precision” is used to refer to the quality of top ranked results—as measured by a metric
such as Precision@k or nDCG@k with a relatively small cutoff (e.g., k = 10). It is entirely
possible for a system to excel at early precision (i.e., identify a few relevant documents and
place them near the top of the ranked list) but not necessarily be effective when measured using
recall-oriented metrics (which requires identifying all relevant documents).
• Reporting metrics. Most test collections or evaluations adopt an official metric, or sometimes,
a few official metrics. It is customary when reporting results to at least include those official
metrics; including additional metrics is usually fine, but the official metrics should not be
neglected. The choice of metric is usually justified by the creators of the test collection or the
organizers of the evaluation (e.g., we aim to solve this problem, and the quality of the solution
is best captured by this particular metric). Unless there is a compelling reason otherwise, follow
established conventions; otherwise, results will not be comparable.
It has been a convention, for example, at TREC, that metrics are usually reported to four places
after the decimal, e.g., 0.2932. In prose, a unit of 0.01 in score is often referred to as a point, as
in, an improvement from 0.19 to 0.29 is a ten-point gain. In some cases, particularly in NLP
papers, metrics are reported in these terms, e.g., multiplied by 100, so 0.2932 becomes 29.32.51
We find this convention acceptable, as there is little chance for confusion. Finally, recognizing
that a difference of 0.001 is just noise, some researchers opt to only report values to three digits
after the decimal point, so 0.2932 becomes 0.293.
• Comparing metrics. Entire tomes have been written about proper evaluation practices when
comparing results, for example, what statistical tests of significance to use and when. As we
lack the space for a detailed exposition, we refer readers to Sakai [2014] and Fuhr [2017] as
starting points into the literature.
Having defined metrics for measuring the quality of a ranked list, we have now described all
components of the text ranking problem: Given an information need expressed as a query q, the text
ranking task is to return a ranked list of k texts {d1, d2 . . . dk} from an arbitrarily large but finite
collection of texts C = {di} that maximizes a metric of interest. Where are the resources we need to
concretely tackle this challenge? We turn our attention to this next.
2.6 Community Evaluations and Reusable Test Collections
Based on the discussions above, we can enumerate the ingredients necessary to evaluate a text ranking
model: a corpus or collection of texts to search, a set of information needs (i.e., topics), and relevance
judgments (qrels) for those needs. Together, these comprise the components of what is known as a
test collection for information retrieval research. With a test collection, it become straightforward to
generate rankings with a particular ranking model and then compute metrics to quantify the quality
of those rankings, for example, using any of those discussed in the previous section. And having
quantified the effectiveness of results, it then becomes possible to make measurable progress in
improving ranking models. We have our hill and we know how high up we are. And if we have
enough relevance judgments (see Section 2.4), we can directly train ranking models. In other words,
we have a means to climb the hill.
50Some texts use MAP to refer to the score for a specific topic, which is technically incorrect. This is related to a
somewhat frivolous argument on metric names that has raged on in the information retrieval community for
decades now: there are those who argue that even the summary statistic across multiple topics for AP should
be referred to as AP. They point as evidence the fact that no researcher would ever write “MP@5” (i.e., mean
precision at rank cutoff 5), and thus to be consistent, every metric should be prefixed by “mean”, or none at
all. Given the awkwardness of “mean precision”, the most reasonable choice is to omit “mean” from average
precision as well. We do not wish to take part in this argument, and use “MAP” and “MRR” simply because
most researchers do.
51This likely started with BLEU scores in machine translation.
30
Although conceptually simple, the creation of resources to support reliable, large-scale evaluation of
text retrieval methods is a costly endeavor involving many subtle nuances that are not readily apparent,
and is typically beyond the resources of individual research groups. Fortunately, events such as the
Text Retrieval Conferences (TRECs), organized by the U.S. National Institute for Standards and
Technology (NIST), provide the organizational structure as well as the resources necessary to bring
together multiple teams in community-wide evaluations. These exercises serve a number of purposes:
First, they provide an opportunity for the research community to collectively set its agenda through
the types of tasks that are proposed and evaluated; participation serves as a barometer to gauge interest
in emerging information access tasks. Second, they provide a neutral forum to evaluate systems in a
fair and rigorous manner. Third, typical byproducts of evaluations include reusable test collections
that are capable of evaluating systems that did not participate in the evaluation (more below). Some
of these test collections are used for many years, some even decades, after the original evaluations
that created them. Finally, the evaluations may serve as testbeds for advancing novel evaluation
methodologies themselves; that is, the goal is not only to evaluate systems, but the processes for
evaluating systems.
TREC, which has been running for three decades, kicks off each spring with a call for participation.
The evaluation today is divided into (roughly half a dozen) “tracks” that examine different information
access problems. Proposals for tracks are submitted the previous year in the fall, where groups of
volunteers (typically, researchers from academia and industry) propose to organize tracks. These
proposals are then considered by a committee, and selected proposals define the evaluation tasks
that are run. Over its history, TREC has explored a wide range of tasks beyond ad hoc retrieval,
including search in a variety of different languages and over speech; in specialized domains such
as biomedicine and chemistry; different types of documents such as blogs and tweets; different
modalities of querying such as filtering and real-time summarization; as well as interactive retrieval,
conversational search, and other user-focused issues. For a general overview of different aspects
of TREC (at least up until the middle of the first decade of the 2000s), the “TREC book” edited
by Voorhees and Harman [2005] provides a useful starting point.
Tracks at TREC often reflect emerging interests in the information retrieval community; explorations
there often set the agenda for the field and achieve significant impact beyond the academic ivory tower.
Writing in 2008, Hal Varian, chief economist at Google, acknowledged that in the early days of the
web, “researchers used industry-standard algorithms based on the TREC research to find documents
on the web”.52 Another prominent success story of TREC is IBM’s Watson question answering
system that resoundingly beat two human champions on the quiz show Jeopardy! in 2011. There is a
direct lineage from Watson, including both the techniques it used and the development team behind
the scenes, to the TREC question answering tracks held in the late 1990s and early 2000s.
Participation in TREC is completely voluntary with no external incentives (e.g., prize money),53 and
thus researchers “vote with their feet” in selecting tracks that are of interest to them. While track
organizers begin with a high-level vision, the development of individual tracks is often a collaboration
between the organizers and participants, aided by guidance from NIST. System submissions for the
tasks are typically due in the summer, with evaluation results becoming available in the fall time
frame. Each TREC cycle concludes with a workshop held on the grounds of the National Institute
of Standards and Technology in Gaithersburg, Maryland, where participants convene to discuss the
evaluation results and present their solutions to the challenges defined in the different tracks.54 The
cycle then begins anew with planning for the next year.
Beyond providing the overarching organizational framework for exploring different tracks at TREC,
NIST also contributes evaluation resources and expertise, handling the bulk of the “mechanics”
of the evaluation. Some of this was already discussed in Section 2.2: Unless specialized domain
expertise is needed, for example, in biomedicine, NIST assessors perform topic development, or the
creation of the information needs, and provide the relevance assessments as well. Historically, most
of the NIST assessors are retired intelligence analysts, which means that assessing, synthesizing,
and otherwise drawing conclusions from information was, literally, their job. Topic development is
usually performed in the spring, based on initial exploration of the corpus used in the evaluation. To
52https://googleblog.blogspot.com/2008/03/why-data-matters.html
53An exception is that sometimes a research sponsor (funding agency) uses TREC as an evaluation vehicle, in
which case teams that receive funding are compelled to participate.
54In the days before the COVID-19 pandemic, that is.
31
the extent possible, the assessor who created the topic (and wrote the topic statement) is the person
who provides the relevance judgments (later that year, generally in the late summer to early fall time
frame). This ensures that the judgments are as consistent as possible. To emphasize a point we have
already made in Section 2.2: the relevance judgments are the opinion of this particular person.
55
What do NIST assessors actually evaluate? In short, they evaluate the submissions (i.e., “runs”) of
teams who participated in the evaluation. For each topic, using a process known as pooling [Sparck
Jones and van Rijsbergen, 1975, Buckley et al., 2007], runs from the participants are gathered, with
duplicates removed, and presented to the assessor. To be clear, a separate pool is created for each
topic. The most common (and fair) way to construct the pools is to select the top k results from
each participating run, where k is determined by the amount of assessment resources available. This
is referred to as top-k pooling or pooling to depth k. Although NIST has also experimented with
different approaches to constructing the pools, most recently, using bandit techniques [Voorhees,
2018], top-k pooling remains the most popular approach due to its predictability and well-known
properties (both advantages and disadvantages).
System results for each query (i.e., from the pools) are then presented to an assessor in an evaluation
interface, who supplies the relevance judgments along the previously agreed scale (e.g., a three-way
relevance grade). To mitigate systematic biases, pooled results are not associated with the runs they
are drawn from, so the assessor only sees (query, result) pairs and has no explicit knowledge of the
source. After the assessment process completes, all judgments are then gathered to assemble the qrels
for those topics, and these relevance judgments are used to evaluate the submitted runs (e.g., using
one or a combination of the metrics discussed in the previous section).
Relevance judgments created from TREC evaluations are used primarily in one of two ways:
1. They are used to quantify the effectiveness of systems that participated in the track. The
evaluation of the submitted runs using the relevance judgments created from the pooling process
accomplishes this goal, but the results need to be interpreted in a more nuanced way than
just comparing the value of the metrics. Whether system differences can be characterized as
significant or meaningful is more than just a matter of running standard significance tests, but
must consider a multitude of other factors, including all the issues discussed in Section 2.2
and more [Sanderson and Zobel, 2005]. Details of how this is accomplished depend on the
task and vary from track to track; for an interested reader, Voorhees and Harman [2005] offer a
good starting point. For more details, in each year’s TREC proceedings, each track comes with
an overview paper written by the organizers that explains the task setup and summarizes the
evaluation results.
2. Relevance judgments contribute to a test collection that can be used as a standalone evaluation
instrument by researchers beyond the original TREC evaluation that created them. These test
collections can be used for years and even decades; for example, as we will describe in more
detail in the next section, the test collection from the TREC 2004 Robust Track is still widely
used today!
In the context of using relevance judgments from a particular test collection, there is an important
distinction between runs that participated in the evaluation vs. those that did not. These “after-the-fact”
runs are sometimes called “post hoc” runs.
First, the results of official submissions are considered by most researchers to be more “credible”
than post-hoc runs, due to better methodological safeguards (e.g., less risk of overfitting). We return
to discuss this issue in more detail in Section 2.7.
Second, relevance judgments may treat participating systems and post-hoc submissions differently,
as we explain. There are two common use cases for test collections: A team that participated in
the TREC evaluation might use the relevance judgments to further investigate model variants or
perhaps conduct ablation studies. A team that did not participate in the TREC evaluation might use
the relevance judgments to evaluate a newly proposed technique, comparing it against runs submitted
to the evaluation. In the former case, a variant technique is likely to retrieve similar documents as
a submitted run, and therefore less likely to encounter unjudged documents—which, as we have
previously mentioned, are treated as not relevant by standard evaluation tools (see Section 2.5). In
55The NIST assessors are invited to the TREC workshop, and every year, some subset of them do attend. And
they’ll sometimes even tell you what topic was theirs. Sometimes they even comment on your system.
32
the latter case, a newly proposed technique may encounter more unjudged documents, and thus
score poorly—not necessarily because it was “worse” (i.e., lower quality), but simply because it was
different. That is, the new technique surfaced documents that had not been previously retrieved (and
thus never entered the pool to be assessed).
In other words, there is a danger that test collections encourage researchers to search only “under the
lamplight”, since the universe of judgments is defined by the participants of a particular evaluation
(and thus represents a snapshot of the types of techniques that were popular at the time). Since many
innovations work differently than techniques that came before, old evaluation instruments may not be
capable of accurately quantifying effectiveness improvements associated with later techniques. As a
simple (but contrived) example, if the pools were constructed exclusively from techniques based on
exact term matches, the resulting relevance judgments would be biased against systems that exploited
semantic match techniques that did not rely exclusively on exact match signals. In general, old
test collections may be biased negatively against new techniques, which is particularly undesirable
because they may cause researchers to prematurely abandon promising innovations simply because
the available evaluation instruments are not able to demonstrate their improvements.
Fortunately, IR researchers have long been cognizant of these dangers and evaluations usually take
a variety of steps to guard against them. The most effective strategy is to ensure a rich and diverse
pool, where runs adopt a variety of different techniques, and to actively encourage “manual” runs
that involve humans in the loop (i.e., users interactively searching the collection to compile results).
Since humans obviously do more than match keywords, manual runs increase the diversity of the
pool. Furthermore, researchers have developed various techniques to assess the reusability of test
collections, characterizing their ability to fairly evaluate runs from systems that did not participate
in the original evaluation [Zobel, 1998, Buckley et al., 2007]. The literature describes a number of
diagnostics, and test collections that pass this vetting are said to be reusable.
From a practical perspective, there are several steps that researchers can take to sanity check their
evaluation scores to determine if a run is actually worse, or simply different. One common technique
is to compute and report the fraction of unjudged documents, as discussed in the previous section.
If two runs have very different proportions of unjudged documents, this serves as a strong signal
that one of those runs may not have been evaluated fairly. Another approach is to use a metric that
explicitly attempts to account for unjudged documents, such as bpref or RBP (also discussed in the
previous section).
Obviously, different proportions of unjudged documents can be a sign that effectiveness differences
might be attributable to missing relevance judgments. However, an important note is that the absolute
proportion of unjudged documents is not necessarily a sign of unreliable evaluation results in itself.
The critical issue is bias, in the sense of Buckley et al. [2007]: whether the relevance judgments
represent a random (i.e., non-biased) sample of all relevant documents. Consider the case where two
runs have roughly the same proportion of unjudged documents (say, half are unjudged). There are few
firm conclusions that can be drawn in this situation without more context. Unjudged documents are
inevitable, and even a relatively high proportion of unjudged isn’t “bad” per se. This could happen,
for example, when two runs that participated in an evaluation are assessed with a metric at a cutoff
larger than the number of documents each run contributed to the pool. For example, the pool was
constructed with top-100 pooling, but MAP is measured to rank 1000. In such cases, there is no
reason to believe that the unjudged documents are systematically biased against one run or the other.
However, in other cases (for example, the bias introduced by systems based on exact term matching),
there may be good reason to suspect the presence of systematic biases.
TREC, as a specific realization of the Cranfield paradigm, has been incredibly influential, both on IR
research and more broadly in the commercial sphere; for example, see an assessment of the economic
impact of TREC conducted in 2010 [Rowe et al., 2010]. TREC’s longevity—2021 marks the thirtieth
iteration—is just one testament to its success. Another indicator of success is that the “TREC model”
has been widely emulated around the world. Examples include CLEF in Europe and NTCIR and
FIRE in Asia, which are organized in much the same way.
With this exposition, we have provided a high-level overview of modern evaluation methodology for
information retrieval and text ranking under the Cranfield paradigm—covering inputs to and outputs
of the ranking model, how the results are evaluated, and how test collections are typically created.
33
We conclude with a few words of caution already mentioned in the introductory remarks: The beauty
of the Cranfield paradigm lies in a precise formulation of the ranking problem with a battery of
quantitative metrics. This means that, with sufficient training data, search can be tackled as an
optimization problem using standard supervised machine-learning techniques. Beyond the usual
concerns with overfitting, and whether test collections are realistic instances of information needs
“in the wild”, there is a fundamental question regarding the extent to which system improvements
translates into user benefits. Let us not forget that the latter is the ultimate goal, because users
seek information to “do something”, e.g., decide what to buy, write a report, find a job, etc. A
well-known finding in information retrieval is that better search systems (as evaluated by the Cranfield
methodology) might not lead to better user task performance as measured in terms of these ultimate
goals; see, for example, Hersh et al. [2000], Allan et al. [2005]. Thus, while evaluations using the
Cranfield paradigm undoubtedly provide useful signals in characterizing the effectiveness of ranking
models, they do not capture “the complete picture”.
2.7 Descriptions of Common Test Collections
Supervised machine-learning techniques require data, and the community is fortunate to have access
to many test collections, built over decades, for training and evaluating text ranking models. In this
section, we describe test collections that are commonly used by researchers today. Our intention
is not to exhaustively cover all test collections used by every model in this survey, but to focus on
representative resources that have played an important role in the development of transformer-based
ranking models.
When characterizing and comparing test collections, there are a few key statistics to keep in mind:
• Size of the corpus or collection, in terms of the number of texts |C|, the mean length of each
text L(C), the median length of each text Le(C), and more generally, the distribution of the
lengths. The size of the corpus is one factor in determining the amount of effort required to
gather sufficient relevance judgments to achieve “good” coverage. The average length of a text
provides an indication of the amount of effort required to assess each result, and the distribution
of lengths may point to ranking challenges.56
• Size of the set of evaluation topics, both in terms of the number of queries |q| and the average
length of each query L(q). Obviously, the more queries, the better, from the perspective of
accurately quantifying the effectiveness of a particular approach. Average query length offers
clues about the expression of the information needs (e.g., amount of detail).
• The number of relevance judgments available, both in terms of positive and negative labels.
We can quantify this in terms of the average number of judgments per query |J|/q as well as
the number of relevant labels per query |Rel|/q.
57 Since the amount of resources (assessor
time, money for paying assessors, etc.) that can be devoted to performing relevance judgments
is usually fixed, there are different strategies for allocating assessor effort. One choice is to
judge many queries (say, hundreds), but examine relatively few results per query, for example,
by using a shallow pool depth. An alternative is to judge fewer queries (say, dozens), but
examine more texts per query, for example, by using a deeper pool depth. Colloquially, these
are sometimes referred to as “shallow but wide” (or “sparse”) judgments vs. “narrow but deep”
(or “dense”) judgments. We discuss the implications of these different approaches in the context
of specific test collections below.
In addition, the number of relevant texts (i.e., positive judgments) per topic is an indicator of
difficulty. Generally, evaluation organizers prefer topics that are neither too difficult nor too
easy. If the topics are too difficult (i.e., too few relevant documents), systems might all perform
poorly, making it difficult to discriminate system effectiveness, or systems might perform well
for idiosyncratic reasons that are difficult to generalize. On the other hand, if the topics are too
56Retrieval scoring functions that account for differences in document lengths, e.g., Singhal et al. [1996],
constituted a major innovation in the 1990s. As we shall see in Section 3, long texts pose challenges for
ranking with transformer-based models. In general, collections with texts that differ widely in length are more
challenging, since estimates of relevance must be normalized with respect to length.
57In the case of graded relevance judgments, there is typically a binarization scheme to separate relevance grades
into “relevant” and “not relevant” categories for metrics that require binary judgments.
34
Corpus |C| L(C) Le(C)
MS MARCO passage corpus 8,841,823 56.3 50
MS MARCO document corpus 3,213,835 1131.3 584
Robust04 corpus (TREC disks 4&5) 528,155 548.6 348
Table 2: Summary statistics for three corpora used by many text ranking models presented in this
survey: number of documents |C|, mean document length L(C), and median document length Le(C).
The MS MARCO passage corpus was also used for the TREC 2019/2020 Deep Learning Track
passage ranking task and the MS MARCO document corpus was also used for the TREC 2019/2020
Deep Learning Track document ranking task.
Dataset |q| L(q) |J| |J|/q |Rel|/q
MS MARCO passage ranking (train) 502,939 6.06 532,761 1.06 1.06
MS MARCO passage ranking (development) 6,980 5.92 7,437 1.07 1.07
MS MARCO passage ranking (test) 6,837 5.85 - - -
MS MARCO document ranking (train) 367,013 5.95 367,013 1.0 1.0
MS MARCO document ranking (development 5,193 5.89 5,193 1.0 1.0
MS MARCO document ranking (test) 5,793 5.85 - - -
TREC 2019 DL passage 43 5.40 9,260 215.4 58.2
TREC 2019 DL document 43 5.51 16,258 378.1 153.4
TREC 2020 DL passage 54 6.04 11,386 210.9 30.9
TREC 2020 DL document 45 6.31 9,098 202.2 39.3
Robust04 249 (title) 2.67 311,410 1250.6 69.9
(narr.) 15.32
(desc.) 40.22
Table 3: Summary statistics for select queries and relevance judgments used by many text ranking
models presented in this survey. For Robust04, we separately provide average lengths of the title,
narrative, and description fields of the topics. Note that for the TREC 2019/2020 DL data, relevance
binarization is different for passage vs. documents; here we simply count all judgments that have a
non-zero grade.
easy (i.e., too many relevant documents), then all systems might obtain high scores, also making
it difficult to separate “good” from “bad” systems.
A few key statistics of the MS MARCO passage ranking test collection, MS MARCO document
ranking test collection, and the Robust04 test collection are summarized in Table 2 and Table 3. The
distributions of the lengths of texts from these three corpora are shown in Figure 3. In these analyses,
tokens counts are computed by splitting texts on whitespace,58 which usually yields values that differ
from lengths computed from the perspective of keyword search (e.g., due to stopwords removal and
de-compounding) and lengths from the perspective of input sequences to transformers (e.g., due to
subword tokenization).
We describe a few test collections in more detail below:
MS MARCO passage ranking test collection. This dataset, originally released in 2016 [Nguyen
et al., 2016], deserves tremendous credit for jump-starting the BERT revolution for text ranking.
We’ve already recounted the story in Section 1.2.5: Nogueira and Cho [2019] combined the two
critical ingredients (BERT and training data for ranking) to make a “big splash” on the MS MARCO
passage ranking leaderboard.
The MS MARCO dataset was originally released in 2016 to allow academic researchers to explore
information access in the large-data regime—in particular, to train neural network models [Craswell
et al., 2021a]. Initially, the dataset was designed to study question answering on web passages,
but it was later adapted into traditional ad hoc ranking tasks. Here, we focus only on the passage
ranking task [Bajaj et al., 2018]. The corpus comprises 8.8 million passage-length extracts from web
pages; these passages are typical of “answers” that many search engines today show at the top of
58Specifically, Python’s split() method for strings.
35
0 30 60 90 120 150
0
500,000
1,000,000
1,500,000
Number of texts
MS MARCO Passage
0 1,000 2,000 3,000 4,000 5,000
0
50,000
100,000
150,000
Number of texts
MS MARCO Document
0 500 1,000 1,500 2,000
0
10,000
20,000
30,000
40,000
50,000
Number of texts
Robust04
Figure 3: Histograms capturing the distribution of the lengths of texts (based on whitespace tokenization) in three commonly used corpora.
36
their result pages (these are what Google calls “featured snippets”, and Bing has a similar feature).
The information needs are anonymized natural language questions drawn from Bing’s query logs,
where users were specifically looking for an answer; queries with navigational and other intents
were discarded. Since these questions were drawn from user queries “in the wild”, they are often
ambiguous, poorly formulated, and may even contain typographical and other errors. Nevertheless,
these queries reflect a more “natural” distribution of information needs, compared to, for example,
existing question answering datasets such as SQuAD [Rajpurkar et al., 2016].
For each query, the test collection contains, on average, one relevant passage (as assessed by human
annotators). In the training set, there are a total of 532.8K (query, relevant passage) pairs over 502.9K
unique queries. The development (validation) set contains 7437 pairs over 6980 unique queries. The
test (evaluation) set contains 6837 queries, but relevance judgments are not publicly available; scores
on the test queries can only be obtained via a submission to the official MS MARCO leaderboard.59
The official evaluation metric is MRR@10.
One notable feature of this resource worth pointing out is the sparsity of judgments—there are many
queries, but on average, only one relevant judgment per query. This stands in contrast to most test
collections constructed by pooling, such as those from TREC evaluations. As we discussed above,
these judgments are often referred to as “shallow” or “sparse”, and this design has two important
consequences:
1. Model training requires both positive as well as negative examples. For this, the task organizers
have prepared “triples” files comprising (query, relevant passage, non-relevant passage) triples.
However, these negative examples are heuristically-induced pseudo-labels: they are drawn from
BM25 results that have not been marked as non-relevant by human annotators. In other words,
the negative examples have not been explicitly vetted by human annotators as definitely being
not relevant. The absence of a positive label does not necessarily mean that the passage is
non-relevant.
2. As we will see in Section 3.2, the sparsity of judgments holds important implications for the
ability to properly assess the contribution of query expansion techniques. This is a known
deficiency, but there may be other yet-unknown issues as well. The lack of “deep” judgments
per query in part motivated the need for complementary evaluation data, which are supplied by
the TREC Deep Learning Tracks (discussed below).
These flaws notwithstanding, it is difficult to exaggerate the important role that the MS MARCO
dataset has played in advancing research in information retrieval and information access more broadly.
Never before had such a large and realistic dataset been made available to the academic research
community.60 Previously, such treasures were only available to researchers inside commercial
search engine companies and other large organizations with substantial numbers of users engaged in
information seeking.
Today, this dataset is used by many researchers for diverse information access tasks, and it has become
a common starting point for building transformer-based ranking models. Even for ranking in domains
that are quite distant, for example, biomedicine (see Section 6.2), many transformer-based models
are first fine-tuned with MS MARCO data before further fine-tuning on domain- and task-specific
data (see Section 3.2.4). Some experiments have even shown that ranking models fine-tuned on this
dataset exhibit zero-shot relevance transfer capabilities, i.e., the models are effective in domains
and on tasks without having been previously exposed to in-domain or task-specific labeled data (see
Section 3.5.3 and Section 6.2).
In summary, the impact of the MS MARCO passage ranking test collection has been no less than
transformational. The creators of the dataset (and Microsoft lawyers) deserve tremendous credit for
their contributions to broadening the field.
MS MARCO document ranking test collection. Although in reality the MS MARCO document
test collection was developed in close association with the TREC 2019 Deep Learning Track [Craswell
et al., 2020] (see below), and a separate MS MARCO document ranking leaderboard was established
59http://www.msmarco.org/
60Prior to MS MARCO, a number of learning-to-rank datasets comprising features values were available to
academic researchers, but they did not include actual texts.
37
only in August 2020, it makes more sense conceptually to structure the narrative in the order we
present here.
The MS MARCO document ranking test collection was created as a document ranking counterpart
to the passage ranking test collection. The corpus, which comprises 3.2M web pages with URL,
title, and body text, contains the source pages of the 8.8M passages from the passage corpus [Bajaj
et al., 2018]. However, the alignment between the passages and the documents is imperfect, as the
extraction was performed on web pages that were crawled at different times.
For the document corpus, relevance judgments were “transferred” from the passage judgments; that
is, for a query, if the source web page contained a relevant passage, then the corresponding document
was considered relevant. This data preparation possibly created a systematic bias in that relevant
information was artificially centered on a specific passage within the document, more so than they
might occur naturally. For example, we are less likely to see a relevant document that contains
short relevant segments scattered throughout the text; this has implications for evidence aggregation
techniques that we discuss in Section 3.3.
In total, the MS MARCO document dataset contains 367K training queries and 5193 development
queries; each query has exactly one relevance judgment. There are 5793 test queries, but relevance
judgments are withheld from the public. As with the MS MARCO passage ranking task, scores for
the test queries can only be obtained by a submission to the leaderboard. The official evaluation
metric is MRR@100. Similar comments about the sparsity of relevance judgments, made in the
context of the passage dataset above, apply here as well.
TREC 2019/2020 Deep Learning Tracks. Due to the nature of TREC planning cycles, the organization of the Deep Learning Track at TREC 2019 [Craswell et al., 2020] predated the advent of BERT
for text ranking. Coincidentally, though, it represented the first large-scale community evaluation that
provided a comparison of pre-BERT and BERT-based ranking models, attracting much attention and
participation from researchers. The Deep Learning Track continued in TREC 2020 [Craswell et al.,
2021b] with the same basic setup.
From the methodological perspective, the track was organized to explore the impact of large amounts
of training data, both on neural ranking models as well as learning-to-rank techniques, compared to
“traditional” exact match techniques. Furthermore, the organizers wished to investigate the impact of
different types of training labels, in particular, sparse judgments (many queries but very few relevance
judgments per query) typical of data gathered in an industry setting vs. dense judgments created by
pooling (few queries but many more relevance judgments per query) that represent common practice
in TREC and other academic evaluations. For example, what is the effectiveness of models trained
on sparse judgments when evaluated with dense judgments?
The evaluation had both a document ranking and a passage ranking task; additionally, the organizers
shared a list of results for reranking if participants did not wish to implement initial candidate
generation themselves. The document corpus and the passage corpus used in the track were exactly
the same as the MS MARCO document corpus and the MS MARCO passage corpus, respectively,
discussed above. Despite the obvious connections, the document and passage ranking tasks were
evaluated independently with separate judgment pools.
Based on pooling, NIST assessors evaluated 43 queries for both the document ranking and passage
ranking tasks in TREC 2019; in TREC 2020, there were 54 queries evaluated for the passage ranking
task and 45 queries evaluated for the document ranking task. In all cases relevance judgments were
provided on a four-point scale, although the binarization of the grades (e.g., for the purposes of
computing MAP) differed between the document and passage ranking tasks; we refer readers to the
track overview papers for details [Craswell et al., 2020, 2021b]. Statistics of the relevance judgments
are presented in Table 3. It is likely the case that these relevance judgments alone are insufficient to
effectively train neural ranking models (too few labeled examples), but they serve as a much richer
test set compared to the MS MARCO datasets. Since there are many more relevant documents per
query, metrics such as MAP are (more) meaningful, and since the relevance judgments are graded,
metrics such as nDCG make sense. In contrast, given the sparse judgments in the original MS
MARCO datasets, options for evaluation metrics are limited. In particular, evaluation of document
ranking with MRR@100 is odd and rarely seen.
Mackie et al. [2021] built upon the test collections from the TREC 2019 and 2020 Deep Learning
Tracks to create a collection of challenging queries called “DL-HARD”. The goal of this resource was
38
to increase the difficulty of the Deep Learning Track collections using queries that are challenging for
the “right reasons”. That is, queries that express complex information needs rather than queries that
are, for example, factoid questions (“how old is vanessa redgrave”) or queries that would typically be
answered by a different vertical (“how is the weather in jamaica”). DL-HARD combined difficult
queries judged in the TREC 2019 and 2020 Deep Learning Track document and passage collections
(25 from the document collection and 23 from the passage collection) with additional queries with
new sparse judgments (25 for the document collection and 27 for the passage collection). The authors
assessed query difficulty using a combination of automatic criteria derived from a web search engine
(e.g., whether the query could be answered with a dictionary definition infobox) and manual criteria
like the query’s answer type (e.g., definition, factoid, or long answer). The resource also includes
entity links for the queries and annotations of search engine result type, query intent, answer type,
and topic domain.
TREC 2004 Robust Track (Robust04). Although nearly two decades old, the test collection from
the Robust Track at TREC 2004 [Voorhees, 2004] is widely considered one of the best “general
purpose” ad hoc retrieval test collections available to academic researchers, with relevance judgments
drawn from diverse pools with contributions from different techniques, including manual runs. It
is able to fairly evaluate systems that did not participate in the original evaluation (see Section 2.6).
Robust04 is large as academic test collections go in terms of the number of topics and the richness of
relevance judgments, and created in a single TREC evaluation cycle. Thus, this test collection differs
from the common evaluation practice where test collections from multiple years are concatenated
together to create a larger resource. Merging multiple test collections in this way is possible when the
underlying corpus is the same, but this approach may be ignoring subtle year-to-year differences. For
example, there may be changes in track guidelines that reflect an evolving understanding of the task,
which might, for example, lead to differences in how the topics are created and how documents are
judged. The composition of the judgment pools (e.g., in terms of techniques that are represented)
also varies from year to year, since they are constructed from participants’ systems.
The TREC 2004 Robust Track used the corpus from TREC Disks 4 & 5 (minus Congressional
Records),61 which includes material from the Financial Times Limited, the Foreign Broadcast
Information Service, and the Los Angeles Times totaling approximately 528K documents. Due to its
composition, this corpus is typically referred to as containing text from the newswire domain. The
test collection contains a total of 249 topics with around 311K relevance judgments, with topics ids
301–450 and 601–700.62
Due to its age, this collection is particularly well-studied by researchers; for example, a meta-analysis
by Yang et al. [2019b] identified over 100 papers that have used the collection up until early 2019.63
This resource provides the context for interpreting effectiveness results across entire families of
approaches and over time. However, the downside is that the Robust04 test collection is particularly
vulnerable to overfitting.
Unlike most TREC test collections with only around 50 topics, researchers have had some success
training ranking models using Robust04. However, for this use, there is no standard agreed-upon
split, but five-fold cross validation is the most common configuration. It is often omitted in papers,
but researchers typically construct the splits by taking consecutive topic ids, e.g., the first fifty topics,
the next fifty topics, etc.
Additional TREC newswire test collections. Beyond Robust04, there are two more recent newswire
test collections that have been developed at TREC:
• Topics and relevance judgments from the TREC 2017 Common Core Track [Allan et al.,
2017], which used 1.8M articles from the New York Times Annotated Corpus.64 Note that this
evaluation experimented with a pooling methodology based on bandit techniques, which was
found after-the-fact to have a number of flaws [Voorhees, 2018], making it less reusable than
desired. Evaluations conducted on this test collection should bear in mind this caveat.
61https://trec.nist.gov/data/cd45/index.html
62In the original evaluation, 250 topics were released, but for one topic no relevant documents were found in the
collection.
63https://github.com/lintool/robust04-analysis
64https://catalog.ldc.upenn.edu/LDC2008T19
39
• Topics and relevance judgments from the TREC 2018 Common Core Track [Allan et al., 2018],
which used a corpus of 600K articles from the TREC Washington Post Corpus.65
Note that corpora for these two test collections are small by modern standards, so they may not
accurately reflect search scenarios today over large amounts of texts. In addition, both test collections
are not as well-studied as Robust04. As a positive, this means there is less risk of overfitting, but this
also means that there are fewer effective models to compare against.
TREC web test collections. There have been many evaluations at TREC focused on searching
collections of web pages. In particular, the following three are commonly used:
• Topics and relevance judgments from the Terabyte Tracks at TREC 2004–2006, which used
the GOV2 corpus, a web crawl of the .gov domain comprising approximately 25.2M pages by
CSIRO (Commonwealth Scientific and Industrial Research Organisation), distributed by the
University of Glasgow.66
• Topics and relevance judgments from the Web Tracks at TREC 2010–2012. The evaluation used
the ClueWeb09 web crawl,67 which was gathered by Carnegie Mellon University in 2009. The
complete corpus contains approximately one billion web pages in 10 different languages, totaling
5 TB compressed (25 TB uncompressed). Due to the computational requirements of working
with such large datasets, the organizers offered participants two conditions: retrieval over the
entire English portion of the corpus (503.9M web pages), or just over a subset comprising
50.2M web pages, referred to as ClueWeb09b. For expediency, most researchers, even today,
report experimental results only over the ClueWeb09b subset.
• Topics and relevance judgments from the Web Tracks at TREC 2013 and TREC 2014. Typically,
researchers use the ClueWeb12-B13 web crawl, which is a subset comprising 52.3M web
pages taken from the full ClueWeb12 web crawl, which contains 733M web pages (5.54 TB
compressed, 27.3 TB uncompressed).68 This corpus was also gathered by Carnegie Mellon
University, in 2012, as an update of ClueWeb09. Unlike ClueWeb09, ClueWeb12 only contains
web pages in English.
Unfortunately, there is no standard agreed-upon evaluation methodology (for example, training/test
splits) for working with these test collections, and thus results reported in research papers are
frequently not comparable (this issue applies to many other TREC collections as well). Additionally,
unjudged documents are a concern, particularly with the ClueWeb collections, because the collection
is large relative to the amount of assessment effort that was devoted to evaluating the judgment
pools. Furthermore, due to the barrier of entry in working with large collections, there were fewer
participating teams and less diversity in the retrieval techniques deployed in the run submissions.
We end this discussion with a caution, that as with any data for supervised machine learning, test
collections can be abused and there is the ever-present danger of overfitting. When interpreting
evaluation results, it is important to examine the evaluation methodology closely—particularly issues
related to training/test splits and how effectiveness metrics are aggregated (e.g., if averaging is
performed over topics from multiple years).
For these reasons, results from the actual evaluation (i.e., participation in that year’s TREC) tend
to be more “credible” in the eyes of many researchers than “post hoc” (after-the-fact) evaluations
using the test collections, since there are more safeguards to prevent overfitting and (inadvertently)
exploiting knowledge from the test set. Section 2.6 mentioned this issue in passing, but here we
elaborate in more detail:
Participants in a TREC evaluation only get “one shot” at the test topics, and thus the test set
can be considered blind and unseen. Furthermore, TREC evaluations limit the total number of
submissions that are allowed from each research group (typically three), which prevents researchers
from evaluating many small model variations (e.g., differing only in tuning parameters), reporting
65https://trec.nist.gov/data/wapost/
66http://ir.dcs.gla.ac.uk/test_collections/
67https://lemurproject.org/clueweb09/
68https://lemurproject.org/clueweb12/
40
only the best result, and neglecting to mention how many variants were examined. This is an example
of so-called “p-hacking”; here, in essence, tuning on the test topics. More generally, it is almost
never reported in papers how many different techniques the researchers had tried before obtaining a
positive result. Rosenthal [1979] called this the “file drawer problem”—techniques that “don’t work”
are never reported and simply stuffed away metaphorically in a file drawer.
With repeated trials, of course, comes the dangers associated with overfitting, inadvertently exploiting
knowledge about the test set, or simply “getting lucky”. Somewhat exaggerating, of course: if you
try a thousand things, something is likely to work on a particular set of topics.69 Thus, post-hoc
experimental results that show a technique beating the top submission in a TREC evaluation should be
taken with a grain of salt, unless the researchers answer the question: How many attempts did it take
to beat that top run? To be clear, we are not suggesting that researchers are intentionally “cheating” or
engaging in any nefarious activity; quite the contrary, we believe that researchers overwhelmingly act
in good faith all the time. Nevertheless, inadvertent biases inevitably creep into our methodological
practices as test collections are repeatedly used.
Note that leaderboards with private held-out test data70 mitigate, but do not fundamentally solve this
issue. In truth, there is “leakage” any time researchers evaluate on test data—at the very least, the
researchers obtain a single bit of information: Is this technique effective or not? When “hill climbing”
on a metric, this single bit of information is crucial to knowing if the research is “heading in the
right direction”. However, accumulated over successive trials, this is, in effect, training on the test
data. One saving grace with most leaderboards, however, is that they keep track of the number of
submissions by each team. For more discussion of these issues, specifically in the context of the MS
MARCO leaderboards, we refer the reader to Craswell et al. [2021a].
There isn’t a perfect solution to these issues, because using a test collection once and then throwing
it away is impractical. However, one common way to demonstrate the generality of a proposed
innovation is to illustrate its effectiveness on multiple test collections. If a model is applied in a
methodologically consistent manner across multiple test collections (e.g., the same parameters, or at
least the same way of tuning parameters without introducing any collection-specific “tricks”), the
results might be considered more credible.
2.8 Keyword Search
Although there are active explorations of alternatives (the entirety of Section 5 is devoted to this topic),
most current applications of transformers for text ranking rely on keyword search in a multi-stage
ranking architecture, which is the focus of Section 3 and Section 4. In this context, keyword search
provides candidate generation, also called initial retrieval or first-stage retrieval. The results are then
reranked by transformer-based models. Given the importance of keyword search in this context, we
offer some general remarks to help the reader understand the role it plays in text ranking.
By keyword search or keyword querying, we mean a large class of techniques that rely on exact term
matching to compute relevance scores between queries and texts from a corpus, nearly always with
an inverted index (sometimes called inverted files or inverted lists); see Zobel and Moffat [2006]
for an overview. This is frequently accomplished with bag-of-words queries, which refers to the
fact that evidence (i.e., the relevance score) from each query term is considered independently. A
bag-of-words scoring function can be cast into the form of Equation (1) in Section 1.2, or alternatively,
as the inner product between two sparse vectors (where the vocabulary forms the dimension of the
vector). However, keyword search does not necessarily imply bag-of-words queries, as there is a rich
body of literature in information retrieval on so-called “structured queries” that attempt to capture
relationships between query terms—for example, query terms that co-occur in a window or are
contiguous (i.e., n-grams) [Metzler and Croft, 2004, 2005].
Nevertheless, one popular choice for keyword search today is bag-of-words queries with BM25
scoring (see Section 1.2),71 but not all BM25 rankings are equivalent. In fact, there are many examples
of putative BM25 rankings that differ quite a bit in effectiveness. One prominent example appears on
the leaderboard of the MS MARCO passage ranking task: a BM25 ranking produced by the Anserini
69https://xkcd.com/882/
70And even those based on submitting code, for example, in a Docker image.
71However, just to add to the confusion, BM25 doesn’t necessarily imply bag-of-words queries, as there are
extensions of BM25 to phrase queries, for example, Wang et al. [2011]
41
system [Yang et al., 2017, 2018] scores 0.186 in terms of MRR@10, but the Microsoft BM25 baseline
scores two points lower at 0.165.
Non-trivial differences in “BM25 rankings” have been observed by different researchers in multiple
studies [Trotman et al., 2014, Mühleisen et al., 2014, Kamphuis et al., 2020]. There are a number
of reasons why different implementations of BM25 yield different rankings and achieve different
levels of effectiveness. First, BM25 should be characterized as a family of related scoring functions:
Beyond the original formulation by Robertson et al. [1994], many researchers have introduced
variants, as studied by Trotman et al. [2014], Mühleisen et al. [2014], Kamphuis et al. [2020]. Thus,
when researchers refer to BM25, it is often not clear which variant they mean. Second, document
preprocessing—which includes document cleaning techniques, stopwords lists, tokenizers, and
stemmers—all have measurable impact on effectiveness. This is particularly the case with web search,
where techniques for removing HTML tags, JavaScript, and boilerplate make a big difference [Roy
et al., 2018]. The additional challenge is that document cleaning includes many details that are
difficult to document in a traditional publication, making replicability difficult without access to
source code. See Lin et al. [2020a] for an effort to tackle this challenge via a common interchange
format for index structures. Finally, BM25 (like most ranking functions) has free parameters that
affect scoring behavior, and researchers often neglect to properly document these settings.
All of these issues contribute to differences in “BM25”, but previous studies have generally found
that the differences are not statistically significant. Nevertheless, in the context of text ranking with
transformers, since the BM25 rankings are used as input for further reranking, prudent evaluation
methodology dictates that researchers carefully control for these differences, for example with careful
ablation studies.
In addition to bag-of-words keyword search, it is also widely accepted practice in research papers
to present ranking results with query expansion using pseudo-relevance feedback as an additional
baseline. As discussed in Section 1.2.2, query expansion represents one main strategy for tackling
the vocabulary mismatch problem, to bring representations of queries and texts from the corpus
into closer alignment. Specifically, pseudo-relevance feedback is a widely studied technique that
has been shown to improve retrieval effectiveness on average; this is a robust finding supported
by decades of empirical evidence. Query expansion using the RM3 pseudo-relevance feedback
technique [Abdul-Jaleel et al., 2004], on top of an initial ranked list of documents scored by BM25,
is a popular choice (usually denoted as BM25 + RM3) [Lin, 2018, Yang et al., 2019b].
To summarize, it is common practice to compare neural ranking models against both a bag-of-words
baseline and a query expansion technique. Since most neural ranking models today (all of those
discussed in Section 3) act as rerankers over a list of candidates, these two baselines also serve as the
standard candidate generation approaches. In this way, we are able to isolate the contributions of the
neural ranking models.
A related issue worth discussing is the methodologically poor practice of comparisons to low baselines.
In a typical research paper, researchers might claim innovations based on beating some baseline
with a novel ranking model or approach. Such claims, however, need to be carefully verified by
considering the quality of the baseline, in that it is quite easy to demonstrate improvements over low
or poor quality baselines. This observation was made by Armstrong et al. [2009], who conducted
a meta-analysis of research papers between 1998 and 2008 from major IR research venues that
reported results on a diverse range of TREC test collections. Writing over a decade ago in 2009, they
concluded: “There is, in short, no evidence that ad-hoc retrieval technology has improved during the
past decade or more”. The authors attributed much of the blame to the “selection of weak baselines
that can create an illusion of incremental improvement” and “insufficient comparison with previous
results”. On the eve of the BERT revolution, Yang et al. [2019b] conducted a similar meta-analysis
and showed that pre-BERT neural ranking models were not any more effective than non-neural
ranking techniques, at least with limited amounts of training data; but see a follow-up by Lin [2019]
discussing BERT-based models. Nevertheless, the important takeaway message remains: when
assessing the effectiveness of a proposed ranking model, it is necessary to also assess the quality of
the comparison conditions, as it is always easy to beat a poor model.
There are, of course, numerous algorithmic and engineering details to building high-performance and
scalable keyword search engines. However, for the most part, readers of this survey—researchers and
practitioners interested in text ranking with transformers—can treat keyword search as a “black box”
using a number of open-source systems. From this perspective, keyword search is a mature technology
42
that can be treated as reliable infrastructure, or in modern “cloud terms”, as a service.72 It is safe to
assume that this infrastructure can robustly deliver high query throughput at low query latency on
arbitrarily large text collections; tens of milliseconds is typical, even for web-scale collections. As
we’ll see in Section 3.5, the inference latency of BERT and transformer models form the performance
bottleneck in current reranking architectures; candidate generation is very fast in comparison.
There are many choices for keyword search. Academic IR researchers have a long history of building
and sharing search systems, dating back to Cornell’s SMART system [Buckley, 1985] from the
mid 1980s. Over the years, many open-source search engines have been built to aid in research, for
example, to showcase new ranking models, query evaluation algorithms, or index organizations. An
incomplete list, past and present, includes (in an arbitrary order) Lemur/Indri [Metzler and Croft,
2004, Metzler et al., 2004], Galago [Cartright et al., 2012], Terrier [Ounis et al., 2006, Macdonald
et al., 2012], ATIRE [Trotman et al., 2012], Ivory [Lin et al., 2009], JASS [Lin and Trotman, 2015],
JASSv2 [Trotman and Crane, 2019], MG4J [Boldi and Vigna, 2005], Wumpus, and Zettair.73
Today, only a few organizations—mostly commercial web search engines such as Google and
Bing—deploy their own custom infrastructure for search. For most other organizations building
and deploying search applications—in other words, practitioners of information retrieval—the opensource Apache Lucene search library74 has emerged as the de facto standard solution, usually via either
OpenSearch,75 Elasticsearch,76 or Apache Solr,77 which are popular search platforms that use Lucene
at their cores. Lucene powers search in production deployments at numerous companies, including
Twitter, Bloomberg, Netflix, Comcast, Disney, Reddit, Wikipedia, and many more. Over the past
few years, there has been a resurgence of interest in using Lucene for academic research [Azzopardi
et al., 2017b,a], to take advantage of its broad deployment base and “production-grade” features; one
example is the Anserini toolkit [Yang et al., 2017, 2018].
2.9 Notes on Parlance
We conclude this section with some discussion of terminology used throughout this survey, where
we have made efforts to be consistent in usage. As search is the most prominent instance of text
ranking, our parlance is unsurprisingly dominated by information retrieval. However, since IR has
a long and rich history stretching back well over half a century, parlance has evolved over time,
creating inconsistencies and confusion, even among IR researchers. These issues are compounded by
conceptual overlap with neighboring sub-disciplines of computer science such as natural language
processing or data mining, which sometimes use different terms to refer to the same concept or use a
term in a different technical sense.
To start, IR researchers tend to favor the term “document collection” or simply “collection” over
“corpus” (plural: corpora), which is more commonly used by NLP researchers. We use these terms
interchangeably to refer to the “thing” containing the texts to be ranked.
In the academic literature (both in IR and across other sub-disciplines of computer science), the
meaning of the term “document” is overloaded: In one sense, it refers to the units of texts in the
raw corpus. For example, a news article from the Washington Post, a web page, a journal article,
a PowerPoint presentation, an email, etc.—these would all be considered documents. However,
“documents” can also refer generically to the “atomic” unit of ranking (or equivalently, the unit of
retrieval). For example, if Wikipedia articles are segmented into paragraphs for the purposes of
ranking, each paragraph might be referred to as a document. This may appear odd and may be a
source of confusion as a researcher might continue to discuss document ranking, even though the
documents to be ranked are actually paragraphs.
In other cases, document ranking is explicitly distinguished from passage ranking—for example,
there are techniques that retrieve documents from an inverted index (documents form the unit of
retrieval), segment those documents into passages, score the passages, and then accumulate the scores
to produce a document ranking, e.g., Callan [1994]. To add to the confusion, there are also examples
72Indeed, many of the major cloud vendors do offer search as a service.
73http://www.seg.rmit.edu.au/zettair/
74https://lucene.apache.org/
75https://opensearch.org/
76https://github.com/elastic/elasticsearch
77https://solr.apache.org/
43
where passages form the unit of retrieval, but passage scores are aggregated to rank documents,
e.g., Hearst and Plaunt [1993] and Lin [2009]. We attempt to avoid this confusion by using the
term “text ranking”, leaving the form of the text underspecified and these nuances to be recovered
from context. The compromise is that text ranking may sound foreign to a reader familiar with the
IR literature. However, text ranking more accurately describes applications in NLP, e.g., ranking
candidates in entity linking, as document ranking would sound especially odd in that context.
The information retrieval community often uses “retrieval” and “ranking” interchangeably, although
the latter is much more precise. They are not, technically, the same: it would be odd refer to boolean
retrieval as ranking, since such operations are manipulations of unordered sets. In a sense, retrieval is
more generic, as it can be applied to situations where no ranking is involved, for example, fetching
values from a key–value store. However, English lacks a verb that is more precise than to retrieve, in
the sense of “to produce a ranking of texts” from, say, an inverted index,78 and thus in cases where
there is little chance for confusion, we continue to use the verbs “retrieve” and “rank” as synonyms.
Next, discussions about the positions of results in a ranked list can be a source of confusion, since
rank monotonically increases but lower (numbered) ranks (hopefully) represent better results. Thus, a
phrase like “high ranks” is ambiguous between rank numbers that are large (e.g., a document at rank
1000) or documents that are “highly ranked” (i.e., high scores = low rank numbers = good results).
The opposite ambiguity occurs with the phrase “low ranks”. To avoid confusion, we refer to texts
that are at the “top” of the ranked list (i.e., high scores = low rank numbers = good results) and texts
that are near the “bottom” of the ranked list or “deep” in ranked list.
A note about the term “performance”: Although the meaning of performance varies across different
sub-disciplines of computer science, it is generally used to refer to measures related to speed such
as latency, throughput, etc. However, NLP researchers tend to use performance to refer to output
quality (e.g., prediction accuracy, perplexity, BLEU score, etc.). This can be especially confusing
in a paper (for example, about model compression) that also discusses performance in the speed
sense, because “better performance” is ambiguous between “faster” (e.g., lower inference latency)
and “better” (e.g., higher prediction accuracy). In the information retrieval literature, “effectiveness”
is used to refer to output quality,79 while “efficiency” is used to refer to properties such a latency,
throughput, etc.80 Thus, it is common to discuss effectiveness/efficiency tradeoffs. In this survey, our
use of terminology is more closely aligned with the parlance in information retrieval—that is, we
use effectiveness (as opposed to “performance”) as a catch-all term for output quality and we use
efficiency in the speed sense.
Finally, “reproducibility”, “replicability”, and related terms are often used in imprecise and confusing
ways. In the context of this survey, we are careful to use the relevant terms in the sense defined by
ACM’s Artifact Review and Badging Policy.81 Be aware that a previous version of the policy had the
meaning of “reproducibility” and “replicability” swapped, which is a source of great confusion.
We have found the following short descriptions to be a helpful summary of the differences:
• Repeatability: same team, same experimental setup
• Reproducibility: different team, same experimental setup
• Replicability: different team, different experimental setup
For example, if the authors of a paper have open-sourced the code to their experiments, and another
individual (or team) is able to obtain the results reported in their paper, we can say that the results
have be successfully reproduced. The definition of “same results” can be sometimes fuzzy, as it is
frequently difficult to arrive at exactly the same evaluation figures (say, nDCG@10) as the original
paper, especially in the context of experiments based on neural networks, due to issues such as
random seed selection, the stochastic nature of the optimizer, different versions of the underlying
software toolkit, and a host of other complexities. Generally, most researchers would consider a
78“To rank text from an inverted index” sounds very odd.
79Although even usage by IR researchers is inconsistent; there are still plenty of IR papers that use “performance”
to refer to output quality.
80Note that, however, efficiency means something very different in the systems community or the highperformance computing community.
81https://www.acm.org/publications/policies/artifact-review-and-badging-current
44
result to be reproducible as long as others were able to confirm the veracity of the claims at a high
level, even if the experimental results do not perfectly align.
If the individual (or team) was able to obtain the same results reported in a paper, but with an independent implementation, then we say that the findings are replicable. Here though, the definition of an
“independent implementation” can be somewhat fuzzy. For example, if the original implementation
was built using TensorFlow and the reimplementation used PyTorch, most researchers would consider
it a successful replication effort. But what about two different TensorFlow implementations where
there is far less potential variation? Would this be partway between reproduction and replication?
The answer isn’t clear.
The main point of this discussion is that while notions of reproducibility and replicability may seem
straightforward, there are plenty of nuance and complexities that are often swept under the rug. For
the interested reader, see Lin and Zhang [2020] for additional discussions of these issues.
Okay, with the stage set and all these terminological nuances out of the way, we’re ready to dive into
transformers for text ranking!
45
3 Multi-Stage Architectures for Reranking
The simplest and most straightforward formulation of text ranking is to convert the task into a text
classification problem, and then sort the texts to be ranked based on the probability that each item
belongs to the desired class. For information access problems, the desired class comprises texts that
are relevant to the user’s information need (see Section 2.2), and so we can refer to this approach as
relevance classification.
More precisely, the approach involves training a classifier to estimate the probability that each text
belongs to the “relevant” class, and then at ranking (i.e., inference) time sort the texts by those
estimates.82 This approach represents a direct realization of the Probability Ranking Principle,
which states that documents should be ranked in decreasing order of the estimated probability of
relevance with respect to the information need, first formulated by Robertson [1977]. Attempts
to build computational models that directly perform ranking using supervised machine-learning
techniques date back to the late 1980s [Fuhr, 1989]; see also Gey [1994]. Both these papers describe
formulations and adopt terminological conventions that would be familiar to readers today.
The first application of BERT to text ranking, by Nogueira and Cho [2019], used BERT in exactly
this manner. However, before describing this relevance classification approach in detail, we begin
the section with a high-level overview of BERT (Section 3.1). Our exposition is not meant to be a
tutorial: rather, our aim is to highlight the aspects of the model that are important for explaining its
applications to text ranking. Devlin et al. [2019] had already shown BERT to be effective for text
classification tasks, and the adaptation by Nogueira and Cho—known as monoBERT—has proven to
be a simple, robust, effective, and widely replicated model for text ranking. It serves as the starting
point for text ranking with transformers and provides a good baseline for subsequent ranking models.
The progression of our presentation takes the following course:
• We present a detailed study of monoBERT, starting with the basic relevance classification design
proposed by Nogueira and Cho [2019] (Section 3.2.1). Then:
– A series of contrastive and ablation experiments demonstrate monoBERT’s effectiveness
under different conditions, including the replacement of BERT with simple model variants
(Section 3.2.2). This is followed by a discussion of a large body of research that investigates
how BERT works (Section 3.2.3).
– The basic “recipe” of applying BERT (and other pretrained transformers) to perform a
downstream task is to start with a pretrained model and then fine-tune it further using labeled data from the target task. This process, however, is much more nuanced: Section 3.2.4
discusses many of these techniques, which are broadly applicable to transformer-based
models for a wide variety of tasks.
• The description of monoBERT introduces a key limitation of BERT for text ranking: its inability
to handle long input sequences, and hence difficulty in ranking texts whose lengths exceed the
designed model input (e.g., “full-length” documents such as news articles, scientific papers, and
web pages). Researchers have devised multiple solutions to overcome this challenge, which are
presented in Section 3.3. Three of these approaches—Birch [Akkalyoncu Yilmaz et al., 2019b],
BERT–MaxP [Dai and Callan, 2019b], and CEDR [MacAvaney et al., 2019a]—are roughly
contemporaneous and represent the “first wave” of transformer-based neural ranking models
designed to handle longer texts.
• After presenting a number of BERT-based ranking models, we turn our attention to discuss
the architectural context in which these models are deployed. A simple retrieve-and-rerank
approach can be elaborated into a multi-stage ranking architecture with reranker pipelines,
which Section 3.4 covers in detail.
• Finally, we describe a number of efforts that attempt to go beyond BERT, to build ranking
models that are faster (i.e., achieve lower inference latency), are better (i.e., obtain higher
ranking effectiveness), or realize an interesting tradeoff between effectiveness and efficiency
(Section 3.5). We cover ranking models that exploit knowledge distillation to train more compact
82Note that treating relevance as a binary property is already an over-simplification. Modeling relevance on
an ordinal scale (e.g., as nDCG does) represents an improvement, but whether a piece of text satisfies an
information need requires considerations from many facets; see discussion in Section 2.2.
46
… … … … … … … … …
[SEP]
EA
P8
E[CLS]
T[CLS]
[CLS]
E1
T1
A1
E2
T2
A2
E3
T3
A3
E4
T4
[SEP]
E5
T5
B1
E6
T6
B2
E7
T7
E[SEP]
T[SEP]
[CLS] t1 t2 t3 t4 t5 t6
EA EA EA EA EA EA EA
P0 P1 P2 P3 P4 P5 P6
+ + + + + + + +
+ + + + + + + +
Token
Embeddings
Segment
Embeddings
Position
Embeddings
t7
EA
P7
+
+
Illustration of BERT, showing composition of input embeddings. Redrawn from Devlin et al. (NAACL 2019)
By Jimmy Lin (jimmylin@uwaterloo.ca), released under Creative Commons Attribution 4.0 International (CC BY 4.0): https://creativecommons.org/licenses/by/4.0/
Figure 4: The architecture of BERT. Input vectors comprise the element-wise summation of token
embeddings, segment embeddings, and position embeddings. The output of BERT is a contextual
embedding for each input token. The contextual embedding of the [CLS] token is typically taken as
an aggregate representation of the entire sequence for classification-based downstream tasks.
student models and other transformer architectures, including ground-up redesign efforts and
adaptations of pretrained sequence-to-sequence models.
By concluding this section with efforts that attempt to go “beyond BERT”, we set up a natural
transition to ranking based on learned dense representations, which is the focus of Section 5.
3.1 A High-Level Overview of BERT
At its core, BERT (Bidirectional Encoder Representations from Transformers) [Devlin et al., 2019] is
a neural network model for generating contextual embeddings for input sequences in English, with a
multilingual variant (often called “mBERT”) that can process input in over 100 different languages.
Here we focus only on the monolingual English model, but mBERT has been extensively studied as
well [Wu and Dredze, 2019, Pires et al., 2019, Artetxe et al., 2020].
BERT takes as input a sequence of tokens (more specifically, input vector representations derived
from those tokens, more details below) and outputs a sequence of contextual embeddings, which
provide context-dependent representations of the input tokens.83 This stands in contrast to contextindependent (i.e., static) representations, which include many of the widely adopted techniques that
came before such as word2vec [Mikolov et al., 2013a] or GloVe [Pennington et al., 2014].
The input–output behavior of BERT is illustrated in Figure 4, where the input vector representations
are denoted as:
[E[CLS], E1, E2, . . . , E[SEP]], (9)
and the output contextual embeddings are denoted as:
[T[CLS], T1, T2, . . . , T[SEP]], (10)
after passing through a number of transformer encoder layers. In addition to the text to be processed,
input to BERT typically includes two special tokens, [CLS] and [SEP], which we explain below.
BERT can be seen as a more sophisticated model with the same aims as ELMo [Peters et al., 2018],
from which BERT draws many important ideas: the goal of contextual embeddings is to capture
complex characteristics of language (e.g., syntax and semantics) as well as how meanings vary
across linguistic contexts (e.g., polysemy). The major difference is that BERT takes advantage of
transformers, as opposed to ELMo’s use of LSTMs. BERT can be viewed as the “encoder half”
83The literature alternately refers to “contextual embeddings” or “contextualized embeddings”. We adopt the
former in this survey.
47
of the full transformer architecture proposed by Vaswani et al. [2017], which was designed for
sequence-to-sequence tasks (i.e., where both the input and output are sequences of tokens) such as
machine translation.
BERT is also distinguished from GPT [Radford et al., 2018], another model from which it traces
intellectual ancestry. If BERT can be viewed as an encoder-only transformer, GPT is the opposite: it
represents a decoder-only transformer [Liu et al., 2018a], or the “decoder half” of a full sequenceto-sequence transformer model. GPT is pretrained to predict the next word in a sequence based on
its past history; in contrast, BERT uses a different objective, which leads to an important distinction
discussed below. BERT and GPT are often grouped together (along with a host of other models) and
referred to collectively as pretrained language models, although this characterization is somewhat
misleading because, strictly speaking, a language model in NLP provides a probability distribution
over arbitrary sequences of text tokens; see, for example Chen and Goodman [1996]. In truth, coaxing
such probabilities out of BERT require a bit of effort [Salazar et al., 2020], and transformers in
general can do much more than “traditional” language models!
The significant advance that GPT and BERT represent over the original transformer formulation [Vaswani et al., 2017] is the use of self supervision in pretraining, whereas in contrast, Vaswani
et al. began with random initialization of model weights and proceeded to directly train on labeled
data, i.e., (input sequence, output sequence) pairs, in a supervised manner. This is an important
distinction, as the insight of pretraining based on self supervision is arguably the biggest game
changer in improving model output quality on a multitude of language processing tasks. The beauty
of self supervision is two-fold:
• Model optimization is no longer bound by the chains of labeled data. Self supervision means
that the texts provide their own “labels” (in GPT, the “label” for a sequence of tokens is the
next token that appears in the sequence), and that loss can be computed from the sequence
itself (without needing any other external annotations). Since labeled data derive ultimately
from human effort, removing the need for labels greatly expands the amount of data that can be
fed to models for pretraining. Often, computing power and available data instead become the
bottleneck [Kaplan et al., 2020].
• Models optimized based on one or more self-supervised objectives, without reference to any
specific task, provide good starting points for further fine-tuning with task-specific labeled
data. This led to the “first pretrain, then fine-tune” recipe of working with BERT and related
models, as introduced in Section 1. The details of this fine-tuning process are task specific
but experiments have shown that a modest amount of labeled data is sufficient to achieve a
high level of effectiveness. Thus, the same pretrained model can serve as the starting point for
performing multiple downstream tasks after appropriate fine-tuning.84
In terms of combining the two crucial ingredients of transformers and self supervision, GPT predated
BERT. However, they operationalize the insight in different ways. GPT uses a traditional language
modeling objective: given a corpus of tokens U = {u1, u2, . . . , un}, the objective is to maximize the
following likelihood:
L(U) = X
i
log P(ui
|ui−k, . . . , ui−1; Θ) (11)
where k is the context window size and the conditional probability is modeled by a transformer with
parameters Θ.
In contrast, BERT introduced the so-called “masked language model” (MLM) pretraining objective,
which is inspired by the Cloze task [Taylor, 1953], dating from over half a century ago. MLM is a
fancy name for a fairly simple idea, not much different from peek-a-boo games that adults play with
infants and toddlers: during pretraining, we randomly “cover up” (more formally, “mask”) a token
from the input sequence and ask the model to “guess” (i.e., predict) it, training with cross entropy
loss.85 The MLM objective explains the “B” in BERT, which stands for bidirectional: the model
is able to use both a masked token’s left and right contexts (preceding and succeeding contexts) to
make predictions. In contrast, since GPT uses a language modeling objective, it is only able to
84With adaptors [Houlsby et al., 2019], it is possible to greatly reduce the number of parameters required to
fine-tune the same “base” transformer for many different tasks.
85The actual procedure is a bit more complicated, but we refer the reader to the original paper for details.
48
E[CLS]
T[CLS]
[CLS]
E1
U1
A1
E2
U2
A2
E3
U3
A3
En-2
Un-2
An-2
En-1
Un-1
An-1
En
Un
An
E[SEP]
T[SEP]
[SEP]
Class Label
…
…
… … … … … … … … …
Illustration of BERT for single-sentence classification tasks. Redrawn from Devlin et al. (NAACL 2019)
By Jimmy Lin (jimmylin@uwaterloo.ca), released under Creative Commons Attribution 4.0 International (CC BY 4.0): https://creativecommons.org/licenses/by/4.0/
Sentence
…
(a) Single-Input Classification
E[CLS]
T[CLS]
[CLS]
E1
U1
A1
En
Un
An
E[SEP1]
T[SEP1]
[SEP]
F1
V1
B1
Fm
Vm
Bm
E[SEP2]
T[SEP2]
[SEP]
Class Label
…
…
…
…
… … … … … … … … …
Illustration of BERT for two-sentence classification tasks. Redrawn from Devlin et al. (NAACL 2019)
By Jimmy Lin (jimmylin@uwaterloo.ca), released under Creative Commons Attribution 4.0 International (CC BY 4.0): https://creativecommons.org/licenses/by/4.0/
Sentence 1 Sentence 2
… …
(b) Two-Input Classification
E[CLS]
T[CLS]
[CLS]
E1
U1
A1
E2
U2
A2
E3
U3
A3
En-2
Un-2
An-2
En-1
Un-1
An-1
En
Un
An
E[SEP]
T[SEP]
[SEP]
O O B-PER O O O
…
…
… … … … … … … … …
Illustration of BERT for single-sentence sequence labeling tasks. Redrawn from Devlin et al. (NAACL 2019)
By Jimmy Lin (jimmylin@uwaterloo.ca), released under Creative Commons Attribution 4.0 International (CC BY 4.0): https://creativecommons.org/licenses/by/4.0/
Sentence
…
(c) Single-Input Token Labeling
E[CLS]
T[CLS]
[CLS]
E1
U1
A1
En
Un
An
E[SEP1]
T[SEP1]
[SEP]
F1
V1
B1
Fm
Vm
Bm
E[SEP2]
T[SEP2]
[SEP]
Start/End Span
…
…
…
…
… … … … … … … … …
Illustration of BERT for two-sentence sequence labeling tasks. Redrawn from Devlin et al. (NAACL 2019)
By Jimmy Lin (jimmylin@uwaterloo.ca), released under Creative Commons Attribution 4.0 International (CC BY 4.0): https://creativecommons.org/licenses/by/4.0/
Question Candidate
… …
(d) Two-Input Token Labeling
Figure 5: Illustration of how BERT is used for different NLP tasks. The inputs are typically, but not
always, sentences.
use preceding tokens (i.e., the left context in a language written from left to right; formally, this is
called “autoregressive”). Empirically, bidirectional modeling turns out to make a big difference—as
demonstrated, for example, by higher effectiveness on the popular GLUE benchmark.
While the MLM objective was an invention of BERT, the idea of pretraining has a long history.
ULMFiT (Universal Language Model Fine-tuning) [Howard and Ruder, 2018] likely deserves the
credit for popularizing the idea of pretraining using language modeling objectives and then fine-tuning
on task-specific data—the same procedure that has become universal today—but the application of
pretraining in NLP can be attributed to Dai and Le [2015]. Tracing the intellectual origins of this idea
even back further, the original inspiration comes from the computer vision community, dating back at
least a decade [Erhan et al., 2009].
Input sequences to BERT are usually tokenized with the WordPiece tokenizer [Wu et al., 2016],
although BPE [Sennrich et al., 2016] is a common alternative, used in GPT as well as RoBERTa [Liu
et al., 2019c]. These tokenizers have the aim of reducing the vocabulary space by splitting words
into “subwords”, usually in an unsupervised manner. For example, with the WordPiece vocabulary
used by BERT,86 “scrolling” becomes “scroll” + “##ing”. The convention of prepending two hashes
(##) to a subword indicates that it is “connected” to the previous subword (i.e., in a language usually
written with spaces, there is no space between the current subword and the previous one).
For the most part, any correspondence between “wordpieces” and linguistically meaningful units
should be considered accidental. For example, “walking” and “talking” are not split into subwords,
and “biking” is split into “bi” + “##king”, which obviously do not correspond to morphemes. Even
more extreme examples are “biostatistics” (“bio” + “##sta” + “##tist” + “##ics”) and “adversarial”
(“ad”, “##vers”, “##aria”, “##l”). Nevertheless, the main advantage of WordPiece tokenization (and
related methods) is that a relatively small vocabulary (e.g., 30,000 wordpieces) is sufficient to model
large, naturally-occurring corpora that may have millions of unique tokens (based on a simple method
like tokenization by whitespace).
86Specifically, bert-base-cased.
49
While BERT at its core converts a sequence of input embeddings into a sequence of corresponding
contextual embeddings, in practice it is primarily applied to four types of tasks (see Figure 5):
• Single-input classification tasks, for example, sentiment analysis on a single segment of text.
BERT can also be used for regression, but we have decided to focus on classification to be
consistent with the terminology used in the original paper.
• Two-input classification tasks, for example, detecting if two sentences are paraphrases. In
principle, regression is possible here also.
• Single-input token labeling tasks, for example, named-entity recognition. For these tasks, each
token in the input is assigned a label, as opposed to single-input classification, where the label
is assigned to the entire sequence.
• Two-input token labeling tasks, e.g., question answering (or more precisely, machine reading
comprehension), formulated as the task of labeling the begin and end positions of the answer
span in a candidate text (typically, the second input) given a question (typically, the first input).
The first token of every input sequence to BERT is a special token called [CLS]; the final representation of this special token is typically used for classification tasks. The [CLS] token is followed by the
input or inputs: these are typically, but not always, sentences—indeed, as we shall see later, the inputs
comprise candidate texts to be ranked, which are usually longer than individual sentences. For tasks
involving a single input, another special delimiter token [SEP] is appended to the end of the input
sequence. For tasks involving two inputs, both are packed together into a single contiguous sequence
of tokens separated by the [SEP] token, with another [SEP] token appended to the end. For token
labeling tasks over single inputs (e.g., named-entity recognition), the contextual embedding of the
first subword is typically used to predict the correct label that should be assigned to the token (e.g.,
in a standard BIO tagging scheme). Question answering or machine reading comprehension (more
generically, token labeling tasks involving two inputs) is treated in a conceptually similar manner,
where the model attempts to label the beginning and end positions of the answer span.
To help the model understand the relationship between different segments of text (in the two-input
case), BERT is also pretrained with a “next sentence prediction” (NSP) task, where the model learns
segment embeddings, a kind of indicator used to differentiate the two inputs. During pretraining,
after choosing a sentence from the corpus (segment A), half of the time the actual next sentence from
the corpus is selected for inclusion in the training instance (as segment B), while the other half of the
time a random sentence from the corpus is chosen instead. The NSP task is to predict whether the
second sentence indeed follows the first. Devlin et al. [2019] hypothesized that NSP pretraining is
important for downstream tasks, especially those that take two inputs. However, subsequent work
by Liu et al. [2019c] questioned the necessity of NSP; in fact, on a wide range of NLP tasks, they
observed no effectiveness degradation in models that lacked such pretraining.
Pulling everything together, the input representation to BERT for each token comprises three components, shown at the bottom of Figure 4:
• the learned token embedding of the token from the WordPiece tokenizer [Wu et al., 2016] (i.e.,
lookup from a dictionary);
• the segment embedding, which is a learned embedding indicating whether the token belongs
to the first input (A) or the second input (B) in tasks involve two inputs (denoted EA and EB)
in Figure 4;
• the position embedding, which is a learned embedding capturing the position of the token in a
sequence, allowing BERT to reason about the linear sequence of tokens (see Section 3.2 for
more details).
The final input representation to BERT for each token comprises the element-wise summation of its
token embedding, segment embedding, and position embedding. It is worth emphasizing that the
three embedding components are summed, not assembled via vector concatenation (this is a frequent
point of confusion).
The representations comprising the input sequence to BERT are passed through a stack of transformer
encoder layers to produce the output contextual embeddings. The number of layers, the hidden
dimension size, and the number of attention heads are hyperparameters in the model architecture.
50
Size Layers Hidden Size Attention Heads Parameters
Tiny 2 128 2 4M
Mini 4 256 4 11M
Small 4 512 4 29M
Medium 8 512 8 42M
Base 12 768 12 110M
Large 24 1024 16 340M
Table 4: The hyperparameter settings of various pretrained BERT configurations. Devlin et al. [2019]
presented BERTBase and BERTLarge, the two most commonly used configurations today; other model
sizes by Turc et al. [2019] support explorations in effectiveness/efficiency tradeoffs.
However, there are a number of “standard configurations”. While the original paper [Devlin et al.,
2019] presented only the BERTBase and BERTLarge configurations, with 12 and 24 transformer
encoder layers, respectively, in later work Turc et al. [2019] pretrained a greater variety of model
sizes with the help of knowledge distillation; these are all shown in Table 4. In general, size correlates
with effectiveness in downstream tasks, and thus these configurations are useful for exploring
effectiveness/efficiency tradeoffs (more in Section 3.5.1).
We conclude our high-level discussion of BERT by noting that its popularity is in no small part
due to wise decisions by the authors (and approval by Google) to not only open source the model
implementation, but also publicly release pretrained models (which are quite computationally expensive to pretrain from scratch). This led to rapid reproduction and replication of the impressive
results reported in the original paper and provided the community with a reference implementation
to build on. Today, the Transformers library87 by Hugging Face [Wolf et al., 2020] has emerged
as the de facto standard implementation of BERT as well as many transformer models, supporting
both PyTorch [Paszke et al., 2019] and TensorFlow [Abadi et al., 2016], the two most popular deep
learning libraries today.
While open source (sharing code) and open science (sharing data and models) have become the norms
in recent years, as noted by Lin [2019], the decision to share BERT wasn’t necessarily a given. For
example, Google could have elected not to share the source code or the pretrained models. There are
many examples of previous Google innovation that were shared in academic papers only, without a
corresponding open-source code release; MapReduce [Dean and Ghemawat, 2004] and the Google
File System [Ghemawat et al., 2003] are two examples that immediately come to mind, although
admittedly there are a number of complex considerations that factor into the binary decision to release
code or not. In cases where descriptions of innovations in papers were not accompanied by source
code, the broader community has needed to build its own open-source implementations from scratch
(Hadoop in the case of MapReduce and the Google File System). This has generally impeded overall
progress in the field because it required the community to rediscover many “tricks” and details from
scratch that may not have been clear or included in the original paper. The community is fortunate that
things turned out the way they did, and Google should be given credit for its openness. Ultimately,
this led to an explosion of innovation in nearly all aspect of natural language processing, including
applications to text ranking.
3.2 Simple Relevance Classification: monoBERT
The task of relevance classification is to estimate a score si quantifying how relevant a candidate text
di
is to a query q, which we denote as:
P(Relevant = 1|di
, q). (12)
Before describing the details of how BERT is adapted for this task, let us first address the obvious
question of where the candidate texts come from: Applying inference to every text in a corpus for
every user query is (obviously) impractical from the computational perspective, not only due to costly
neural network inference but also the linear growth of query latency with respect to corpus size.
While such a brute-force approach can be viable for small corpora, it quickly runs into scalability
challenges. It is clearly impractical to apply BERT inference to, say, a million texts for every query.88
87https://github.com/huggingface/transformers
88Even if you’re Google!
51
Inverted
Index
Initial
Retrieval
Texts
Ranked List
Candidate
Texts
Queries
Reranker
Inverted
Index
Initial
Retrieval
Texts
Candidate
Texts
Queries
Reranker
Reranked
Candidates
Reranker … Reranker
Ranked List
Figure 6: A retrieve-and-rerank architecture, which is the simplest instantiation of a multi-stage
ranking architecture. In the candidate generation stage (also called initial retrieval or first-stage
retrieval), candidate texts are retrieved from the corpus, typically with bag-of-words queries against
inverted indexes. These candidates are then reranked with a transformer-based model such as
monoBERT.
Although architectural alternatives are being actively explored by many researchers (the topic of
Section 5), most applications of BERT for text ranking today adopt a retrieve-and-rerank approach,
which is shown in Figure 6. This represents the simplest instance of a multi-stage ranking architecture,
which we detail in Section 3.4. In most designs today, candidate texts are identified from the corpus
using keyword search, usually with bag-of-words queries against inverted indexes (see Section 2.8).
This retrieval stage is called candidate generation, initial retrieval, or first-stage retrieval, the output
of which is a ranked list of texts, typically ordered by a scoring function based on exact term
matches such as BM25 (see Section 1.2). This retrieve-and-rerank approach dates back to at least the
1960s [Simmons, 1965] and this architecture is mature and widely adopted (see Section 3.4).
BERT inference is then applied to rerank these candidates to generate a score si for each text di
in
the candidates list. The BERT-derived scores may or may not be further combined or aggregated
with other relevance signals to arrive at the final scores used for reranking. Nogueira and Cho [2019]
used the BERT scores directly to rerank the candidates, thus treating the candidate texts as sets, but
other approaches take advantage of, for example, the BM25 scores from the initial retrieval (more
details later). Naturally, we expect that the ranking induced by these final scores have higher quality
than the scores from the initial retrieval stage (for example, as measured by the metrics discussed in
Section 2.5). Thus, many applications of BERT to text ranking today (including everything we present
in this section) are actually performing reranking. However, for expository clarity, we continue to
refer to text ranking unless the distinction between ranking and reranking is important (see additional
discussion in Section 2.2).
This two-stage retrieve-and-rerank design also explains the major difference between Nogueira and
Cho [2019] and the classification tasks described in the original BERT paper. Devlin et al. [2019]
only tackled text classification tasks that involve comparisons of two input texts (e.g., paraphrase
detection), as opposed to text ranking, which requires multiple inferences. Nogueira and Cho’s
original paper never gave their model a name, but Nogueira et al. [2019a] later called the model
“monoBERT” to establish a contrast with another model they proposed called “duoBERT” (described
in Section 3.4.1). Thus, throughout this survey we refer to this basic model as monoBERT.
3.2.1 Basic Design of monoBERT
The complete monoBERT ranking model is shown in Figure 7. For the relevance classification task,
the model takes as input a sequence comprised of the following:
[[CLS], q, [SEP], di
, [SEP]], (13)
where q comprises the query tokens and di comprises tokens from the candidate text to be scored.
This is the same input sequence configuration as in Figure 5(b) for classification tasks involving two
inputs. Note that the query tokens are taken verbatim from the user (or from a test collection); this
detail will become important when we discuss the effects of feeding BERT different representations of
the information need (e.g., “title” vs. “description” fields in TREC topics) in Section 3.3. Additionally,
the segment A embedding is added to query tokens and the segment B embedding is added to the
candidate text (see Section 3.1). The special tokens [CLS] and [SEP] are exactly those defined by
52
E[CLS]
T[CLS]
[CLS]
E1
U1
q1
E2
U2
q2
E3
U3
q3
E[SEP1]
T[SEP1]
[SEP]
F1
V1
d1
F2
V2
d2
Fm
Vm
dm
E[SEP2]
T[SEP2]
[SEP]
…
…
query text
s
EA EA EA EA EA EB EB … EB EB
P0 P1 P2 P3 P4 P5 P6 … Pm+4 Pm+5
+ + + + + + + + +
+ + + + + + + + +
Token
Embeddings
Segment
Embeddings
Position
Embeddings
… … … … … … … … … …
…
Figure 7: The monoBERT ranking model adapts BERT for relevance classification by taking as
input the query and a candidate text to be scored (surrounded by appropriate special tokens). The
input vector representations comprise the element-wise summation of token embeddings, segment
embeddings, and position embeddings. The output of the BERT model is a contextual embedding for
each input token. The final representation of the [CLS] token is fed to a fully-connected layer that
produces the relevance score s of the text with respect to the query.
BERT. The final contextual representation of the [CLS] token is then used as input to a fully-connected
layer that generates the document score s (more details below).
Collectively, this configuration of the input sequence is sometimes called the “input template” and
each component has (a greater or lesser) impact on effectiveness; we empirically examine variations
in Section 3.2.2. This general style of organizing task inputs (query and candidate texts) into an input
template to feed to a transformer for inference is called a “cross-encoder”. This terminology becomes
particularly relevant in Section 5, when it is contrasted with a “bi-encoder” design where inference is
performed on queries and texts from the corpus independently.
Since BERT was pretrained with sequences of tokens that have a maximum length of 512, tokens in
an input sequence that is longer will not have a corresponding position embedding, and thus cannot
be meaningfully fed to the model. Without position embeddings, BERT has no way to model the
linear order and relative positions between tokens, and thus the model will essentially treat input
tokens as a bag of words. In the datasets that Nogueira and Cho explored, this limitation was not an
issue because the queries and candidate texts were shorter than the maximum length (see Figure 3 in
Section 2.7).
However, in the general case, the maximum sequence length of 512 tokens presents a challenge to
using BERT for ranking longer texts. We set aside this issue for now and return to discuss solutions
in Section 3.3, noting, however, that the simplest solution is to truncate the input. Since transformers
exhibit quadratic complexity in both time and space with respect to the input length, it is common
practice in production deployments to truncate the input sequence to a length that is shorter than the
maximum length to manage latency. This might be a practical choice independent of BERT’s input
length limitations.
An important detail to note here is that the length limitation of BERT is measured in terms of the
WordPiece tokenizer [Wu et al., 2016]. Because many words are split into subwords, the number of
actual WordPiece tokens is always larger than the output of a simple tokenization method such as
splitting on whitespace. The practical consequence of this is that analyses of document lengths based
on whitespace tokenization such as Figure 3 in Section 2.7, or tokenization used by standard search
53
engines that include stopword removal, can only serve as a rough guide of whether a piece of text
will “fit into” BERT.
The sequence of input tokens constructed from the query and a candidate text is then passed to
BERT, which produces a contextual vector representation for each token (exactly as the model was
designed to do). In monoBERT, the contextual representation of the [CLS] token (TCLS) as input to a
single-layer, fully-connected neural network to obtain a probability si
that the candidate di
is relevant
to q. The contextual representations of the other tokens are not used by monoBERT, but later we will
discuss models that do take advantage of those representations. More formally:
P(Relevant = 1|di
, q) = si
∆= softmax(T[CLS]W + b)1, (14)
where T[CLS] ∈ R
D, D is the model embedding dimension, W ∈ R
D×2
is a weight matrix, b ∈ R
2
is
a bias term, and softmax(·)i denotes the i-th element of the softmax output. Since the last dimension
of the matrix W is two, the softmax output has two dimensions (that is, the single-layer neural
network has two output neurons), one for each class, i.e., “relevant” and “non-relevant”.
BERT and the classification layer together comprise the monoBERT model. Following standard
practices, the entire model is trained end-to-end for the relevance classification task using crossentropy loss:
L = −
X
j∈Jpos
log(sj ) −
X
j∈Jneg
log(1 − sj ), (15)
where Jpos is the set of indexes of the relevant candidates and Jneg is the set of indexes of the
non-relevant candidates, which is typically part of the training data. Since the loss function takes
into account only one candidate text at a time, this can be characterized as belonging to the family
of pointwise learning-to-rank methods [Liu, 2009, Li, 2011]. We refer the interested reader to the
original paper by Nogueira and Cho for additional details, including hyperparameter settings.
To be clear, “training” monoBERT starts with a pretrained BERT model, which can be downloaded
from a number of sources such as the Hugging Face Transformers library [Wolf et al., 2020]. This
is often referred to as a “model checkpoint”, which encodes a specific set of model parameters
that capture the results of pretraining. From this initialization, the model is then fine-tuned with
task-specific labeled data, in our case, queries and relevance judgments. This “recipe” has emerged as
the standard approach of applying BERT to perform a wide range of tasks, and ranking is no exception.
In the reminder of this survey, we take care to be as precise as possible, distinguishing pretraining
from fine-tuning;89 Section 3.2.4 introduces additional wrinkles such as “further pretraining” and
“pre–fine-tuning”. However, we continue to use “training” (in a generic sense) when none of these
terms seem particularly apt.90
Before presenting results, it is worthwhile to explicitly point out two deficiencies of this approach to
monoBERT training:
• The training loss makes no reference to the metric that is used to evaluate the final ranking
(e.g., MAP), since each training example is considered in isolation; this is the case with all
pointwise approaches. Thus, optimizing cross-entropy for classification may not necessarily
improve an end-to-end metric such as mean average precision; in the context of ranking, this
was first observed by Morgan et al. [2004], who called this phenomenon “metric divergence”.
In practice, though, more accurate relevance classification generally leads to improvements as
measured by ranking metrics, and ranking metrics are often correlated with each other, e.g.,
improving MRR tends to improve MAP and vice versa.
• Texts that BERT sees at inference (reranking) time are different from examples fed to it during
training. During training, examples are taken directly from labeled examples, usually as part
of an information retrieval test collection. In contrast, at inference time, monoBERT sees
candidates ranked by BM25 (for example), which may or may not correspond to how the
training examples were selected to begin with, and in some cases, we have no way of knowing
since this detail may not have been disclosed by the creators of the test collection. Typically,
89And indeed, according to a totally scientific poll, this is what the interwebs suggest: https://twitter.com/
lintool/status/1375064796912087044.
90For example, it seems odd to use “fine-tuning” when referring to a model that uses a pretrained BERT as a
component, e.g., “to fine-tune a CEDR model” (see Section 3.3.3).
54
MS MARCO Passage
Development Test
Method MRR@10 Recall@1k MRR@10
(1) IRNet (best pre-BERT) 0.278 - 0.281
(2a) BM25 (Microsoft Baseline, k = 1000) 0.167 - 0.165
(2b) + monoBERTLarge [Nogueira and Cho, 2019] 0.365 - 0.359
(2c) + monoBERTBase [Nogueira and Cho, 2019] 0.347 - -
(3a) BM25 (Anserini, k = 1000) 0.187 0.857 0.190
(3b) + monoBERTLarge [Nogueira et al., 2019a] 0.372 0.857 0.365
(4a) BM25 + RM3 (Anserini, k = 1000) 0.156 0.861 -
(4b) + monoBERTLarge 0.374 0.861 -
Table 5: The effectiveness of monoBERT on the MS MARCO passage ranking test collection.
during training, monoBERT is exposed to fewer candidates per query than at inference time, and
thus the model may not accurately learn an accurate distribution of first-stage retrieval scores
across a pool of candidates varying in quality. Furthermore, the model usually does not see a
realistic distribution of positive and negative examples. In some datasets, for example, positive
and negative examples are balanced (i.e., equal numbers), so monoBERT is unable to accurately
estimate the prevalence of relevant texts (i.e., build a prior) in BM25-scored texts; typically, far
less than half of the texts from first-stage retrieval are relevant.
Interestingly, even without explicitly addressing these two issues, the simple training process described
above yields a relevance classifier that works well as a ranking model in practice.91
Results. The original paper by Nogueira and Cho [2019] evaluated monoBERT on two datasets: the
MS MARCO passage ranking test collection and the dataset from the Complex Answer Retrieval
(CAR) Track at TREC 2017. We focus here on results from MS MARCO, the more popular of the
two datasets, shown in Table 5. In addition to MRR@10, which is the official metric, we also report
recall at cutoff 1000, which helps to quantify the upper bound effectiveness of the retrieve-and-rerank
strategy. That is, if first-stage retrieval fails to return relevant passages, the reranker cannot conjure
relevant results out of thin air. Since we do not have access to relevance judgments for the test set, it
is only possible to compute recall for the development set.
The original monoBERT results, copied from Nogueira and Cho [2019] as row (2b) in Table 5, was
based on reranking baseline BM25 results provided by Microsoft, row (2a), with BERTLarge. This
is the result that in January 2019 kicked off the “BERT craze” for text ranking, as we’ve already
discussed in Section 1.2. The effectiveness of IRNet in row (1), the best system right before the
introduction of monoBERT, is also copied from Table 1. The effectiveness of ranking with BERTBase
is shown in row (2c), also copied from the original paper. We see that, as expected, a larger model
yields higher effectiveness. Nogueira and Cho [2019] did not compute recall, and so the figures are
not available for the conditions in rows (2a)–(2c).
Not all BM25 implementations are the same, as discussed in Section 2.8. The baseline BM25
results from Anserini (at k = 1000), row (3a), is nearly two points higher in terms of MRR@10
than the results provided by Microsoft’s BM25 baseline, row (2a). Reranking Anserini results
using monoBERT is shown in row (3b), taken from Nogueira et al. [2019a], a follow-up paper;
note that reranking does not change recall. We see that improvements to first-stage retrieval do
translate into more effective reranked results, but the magnitude of the improvement is not as large
as the difference between Microsoft’s BM25 and Anserini’s BM25. The combination of Anserini
BM25 + monoBERTLarge, row (3b), provides a solid baseline for comparing BERT-based reranking
models. These results can be reproduced with PyGaggle,92 which provides the current reference
implementation of monoBERT recommended by the model’s authors.
91Many feature-based learning-to-rank techniques [Liu, 2009, Li, 2011] are also quite effective without explicitly
addressing these issues, and so this behavior of BERT is perhaps not surprising.
92http://pygaggle.ai/
55
1 2.5 10 530
0.1
0.15
0.2
0.25
0.3
0.35
0.4
# relevant query–passage training instances (thousands)
MRR@10
BERT-BASE
BM25
monoBERTBase Effectiveness vs. Training Data Size on MS MARCO Passage
Figure 8: The effectiveness of monoBERTBase on the development set of the MS MARCO passage
ranking test collection varying the amount of training data used to fine-tune the model and reranking
k = 1000 candidate texts provided by first-stage retrieval using BM25. Results report means and
95% confidence intervals over five trials.
3.2.2 Exploring monoBERT
To gain a better understanding of how monoBERT works, we present a series of additional experiments that examine the effectiveness of the model under different contrastive and ablation settings.
Specifically, we investigate the following questions:
1. How much data is needed to train an effective model?
2. What is the effect of different candidate generation approaches?
3. How does retrieval depth k impact effectiveness?
4. Do exact match scores from first-stage retrieval contribute to overall effectiveness?
5. How important are different components of the input template?
6. What is the effect of swapping out BERT for another model that is a simple variant of BERT?
We answer each of these questions in turn, and then move on to discuss efforts that attempt to
understand why the model “works” so well.
Effects of Training Data Size. How much data do we need to train an effective monoBERT model?
The answer to this first question is shown in Figure 8, with results taken from Nogueira et al. [2020].
In these experiments, BERTBase was fine-tuned with 1K, 2.5K, and 10K positive query–passage
instances and an equal number of negative instances sampled from the training set of the MS
MARCO passage ranking test collection. Effectiveness on the development set is reported in terms of
MRR@10 with the standard setting of reranking k = 1000 candidate texts provided by Anserini’s
BM25; note that the x-axis is in log scale. For the sampled conditions, the experiment was repeated
five times, and the plot shows the 95% confidence intervals. The setting that uses all training instances
was only run once due to computational costs. Note that these figures come from a different set
of experimental trials than the results reported in the previous section, and thus MRR@10 from
fine-tuning with all data is slightly different from the comparable condition in Table 5. The dotted
horizontal black line shows the effectiveness of BM25 without any reranking.
As we expect, effectiveness improves as monoBERT is fine-tuned with more data. Interestingly, in a
“data poor” setting, that is, without many training examples, monoBERT actually performs worse
than BM25; this behavior has been noted by other researchers as well [Zhang et al., 2020g, Mokrii
56
TREC 2019 DL Passage
Method nDCG@10 MAP Recall@1k
(3a) BM25 (Anserini, k = 1000) 0.5058 0.3013 0.7501
(3b) + monoBERTLarge 0.7383 0.5058 0.7501
(4a) BM25 + RM3 (Anserini, k = 1000) 0.5180 0.3390 0.7998
(4b) + monoBERTLarge 0.7421 0.5291 0.7998
Table 6: The effectiveness of monoBERT on the TREC 2019 Deep Learning Track passage ranking
test collection, where the row numbers are consistent with Table 5.
et al., 2021]. As a rough point of comparison, the TREC 2019 Deep Learning Track passage ranking
test collection comprises approximately 9K relevance judgments (both positive and negative); see
Table 3. This suggests that monoBERT is quite “data hungry”: with 20K total training instances,
monoBERT barely improves upon the BM25 baseline. The log–linear increase in effectiveness as a
function of data size is perhaps not surprising, and consistent with previous studies that examined the
effects of training data size [Banko and Brill, 2001, Brants et al., 2007, Kaplan et al., 2020].
Effects of Candidate Generation. Since monoBERT operates by reranking candidates from firststage retrieval, it makes sense to investigate its impact on end-to-end effectiveness. Here, we examine
the effects of query expansion using pseudo-relevance feedback, which is a widely studied technique
for improving retrieval effectiveness on average (see Section 2.8). The effectiveness of keyword
retrieval using BM25 + RM3, a standard pseudo-relevance feedback baseline, is presented in row
(4a) of Table 5, with the implementation in Anserini. We see that MRR@10 decreases with pseudorelevance feedback, although there isn’t much difference in terms of recall. Further reranking with
BERT, shown in row (4b), yields MRR@10 that is almost the same as reranking BM25 results, shown
in row (3b). Thus, it appears that starting with worse quality candidates in terms of MRR@10 (BM25
+ RM3 vs. BM25), monoBERT is nevertheless able to identify relevant texts and bring them up into
top-ranked positions.
What’s going on here? These unexpected results can be attributed directly to artifacts of the relevance
judgments in the MS MARCO passage ranking test collection. It is well known that pseudo-relevance
feedback has a recall enhancing effect, since the expanded query is able to capture additional terms
that may appear in relevant texts. However, on average, there is only one relevant passage per query
in the MS MARCO passage relevance judgments; we have previously referred to these as sparse
judgments (see Section 2.7). Recall that unjudged texts are usually treated as not relevant (see
Section 2.5), as is the case here, so a ranking technique is unlikely to receive credit for improving
recall. Thus, due to the sparsity of judgments, the MS MARCO passage ranking test collection
appears to be limited in its ability to detect effectiveness improvements from pseudo-relevance
feedback.
We can better understand these effects by instead evaluating the same experimental conditions, but
with the TREC 2019 Deep Learning Track passage ranking test collection, which has far fewer
topics, but many more judged passages per topic (“dense judgments”, as described in Section 2.7).
These results are shown in Table 6, where the rows have been numbered in the same manner as
Table 5. We can see that these results support our explanation above: in the absence of BERT-based
reranking, pseudo-relevance feedback does indeed increase effectiveness, as shown by row (3a)
vs. row (4a). In particular, recall increases by around five points. The gain in nDCG@10 is more
modest than the gain in MAP because, by definition, nDCG@10 is only concerned with the top 10
hits, and the recall-enhancing effects of RM3 have less impact in improving the top of the ranked
list. Furthermore, an increase in the quality of the candidates does improve end-to-end effectiveness
after reranking, row (3b) vs. row(4b), although the magnitude of the gain is smaller than the impact
of pseudo-relevance feedback over simple bag-of-word queries. An important takeaway here is the
importance of recognizing the limitations of a particular evaluation instrument (i.e., the test collection)
and when an experiment exceeds its assessment capabilities.
Effects of Reranking Depth. Within a reranking setup, how does monoBERT effectiveness change
as the model is provided with more candidates? This question is answered in Figure 9, where we
show end-to-end effectiveness (MRR@10) of monoBERT with BM25 supplying different numbers
of candidates to rerank. It is no surprise that end-to-end effectiveness increases as retrieval depth k
57
10 100 1,000 10,000 50,000
0.26
0.28
0.3
0.32
0.34
0.36
0.38
0.4
Number of Candidate Documents
MRR@10
monoBERTLarge Effectiveness vs. Reranking Depth on MS MARCO Passage
Figure 9: The effectiveness of monoBERTLarge on the development set of the MS MARCO passage
ranking test collection varying the number of candidate documents k provided by first-stage retrieval
using BM25. End-to-end effectiveness grows with reranking depth.
increases, although there is clearly diminishing returns: going from 1000 hits to 10000 hits increases
MRR@10 from 0.372 to 0.377. Further increasing k to 50000 does not measurably change MRR@10
at all (same value). Due to computation costs, experiments beyond 50000 hits were not performed.
Quite interestingly, the effectiveness curve does not appear to be concave. In other words, it is not the
case (at least out to 50000 hits) that effectiveness decreases with more candidates beyond a certain
point. This behavior might be plausible because we are feeding BERT increasingly worse results,
at least from the perspective of BM25 scores. However, it appears that BERT is not “confused” by
such texts. Furthermore, these results confirm that first-stage retrieval serves primarily to increase
computational efficiency (i.e., discarding obviously non-relevant texts), and that there are few relevant
texts that have very low BM25 exact match scores.
Since latency increases linearly with the number of candidates processed (in the absence of intraquery parallelism), this finding also has important implications for real-world deployments: system
designers should simply select the largest k practical given their available hardware budget and
latency targets. There does not appear to be any danger in considering k values that are “too large”
(which would be the case if the effectiveness curve were concave, thus necessitating more nuanced
tuning to operate at the optimal setting). In other words, the tradeoff between effectiveness and
latency appears to be straightforward to manage.
Effects of Combining Exact Match Signals. Given the above results, a natural complementary
question is the importance of exact match signals (e.g., BM25 scores) to end-to-end effectiveness. One
obvious approach to combining evidence from initial BM25 retrieval scores and monoBERT scores is
linear interpolation, whose usage in document ranking dates back to at least the 1990s [Bartell et al.,
1994]:
si
∆= α · sˆBM25 + (1 − α) · sBERT, (16)
where si
is the final document score, sˆBM25 is the normalized BM25 score, sBERT is the monoBERT
score, and α ∈ [0..1] is a weight the indicates their relative importance. Since monoBERT scores are
sBERT ∈ [0, 1], we also normalize BM25 scores to be in the same range via linear scaling:
sˆBM25 =
sBM25 − smin
smax − smin
, (17)
where sBM25 is the original score, sˆBM25 is the normalized score, and smax and smin are the maximum
and minimum scores, respectively, in the ranked list.
58
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.2
0.25
0.3
0.35
0.4
α
MRR@10
monoBERTLarge Effectiveness with BM25 Interpolation on MS MARCO Passage
Figure 10: The effectiveness of monoBERTLarge on the development set of the MS MARCO passage
ranking test collection varying the interpolation weight of BM25 scores: α = 0.0 means that only the
monoBERT scores are used and α = 1.0 means that only the BM25 scores are used. BM25 scores do
not appear to improve end-to-end effectiveness using this score fusion technique.
Experimental results are presented in Figure 10, which shows that MRR@10 monotonically decreases
as we increase the weight placed on BM25 scores. This finding seems consistent with the reranking
depth analysis in Figure 9. It stands to reason that if increasing k from 10000 to 50000 still improves
MRR@10 (albeit slightly), then the BM25 score has limited value, i.e., it is unlikely that the BM25
score has much discriminative power between those ranks. Put differently, monoBERT doesn’t appear
to need “help” from BM25 to identify relevant texts.
So, do exact match scores contribute relevance signals that are not already captured by transformers?
We are careful to emphasize that this experiment alone does not definitively answer the question: it
only shows that with a simple interpolation approach, BM25 scores do not appear to provide additional
value to monoBERT on the MS MARCO passage ranking task. In contrast, Birch [Akkalyoncu Yilmaz
et al., 2019b] (see Section 3.3.1) as well as experiments with CEDR [MacAvaney et al., 2019a] (see
Section 3.3.3) both incorporate BM25 scores, and evidence on question answering tasks is fairly
conclusive that retrieval scores are helpful in boosting end-to-end effectiveness [Yang et al., 2019c,
Yang and Seo, 2020, Karpukhin et al., 2020b, Ma et al., 2021c].
Effects of Input Template Variations. As explained in the previous sections, the input to
monoBERT is comprised of three different sequences of dense vectors summed together at the
token level (token, segment, and position embeddings). The sequence contains the inputs as well
as the special tokens [CLS] and [SEP] that need to be positioned at specific locations. Together,
these elements define the “input template” of how queries and candidate texts are fed to BERT. How
important are each of these components? Here, we investigate which parts of the input are essential
to monoBERT’s effectiveness. Table 7 summarizes the results of these experiments.
We began by confirming that monoBERT is actually making use of relevance signals from token
positions to aid in ranking. If we remove the position embeddings but keep everything else in the
input template the same, which essentially ablates the model to relying only on a bag of words,
MRR@10 drops nearly six points, see rows (1) vs. (2). This suggests that token positions are clearly
an important relevance signal in monoBERT. Yet, interestingly, even without position information,
monoBERT remains much more effective than the BM25 baseline, which suggests that the model
is able to extract token-level signals in a bag-of-words setting (e.g., synonym, polysemy, semantic
relatedness, etc.). This can be interpreted as evidence that monoBERT is performing “soft” semantic
matching between query terms and terms in the candidate text.
59
MS MARCO Passage
Development
Method Input Template MRR@10
(1) BERTLarge, no modification [CLS] q [SEP] d [SEP] 0.365
(2) w/o positional embeddings [CLS] q [SEP] d [SEP] 0.307
(3) w/o segment type embeddings [CLS] q [SEP] d [SEP] 0.359
(4) swapping query and document [CLS] d [SEP] q [SEP] 0.366
(5) No [SEP] [CLS] Query: q Document: d 0.358
Table 7: The effectiveness of different monoBERTLarge input template variations on the development
set of the MS MARCO passage ranking test collection.
For tasks involving two inputs, we face the issue of how to “pack” the disparate inputs into a single
sequence (i.e., the input template) to feed to BERT. The standard solution devised by Devlin et al.
[2019] uses a combination of the [SEP] tokens and segment embeddings. The monoBERT model
inherits this basic design, but here we investigate different techniques to accomplish the goal of
“marking” disparate inputs so that the model can distinguish different parts of the task input.
As a simple ablation, we see that removing the segment embeddings has little impact, with only a
small loss in MRR@10. This shows that monoBERT can distinguish query and document tokens
using only the separator tokens and perhaps the absolute positions of the tokens. Since most queries
in MS MARCO have less than 20 tokens, could it be the case that monoBERT simply memorizes the
fact that query tokens always occur near the beginning of the input sequence, effectively ignoring the
separator tokens? To test this hypothesis, we swapped the order in which the query and the candidate
text are fed to monoBERT. Since the candidate texts have a much larger variation in terms of length
than the queries, the queries will occur in a larger range of token positions in the input sequence, thus
making it harder for monoBERT to identify query tokens based solely on their absolute positions.
Rows (1) vs. (4) show minimal difference in MRR@10 under this swapped treatment, which adds
further evidence that monoBERT is indeed using separator tokens and segment type embeddings to
distinguish between the query and the candidate text (in the default input template).
Given that the [SEP] token does seem to be playing an important role in segmenting the input
sequence to monoBERT, a natural follow-up question is whether different “delimiters” might also
work. As an alternative, we tried replacing [SEP] with the (literal) token “Query:” prepended to the
query and the token “Document:” prepended to the candidate text. This design is inspired by “text
only” input templates that are used in T5, described later in Section 3.5.3. The results are shown in
row (5) in Table 7, where we observe a drop in MRR@10. This suggests that [SEP] indeed does
have a special status in BERT, likely due to its extensive use in pretraining.
Clearly, the organization of the input template is important, which is an observation that has been
noted by other researchers as well across a range of NLP tasks [Haviv et al., 2021, Le Scao and
Rush, 2021]. Specifically for ranking, Boualili et al. [2020] suggested that BERT might benefit from
explicit exact match cues conveyed using marker tokens. However, the authors reported absolute
scores that do not appear to be competitive with the results reported in this section, and thus it is
unclear if such explicit cues continue to be effective with stronger baselines. Nevertheless, it is clear
that the organization of the input sequence can make a big difference in terms of effectiveness (in
ranking and beyond), and there is no doubt a need for more thorough further investigations.
Effects of Simple monoBERT variants. As discussed in the introduction of this survey, the public
release of BERT set off a stampede of follow-up models, ranging from relatively minor tweaks to
simple architectural variants to entirely new models inspired by BERT. Of course, the distinction
between a “variant” and a new model is somewhat fuzzy, but many researchers have proposed models
that are compatible with BERT in the sense that they can easily be “swapped in” with minimal
changes.93 In many cases, a BERT variant takes the same input template as monoBERT and operates
as a relevance classifier in the same way.
One notable BERT variant is RoBERTa [Liu et al., 2019c], which can be described as Facebook’s
replication study of BERT’s pretraining procedures “from scratch”, with additional explorations of
93In some cases, when using the Hugging Face Transformer library, swapping in one of these alternative models
is, literally, a one-line change.
60
MS MARCO Passage (Dev)
Method MRR@10
(1) monoBERTLarge 0.372
(2) monoRoBERTa Large 0.365
Table 8: The effectiveness of monoRoBERTa Large on the development set of the MS MARCO passage
ranking test collection. The monoBERTLarge results are copied from Table 5.
many design choices made in Devlin et al. [2019]. The authors of RoBERTa argued that Google’s
original BERT model was significantly under-trained. By modifying several hyperparameters and
by removing the next sentence prediction (NSP) task (see Section 3.1), RoBERTa is able to match
or exceed the effectiveness of BERT on a variety of natural language processing tasks. Table 8
shows the results of replacing BERTLarge with RoBERTa Large in monoBERT, evaluated on the MS
MARCO passage ranking test collection. These results have not been previously published, but the
experimental setup is the same as in Section 3.2 and the monoBERTLarge results are copied from row
(3b) in Table 5. We see that although RoBERTa achieves higher effectiveness across a range of NLP
tasks, these improvements do not appear to carry over to text ranking, as monoRoBERTa Large reports
a slightly lower MRR@10. This finding suggests that information access tasks need to be examined
independently from the typical suite of tasks employed by NLP researchers to evaluate their models.
Beyond RoBERTa, there is a menagerie of BERT-like models that can serve as drop-in replacements
of BERT for text ranking, just like monoRoBERTa. As we discuss models that tackle ranking longer
texts in the next section (Section 3.3), in which BERT serves as a component in a larger model, these
BERT alternatives can likewise be “swapped in” seamlessly. Because these BERT-like models were
developed at different times, the investigation of their impact on effectiveness has been mostly ad
hoc. For example, we are not aware of a systematic study of monoX, where X spans the gamut of
BERT replacements. Nevertheless, researchers have begun to experimentally study BERT variants
in place of BERT “classic” for ranking tasks. We will interleave the discussion of ranking models
and adaptations of BERT alternatives in the following sections. At a high level, these explorations
allow researchers to potentially “ride the wave” of model advancements at a relatively small cost.
However, since improvements on traditional natural language processing tasks may not translate
into improvements in information access tasks, the effectiveness of each BERT variant must be
empirically validated.
Discussion and Analysis. Reflecting on the results presented above, it is quite remarkable how
monoBERT offers a simple yet effective solution to the text ranking problem (at least for texts
that fit within its sequence length restrictions). The simplicity of the model has contributed greatly
to its widespread adoption. These results have been widely replicated and can be considered
robust findings—for example, different authors have achieved comparable results across different
implementations and hyperparameter settings. Indeed, monoBERT has emerged as the baseline for
transformer-based approaches to text ranking, and some variant of monoBERT serves as the baseline
for many of the papers cited throughout this survey.
3.2.3 Investigating How BERT Works
While much work has empirically demonstrated that BERT can be an effective ranking model, it is not
clear exactly why this is the case. As Lin [2019] remarked, it wasn’t obvious that BERT, specifically
designed for NLP tasks, would “work” for text ranking; in fact, the history of IR is littered with ideas
from NLP that intuitively “should work”, but never panned out, at least with the implementations of
the time. In this section, we present several lines of work investigating why BERT performs well for
both NLP tasks in general and for information access tasks in particular.
What is the relationship between BERT and “pre-BERT” neural ranking models? Figure 11
tries to highlight important architectural differences between BERT and pre-BERT neural ranking
models: for convenience, we repeat the high-level designs of the pre-BERT representation-based and
interaction-based neural ranking models, taken from Figure 1 in Section 1.2.4. As a high-level recap,
there is experimental evidence suggesting that interaction-based approaches (middle) are generally
more effective than representation-based approaches (left) because the similarity matrix explicitly
61
E1
q1
E2
q2
E3
q3
F1
d1
F2
d2
Fm
… dm
s
(a) Representation-Based
E
q 1
1
E
q 2
2
E
q 3
3
F1
d1
F2
d2
Fm
… dm
s
…
…
…
(b) Interaction-Based
E[CLS]
T[CLS]
[CLS]
E1
U1
q1
E2
U2
q2
E3
U3
q3
E[SEP1]
T[SEP1]
[SEP]
F1
V1
d1
F2
V2
d2
Fm
Vm
dm
E[SEP2]
T[SEP2]
[SEP]
…
…
s
… … … … … … … … … …
…
(c) monoBERT
Figure 11: Side-by-side comparison between high-level architectures of the two main classes of
pre-BERT neural ranking models with monoBERT, where all-to-all attention at each transformer
layer captures interactions between and within terms from the query and the candidate text.
captures exact as well as “soft” semantic matches between individual terms and sequences of terms
in the query and the candidate text.
In BERT, all-to-all interactions between and within query terms and terms from the candidate text are
captured by multi-headed attention at each layer in the transformer. Attention appears to serve as a
one-size-fits-all approach to extracting signal from term interactions, replacing the various techniques
used by pre-BERT interaction-based models, e.g., different pooling techniques, convolutional filters,
etc. Furthermore, it appears that monoBERT does not require any specialized neural architectural
components to model different aspects of relevance between queries and a candidate text, since each
layer of the transformer is homogeneous and the same model architecture is used for a variety of
natural language processing tasks. However, it also seems clear that ranking is further improved by
incorporating BERT as a component to extract relevance signals that are further processed by other
neural components, for example, PARADE (see Section 3.3.4). In other words, BERT can be used
directly for ranking or as a building block in a larger model.
What does BERT learn from pretraining? There has been no shortage of research that attempts
to reveal insights about how BERT “works” in general. Typically, this is accomplished through
visualization techniques (for example, of attention and activation patterns), probing classifiers, and
masked word prediction. We discuss a small subset of findings in the context of NLP here and refer the
reader to a survey by Rogers et al. [2020] for more details. Probing classifiers have been used in many
studies to determine whether something can be predicted from BERT’s internal representations. For
example, Tenney et al. [2019] used probes to support the claim that “BERT rediscovers the classical
NLP pipeline” by showing that the model represents part-of-speech tagging, parsing, named-entity
recognition, semantic role labeling, and coreference (in that order) in an interpretable and localizable
way. That is, internal representations encode information useful for these tasks, and some layers are
better than others at producing representations that are useful for a given task. However, Elazar et al.
[2021] used “amnesic probing” to demonstrate that such linguistic information is not necessarily used
when performing a downstream task.
Other researchers have examined BERT’s attention heads and characterized their behavior. For
example, Clark et al. [2019] categorized a few frequently observed patterns such as attending to
delimiter tokens and specific position offsets, and they were able to identify attention heads that
correspond to linguistic notions (e.g., verbs attending to direct objects). Kovaleva et al. [2019]
specifically focused on self-attention patterns and found that a limited set of attention patterns are
repeated across different heads, suggesting that the model is over-parameterized. Indeed, manually
disabling attention in certain heads leads to effectiveness improvements in some NLP tasks [Voita
et al., 2019]. Rather than attempting to train probing classifiers or to look “inside” the model, others
have investigated BERT’s behavior via a technique called masked term prediction. Since BERT was
pretrained with the masked language model (MLM) objective, it is possible to feed the masked token
[MASK] to the model and ask it to predict the masked term, as a way to probe what the model has
learned. Ettinger [2020] found that BERT performs well on some tasks like associating a term with
its hypernym (broader category) but performs much worse on others like handling negations. For
example, BERT’s top three predictions remained the same when presented with both “A hammer is
an [MASK]” and “A hammer is not an [MASK]”.
62
While these studies begin to shed light on the inner workings of BERT, they do not specifically
examine information access tasks, so they offer limited insight on how notions of relevance are
captured by BERT.
How does BERT perform relevance matching? Information retrieval researchers have attempted
to specifically investigate relevance matching by BERT in ranking tasks [Padigela et al., 2019, Qiao
et al., 2019, Câmara and Hauff, 2020, Zhan et al., 2020b, Formal et al., 2021b, MacAvaney et al.,
2020b]. For example, Qiao et al. [2019] argued that BERT should be understood as an “interactionbased sequence-to-sequence matching model” that prefers semantic matches between paraphrase
tokens. Furthermore, the authors also found that BERT’s relevance matching behavior differs from
neural rankers that are trained from user clicks in query logs. Zhan et al. [2020b] attributed the
processes of building semantic representations and capturing interaction signals to different layers,
arguing that the lower layers of BERT focus primarily on extracting representations, while the higher
layers capture interaction signals to ultimately predict relevance.
Câmara and Hauff [2020] created diagnostic datasets to test whether BERT satisfies a range of IR
axioms [Fang et al., 2004, 2011] describing how retrieval scores should change based on occurrences
of query terms, the discriminativeness (idf) of matched terms, the number of non-query terms in a
document, semantic matches against query terms, the proximity of query terms, etc. Using these
diagnostic datasets, they found that a distilled BERT model [Sanh et al., 2019] satisfies the axioms
much less frequently than Indri’s query likelihood model despite being much more effective, leading
to the conclusion that the axioms alone cannot explain BERT’s effectiveness. Similarly, in the context
of the ColBERT ranking model (described later in Section 5.5.2), Formal et al. [2021b] investigated
whether BERT has a notion of term importance related to idf. They found that masking low idf
terms influences the ranking less than masking high idf terms, but the importance of a term does not
necessarily correlate with its idf.
Furthering this thread of research on creating “diagnostics” to investigate ranking behavior, MacAvaney et al. [2020b] proposed using “textual manipulation tests” and “dataset transfer tests” in
addition to the diagnostic tests used in earlier work. They applied these tests to monoBERT as well as
to other models like T5 (described later in Section 3.5.3). The authors found that monoBERT is better
than BM25 at estimating relevance when term frequency is held constant, which supports the finding
from Câmara and Hauff [2020] that monoBERT does not satisfy term frequency axioms. Using
textual manipulation tests in which existing documents are modified, MacAvaney et al. [2020b] found
that shuffling the order of words within a sentence or across sentences has a large negative effect,
while shuffling the order of sentences within a document has a modest negative effect. However,
shuffling only prepositions had little effect. Surprisingly, in their experiments, monoBERT increases
the score of texts when non-relevant sentences are added to the end but decreases the score when
relevant terms from doc2query–T5 (described later in Section 4.3) are added to the end. Using dataset
transfer tests, which pair together two versions of the same document, MacAvaney et al. [2020b]
found that monoBERT scores informal text slightly higher than formal text and fluent text slightly
higher than text written by non-native speakers.
While progress has been made in understanding exactly how BERT “works” for text ranking, the
explanations remain incomplete, to some extent inconsistent, and largely unsatisfying. BERT shows
evidence of combining elements from both representation-based models as well as interaction-based
models. Furthermore, experimental results from input template variations above show that monoBERT
leverages exact match, “soft” semantic match, as well as term position information. How exactly
these different components combine—for different types of queries, across different corpora, and
under different settings, etc.—remains an open question.
3.2.4 Nuances of Training BERT
With transformers, the “pretrain then fine-tune” recipe has emerged as the standard approach of
applying BERT to specific downstream tasks such as classification, sequence labeling, and ranking.
Typically, we start with a “base” pretrained transformer model such as the BERTBase and BERTLarge
checkpoints directly downloadable from Google or the Hugging Face Transformers library. This
model is then fine-tuned on task-specific labeled data drawn from the same distribution as the target
task. For ranking, the model might be fine-tuned using a test collection comprised of queries and
relevance judgments under a standard training, development (validation), and test split.
63
However, there are many variations of this generic “recipe”, for example:
• Additional unsupervised pretraining.
• Fine-tuning on one or more out-of-domain (or more generally, out-of-distribution) labeled data
with respect to the target task.
• Fine-tuning on synthetically generated labeled data or data gathered via distant supervision
techniques (also called weak supervision).
• Specific fine-tuning strategies such as curriculum learning.
An important distinction among these techniques is the dichotomy between those that take advantage
of self supervision and those that require task-specific labeled data. We describe these two approaches
separately below, but for the most part our discussions occur at a high level because the specific
techniques can be applied in different contexts and on different models. Thus, it makes more sense
to introduce the general ideas here, and then interweave experimental results with the contexts or
models they are applied to (throughout this section). This is the narrative strategy we have adopted,
but to introduce yet another layer of complexity, these techniques can be further interwoven with
knowledge distillation, which is presented later in Section 3.5.1.
Additional Unsupervised Pretraining. The checkpoints of publicly downloadable models such as
BERTBase and BERTLarge are pretrained on “general domain” corpora: for example, BERT uses the
BooksCorpus [Zhu et al., 2015] as well as Wikipedia. While there may be some overlap between
these corpora and the target corpus over which ranking is performed, they may nevertheless differ in
terms of vocabulary distribution, genre, register, and numerous other factors. Similarly, while the
masked language model (MLM) and next sentence prediction (NSP) pretraining objectives lead to a
BERT model that performs well for ranking, neither objective is closely related to the ranking task.
Thus, it may be helpful to perform additional pretraining on the target corpus or with a new objective
that is tailored for ranking. It is important here to emphasize that pretraining requires only access to
the corpus we are searching and does not require any queries or relevance judgments.
In order to benefit from additional pretraining on a target corpus, the model should be given the
chance to learn more about the distribution of the vocabulary terms and their co-occurrences prior to
learning how to rank them. Put differently, the ranking model should be given an opportunity to “see”
what texts in a corpus “look like” before learning relevance signals. To our knowledge, Nogueira
et al. [2019a] was the first to demonstrate this idea, which they called target corpus pretraining (TCP),
specifically for ranking in the context of their multi-stage architecture (discussed in Section 3.4.1).
Here, we only present their results with monoBERT. Instead of using Google’s BERT checkpoints as
the starting point of fine tuning, they began by additional pretraining on the MS MARCO passage
corpus using the same objectives from the original BERT paper, i.e., masked language modeling and
next sentence prediction. Only after this additional pretraining stage was the model then fine-tuned
with the MS MARCO passage data. This technique has also been called “further pretraining”, and its
impact can be shown by comparing row (2a) with row (2b). Although the improvement is modest,
the gain is “free” in the sense of not requiring any labeled data, and so adopting this technique might
be worthwhile in certain scenarios.
These results are in line with findings from similar approaches for a variety of natural language
processing tasks [Beltagy et al., 2019, Raffel et al., 2020, Gururangan et al., 2020]. However, as
a counterpoint, Gu et al. [2020] argued that for domains with abundant unlabeled text (such as
biomedicine), pretraining language models from scratch is preferable to further pretraining generaldomain language models. This debate is far from settled and domain adaptation continues to be an
active area of research, both for text ranking and NLP tasks in general.
Other researchers have proposed performing pretraining using a modified objective, with the goal of
improving BERT’s effectiveness on downstream tasks. For example, ELECTRA (described later in
Section 3.3.1) replaces the masked language model task with a binary classification task that involves
predicting whether each term is the original term or a replacement.
Specifically for information retrieval, Ma et al. [2021b] proposed a new “representative words
prediction” (ROP) task that involves presenting the model with two different sets of terms and asking
the model to predict which set is more related to a given document. A pretraining instance comprises
two segments: segment A consists of one set of terms (analogous to the query in monoBERT)
64
MS MARCO Passage
Development Test
Method MRR@10 MRR@10
(1) Anserini (BM25) = Table 5, row (3a) 0.187 0.190
(2a) + monoBERT = Table 5, row (3b) 0.372 0.365
(2b) + monoBERT + TCP 0.379 -
Table 9: The effectiveness of target corpus pretraining (TCP) for monoBERT on the MS MARCO
passage ranking test collection.
and segment B contains a document. Given this input, the [CLS] token is provided as input to a
feedforward network to predict a score. This is performed for a “relevant” and a “non-relevant” set
of terms, and their scores are fed into a pairwise hinge loss. To choose the two sets of terms, a set
size is first sampled from a Poisson distribution, and then two sets of terms of the sampled size are
randomly sampled from a single document with stopwords removed. A multinomial query likelihood
model with Dirichlet smoothing [Zhai, 2008] is then used to calculate a score for each set of terms;
the set with the higher score is treated as the “relevant” set.
Ma et al. [2021b] evaluated the impact of performing additional pretraining with BERTBase on
Wikipedia and the MS MARCO document collection with the MLM objective, their proposed ROP
objective, and a combination of the two. They found that pretraining with ROP improves effectiveness
over pretraining with MLM on the Robust04, ClueWeb09B, and GOV2 test collections when reranking
BM25. Each document in these datasets was truncated to fit into monoBERT. Combining the MLM
and ROP objectives yielded little further improvement. However, the models reported in this work
do not appear to yield results that are competitive with many of the simple models we describe later
in this section, and thus it is unclear if this pretraining technique can yield similar gains on better
ranking models.
“Multi-Step” Supervised Fine-Tuning Strategies. In the context of pretrained transformers, finetuning involves labeled data drawn from the same distribution as the target downstream task. However,
it is often the case that researchers have access to labeled that is not “quite right” with respect to
the target task. In NLP, for example, we might be interested in named-entity recognition (NER) in
scientific articles in the biomedical domain, but we have limited annotated data. Can NER data on
news articles, for example, nevertheless be helpful? The same train of thought can be applied to text
ranking. Often, we are interested in a slightly different task or a different domain than the ones we
have relevance judgments for. Can we somehow exploit these data?
Not surprisingly, the answer is yes and researchers have experimented with different “multi-step”
fine-tuning strategies for a range of NLP applications. The idea is to leverage out-of-task or out-ofdomain labeled data (or out-of-distribution labeled data that’s just not “right” for whatever reason) to
fine-tune a model before fine-tuning on labeled data drawn from the same distribution as the target
task. Since there may be multiple such datasets, the fine-tuning process may span multiple “stages”
or “phases”. In the same way that target corpus pretraining gives the model a sense of what the texts
“look like” before attempting to learn relevance signals, these technique attempts to provide the model
with “general” knowledge of the task before learning from task-specific data. To our knowledge,
the first reported instance of sequential fine-tuning with multiple labeled datasets is by Phang et al.
[2018] on a range of natural language inference tasks.
This technique of sequentially fine-tuning on multiple datasets, as specifically applied to text ranking,
has also been explored by many researchers: Akkalyoncu Yilmaz et al. [2019b] called this crossdomain relevance transfer. Garg et al. [2020] called this the “transfer and adapt” (TANDA) approach.
Dai and Callan [2019a] first fine-tuned on data from search engine logs before further fine-tuning on
TREC collections. Zhang et al. [2021] called this “pre–fine-tuning”, and specifically investigated the
effectiveness of pre–fine-tuning a ranking model on the MS MARCO passage ranking test collection
before further fine-tuning on collection-specific relevance judgments. Mokrii et al. [2021] presented
another study along similar lines. Applied to question answering, Yang et al. [2019d] called this
“stage-wise” fine-tuning, which is further detailed in Xie et al. [2020]. For consistency in presentation,
in this survey we refer to such sequential or multi-step fine-tuning strategies as pre–fine-tuning, with
the convenient abbreviation of pFT (vs. FT for fine tuning). This technique is widely adopted—
65
obviously applicable to monoBERT, but can also be used in the context of other models. We do
not present any experimental results here, and instead examine the impact of pre–fine-tuning in the
context of specific ranking models presented in this section.
One possible pitfall when fine-tuning with multiple labeled datasets is the phenomenon known as
“catastrophic forgetting”, where fine-tuning a model on a second dataset interferes with its ability
to perform the task captured by the first dataset. This is undesirable in many instances because we
might wish for the model to adapt “gradually”. For example, if the first dataset captured text ranking
in the general web domain and the second dataset focuses on biomedical topics, we would want the
model to gracefully “back off’ to general web knowledge if the query was not specifically related to
biomedicine. Lovón-Melgarejo et al. [2021] studied catastrophic forgetting in neural ranking models:
Compared to pre-BERT neural ranking models, they found that BERT-based models seem to be able
to retain effectiveness on the pre–fine-tuning dataset after further fine-tuning.
Pre–fine-tuning need not exploit human labeled data. For example, relevance judgments might
come from distant (also called weak) supervision techniques. Zhang et al. [2020d] proposed a
method for training monoBERT with weak supervision by using reinforcement learning to select
(anchor text, candidate text) pairs during training. In this approach, relevance judgments are used to
compute the reward guiding the selection process, but the selection model does not use the judgments
directly. To apply their trained monoBERT model to rerank a target collection, the authors trained a
learning-to-rank method using coordinate ascent with features consisting of the first-stage retrieval
score and monoBERT’s [CLS] vector. The authors found that these extensions improved over prior
weak supervision approaches used with neural rankers [Dehghani et al., 2017, MacAvaney et al.,
2019b]. Beyond weak supervision, it might even been possible to leverage synthetic data, similar to
the work of Ma et al. [2021a] (who applied the idea to dense retrieval), but this thread has yet to be
fully explored.
The multi-step fine-tuning strategies discussed here are related to the well-studied notion of curriculum
learning. MacAvaney et al. [2020e] investigated whether monoBERT can benefit from a training
curriculum [Bengio et al., 2009] in which the model is presented with progressively more difficult
training examples as training progresses. Rather than excluding training data entirely, they calculate
a weight for each training example using proposed difficulty heuristics based on BM25 ranking. As
training progresses, these weights become closer to uniform. MacAvaney et al. [2020e] found that
this weighted curriculum learning approach can significantly improve the effectiveness of monoBERT.
While both pre–fine-tuning and curriculum learning aim to sequence the presentation of examples
to a model during training, the main difference between these two methods is that pre–fine-tuning
generally involves multiple distinct datasets. In contrast, curriculum learning strategies can be applied
even on a single (homogeneous) dataset.
One main goal of multi-step fine-tuning strategies is to reduce the amount of labeled data needed in
the target domain or task by exploiting existing “out-of-distribution” datasets. This connects to the
broad theme of “few-shot” learning, popular in natural language processing, computer vision, and
other fields as well. Taking this idea to its logical conclusion, researchers have explored zero-shot
approaches to text ranking. That is, the model is trained on (for example) out-of-domain data and
directly applied to the target task. Examples include Birch (see Section 3.3.1) and monoT5 (see
Section 3.5.3), as well as zero-shot domain adaptation techniques (see Section 6.2). We leave details
to these specific sections.
To wrap up the present discussion, researchers have explored many different techniques to “train”
BERT and other transformers beyond the “pretrain then fine-tune” recipe. There is a whole litany
of tricks to exploit “related” data, both in an unsupervised as well as a supervised fashion (and to
even “get away” with not using target data at all in a zero-shot setting). While these tricks can indeed
be beneficial, details of how to properly apply them (e.g., how many epochs to run, how many and
what order to apply out-of-domain datasets, how to heuristically label and select data, when zero-shot
approaches might work, etc.) remain somewhat of an art, and their successful application typically
involves lots of trial and error. Some of these issues are discussed by Zou et al. [2021] in the context
of applying transformer-based models in Baidu search, where they cautioned that blindly fine-tuning
risks unstable predictions, poor generalizations, and deviations from task metrics, especially when
the training data are noisy. While we understand at a high level why various fine-tuning techniques
work, more research is required to sharpen our understanding so that expected gains can be accurately
predicted and modeled without the need to conduct extensive experiments repeatedly.
66
These are important issues that remain unresolved, and in particular, pretraining and pre–fine-tuning
become important when transformers are applied to domain-specific applications, such as legal texts
and scientific articles; see additional discussions in Section 6.2.
3.3 From Passage to Document Ranking
One notable limitation of monoBERT is that it does not offer an obvious solution to the input length
restrictions of BERT (and of simple BERT variants). Nogueira and Cho [2019] did not have this
problem because the test collections they examined did not contain texts that overflowed this limit.
Thus, monoBERT is limited to ranking paragraph-length passages, not longer documents (e.g., news
articles) as is typically found in most ad hoc retrieval test collections. This can be clearly seen
in the histogram of text lengths from the MS MARCO passage corpus, shown in Figure 3 from
Section 2.7. The combination of BERT’s architecture and the pretraining procedure means that the
model has difficulty handling input sequences longer than 512 tokens, both from the perspective
of model effectiveness and computational requirements on present-day hardware. Let us begin by
understanding in more detail what the issues are.
Since BERT was pretrained with only input sequences up to 512 tokens, learned position embeddings
for token positions past 512 are not available. Because position embeddings inform the model about
the linear order of tokens, if the input sequence lacks this signal, then everything the model has
learned about the linear structure of language is lost (i.e., the input will essentially be treated as
a bag of words). We can see from the experimental results in Table 7 that position embeddings
provide important relevance signals for monoBERT. Henderson [2020] explained this by pointing
out that BERT can be thought of as a “bag of vectors”, where structural cues come only from the
position embeddings. This means that the vectors in the bag are exchangeable, in that renumbering
the indices used to refer to the different input representations will not change the interpretation of the
representation (provided that the model is adjusted accordingly as well). While it may be possible to
learn additional position embeddings during fine-tuning with sufficient training data, this does not
seem like a practical general-purpose solution. Without accurate position embeddings, it is unclear
how we would prepare input sequences longer than 512 tokens for inference (more details below).
From the computational perspective, the all-to-all nature of BERT’s attention patterns at each
transformer encoder layer means that it exhibits quadratic complexity in both time and space with
respect to input length. Thus, simply throwing more hardware at the problem (e.g., GPUs with more
RAM) is not a practical solution; see Beltagy et al. [2020] for experimental results characterizing
resource consumption on present-day hardware with increasing sequence lengths. Instead, researchers
have tackled this issue by applying some notion of sparsity to the dense attention mechanism. See
Tay et al. [2020] for a survey of these attempts, which date back to at least 2019 [Child et al., 2019].
We discuss modifications to the transformer architecture that replace all-to-all attention with more
efficient alternatives later in Section 3.3.5.
The length limitation of BERT (and transformers in general) breaks down into two distinct but related
challenges for text ranking:
Training. For training, it is unclear what to feed to the model. The key issue is that relevance
judgments for document ranking (e.g., from TREC test collections) are provided at the document
level, i.e., they are annotations on the document as a whole. Obviously, a judgment of “relevant”
comes from a document containing “relevant material”, but it is unknown how that material is
distributed throughout the document. For example, there could be a relevant passage in the middle of
the document, a few relevant passages scattered throughout the document, or the document may be
relevant “holistically” when considered in its entirety, but without any specifically relevant passages.
If we wish to explicitly model different relevance grades (e.g., relevant vs. highly relevant), then this
“credit assignment” problem becomes even more challenging.
During training, if the input sequence (i.e., document plus the query and the special tokens) exceeds
BERT’s length limitations, it must be truncated somehow, lest we run into exactly the issues discussed
above. Since queries are usually shorter than documents, and it make little sense to truncate the query,
we must sacrifice terms from the document text. While we could apply heuristics, for example, to feed
BERT only spans in the document that contain query terms or even disregard this issue completely
(see Section 3.3.2), there is no guarantee that training passages from the document fed to BERT are
actually relevant. Thus, training will be noisy at best.
67
Inference. At inference time, if a document is too long to feed into BERT in its entirety, we must
decide how to preprocess it. We could segment the document into chunks, but there are many design
choices: For example, fixed-width spans or natural units such as sentences? How wide should these
segments be? Should they be overlapping? Furthermore, applying inference over different chunks
from a document still requires some method for aggregating evidence.
It is possible to address the inference challenge by aggregating either passage scores or passage
representations. Methods that use score aggregation predict a relevance score for each chunk, and
these scores are then aggregated to produce a document relevance score (e.g., by taking the maximum
score across the chunks). Methods that perform representation aggregation first combine passage
representations before predicting a relevance score. With a properly designed aggregation technique,
even if each passage is independently processed, the complete ranking model can be differentiable
and thus amenable to end-to-end training via back propagation. This solves the training challenge as
well, primarily by letting the model figure out how to allocate “credit” by itself.
Breaking this “length barrier” in transitioning from passage ranking to full document ranking was the
next major advance in applying BERT to text ranking. This occurred with three proposed models
that were roughly contemporaneous, dating to Spring 2019, merely a few months after monoBERT:
Birch [Akkalyoncu Yilmaz et al., 2019b], which was first described by Yang et al. [2019e], BERT–
MaxP [Dai and Callan, 2019b], and CEDR [MacAvaney et al., 2019a]. Interestingly, these three
models took different approaches to tackle the training and inference challenges discussed above,
which we detail in turn. We then present subsequent developments: PARADE [Li et al., 2020a], which
incorporates and improves on many of the lessons learned in CEDR, and a number of alternative
approaches to ranking long texts. All of these ranking models are still based on BERT or a simple
BERT variant at their cores; we discuss efforts to move beyond BERT in Section 3.5.
3.3.1 Document Ranking with Sentences: Birch
The solution presented by Birch [Akkalyoncu Yilmaz et al., 2019b] can be summarized as follows:
• Avoid the training problem entirely by exploiting labeled data where length issues don’t exist,
and then transferring the learned relevance matching model on those data to the domain or task
of interest.
• For the inference problem, convert the task of estimating document relevance into the task of
estimating the relevance of individual sentences and then aggregating the resulting scores.
In short, Birch solved the training problem above by simply avoiding it. Earlier work by the same
research group [Yang et al., 2019e] that eventually gave rise to Birch first examined the task of
ranking tweets, using test collections from the TREC Microblog Tracks [Ounis et al., 2011, Soboroff
et al., 2012, Lin and Efron, 2013, Lin et al., 2014]. These evaluations focused on information seeking
in a microblog context, where users desire relevant tweets with respect to an information need at
a particular point in time. As tweets are short (initially 140 characters, now 280 characters), they
completely avoid the length issues we discussed above.
Not surprisingly, fine-tuning monoBERT on tweet data led to large and statistically significant gains
on ranking tweets. However, Yang et al. [2019e] discovered that a monoBERT model fine-tuned with
tweet data was also effective for ranking documents from a newswire corpus. This was a surprising
finding: despite similarities in the task (both are ad hoc retrieval problems), the domains are completely
different. Newswire articles comprise well-formed and high-quality prose written by professional
journalists, whereas tweets are composed by social media users, often containing misspellings,
ungrammatical phrases, and incoherent meanings, not to mention genre-specific idiosyncrasies such
as hashtags and @-mentions.
In other words, Yang et al. [2019e] discovered that, for text ranking, monoBERT appears to have
very strong domain transfer effects for relevance matching. Training on tweet data and performing
inference on articles from a newswire corpus is an instance of zero-shot cross-domain learning, since
the model had never been exposed to annotated data from the specific task.
94 This finding predated
many of the papers discussed in Section 3.2.4, but in truth Birch had begun to explore some of the
ideas presented there (e.g., pre–fine-tuning as well as zero-shot approaches).
94There is no doubt, of course, that BERT had been exposed to newswire text during pretraining.
68
Robust04 Core17 Core18
Method MAP nDCG@20 MAP nDCG@20 MAP nDCG@20
(1) BM25 + RM3 0.2903 0.4407 0.2823 0.4467 0.3135 0.4604
(2a) 1S: BERT(MB) 0.3408† 0.4900† 0.3091† 0.4628 0.3393† 0.4848†
(2b) 2S: BERT(MB) 0.3435† 0.4964† 0.3137† 0.4781 0.3421† 0.4857†
(2c) 3S: BERT(MB) 0.3434† 0.4998† 0.3154† 0.4852† 0.3419† 0.4878†
(3a) 1S: BERT(MSM) 0.3028† 0.4512 0.2817† 0.4468 0.3121 0.4594
(3b) 2S: BERT(MSM) 0.3028† 0.4512 0.2817† 0.4468 0.3121 0.4594
(3c) 3S: BERT(MSM) 0.3028† 0.4512 0.2817† 0.4468 0.3121 0.4594
(4a) 1S: BERT(MSM → MB) 0.3676† 0.5239† 0.3292† 0.5061† 0.3486† 0.4953†
(4b) 2S: BERT(MSM → MB) 0.3697† 0.5324† 0.3323† 0.5092† 0.3496† 0.4899†
(4c) 3S: BERT(MSM → MB) 0.3691† 0.5325† 0.3314† 0.5070† 0.3522† 0.4899†
Table 10: The effectiveness of Birch on the Robust04, Core17, and Core18 test collections. The
symbol † denotes significant improvements over BM25 + RM3 (paired t-tests, p < 0.01, with
Bonferroni correction).
This domain-transfer discovery was later refined by Akkalyoncu Yilmaz et al. [2019b] in Birch.
To compute a document relevance score sf , inference is applied to each individual sentence in the
document, and then the top n scores are combined with the original document score sd (i.e., from
first-stage retrieval) as follows:
sf
∆= α · sd + (1 − α) ·
Xn
i=1
wi
· si (18)
where si
is the score of the i-th top scoring sentence according to BERT. Inference on individual
sentences proceeds in the same manner as in monoBERT, where the input to BERT is comprised of
the concatenation of the query q and a sentence pi ∈ D into the sequence:
[[CLS], q, [SEP], pi
, [SEP]] (19)
In other words, the final relevance score of a document comes from the combination of the original
candidate document score sd and evidence contributions from the top sentences in the document as
determined by the BERT model. The parameters α and wi’s can be tuned via cross-validation.
Results and Analysis. Birch results are reported in Table 10 with BERTLarge on the Robust04, Core17,
and Core18 test collections (see Section 2.7), with metrics directly copied from Akkalyoncu Yilmaz
et al. [2019b]. To be explicit, the query tokens q fed into BERT come from the “title” portion of the
TREC topics (see Section 2.2), i.e., short keyword phrases. This distinction will become important
when we discuss Dai and Callan [2019b] next. The results in the table are based on reranking the
top k = 1000 candidates using BM25 from Anserini for first-stage retrieval using the topic titles as
bag-of-words queries. See the authors’ paper for detailed experimental settings. Note that none of
these collections were used to fine-tune the BERT relevance models; the only learned parameters are
the weights in Eq. (18).
The top row shows the BM25 + RM3 query expansion baseline. The column groups present
model effectiveness on the Robust04, Core17, and Core18 test collections. Each row describes an
experimental condition: nS indicates that inference was performed on the top n scoring sentences
from each document. Up to three sentences were considered; the authors reported that more sentences
did not yield any improvements in effectiveness. The notation in parentheses describes the fine-tuning
procedure: MB indicates that BERT was fine-tuned on data from the TREC Microblog Tracks; MSM
indicates that BERT was fine-tuned on data from the MS MARCO passage retrieval test collection;
MSM → MB refers to a model that was first pre–fine-tuned on the MS MARCO passage data and
then further fine-tuned on MB.95 Table 10 also includes results of significance testing using paired
t-tests, comparing each condition to the BM25 + RM3 baseline. Statistically significant differences
(p < 0.01), with appropriate Bonferroni correction, are denoted by the symbol † next to the result.
Birch fine-tuned on microblog data (MB) alone significantly outperforms the BM25 + RM3 baseline
for all three metrics on Robust04. On Core17 and Core18, significant increases in MAP are observed
95Akkalyoncu Yilmaz et al. [2019b] did not call this pre–fine-tuning since the term was introduced later.
69
as well (and other metrics in some cases). In other words, the relevance classification model learned
from labeled tweet data successfully transferred over to news articles despite the large aforementioned
differences in domain.
Interestingly, Akkalyoncu Yilmaz et al. [2019b] reported that fine-tuning on MS MARCO alone yields
smaller gains over the baselines compared to fine-tuning on tweets. The gains in MAP are statistically
significant for Robust04 and Core17, but not Core18. In her thesis, Akkalyoncu Yilmaz [2019]
conducted experiments that offered an explanation: this behavior is attributable to mismatches in
input text length between the training and test data. The average length of the tweet training examples
is closer to the average length of sentences in Robust04 than the passages in the MS MARCO passage
corpus (which are longer). By simply truncating the MS MARCO training passages to the average
length of sentences in Robust04 and fine-tuning the model with these new examples, Akkalyoncu
Yilmaz reported a large boost in effectiveness: 0.3300 MAP on Robust04. While this result is still
below fine-tuning only with tweets, simply truncating MS MARCO passages also degrades the quality
of the dataset, in that it could have discarded the relevant portions of the passages, thus leaving behind
an inaccurate relevance label.
The best condition in Birch is to pre–fine-tune with MS MARCO passages, and then further fine-tune
with tweet data, which yields effectiveness that is higher than fine-tuning with either dataset alone.
Looking across all fine-tuning configurations of Birch, it appears that the top-scoring sentence of
each candidate document alone is a good indicator of document relevance. Additionally considering
the second ranking sentence yields at most a minor gain, and in some cases, adding a third sentence
actually causes effectiveness to drop. In all cases, however, contributions from BM25 scores remain
important—the model places non-negligible weight on α in Eq. (18). This result does not appear
to be consistent with the monoBERT experiments described in Figure 10, which shows that beyond
defining the top k candidates fed to monoBERT, BM25 scores do not provide any additional relevance
signal, and in fact interpolating BM25 scores hurts effectiveness. The two models, of course, are
evaluated on different test collections, but the question of whether exact term match scores are still
necessary for relevance classification with BERT remains not completely resolved.
The thesis of Akkalyoncu Yilmaz [2019] described additional ablation experiments that reveal
interesting insights about the behavior of BERT for document ranking. It has long been known (see
discussion in Section 1.2.2) that modeling the relevance between queries and documents requires a
combination of exact term matching (i.e., matching the appearance of query terms in the text) as well
as “semantic matching”, which encompasses attempts to capture a variety of linguistic phenomena
including synonymy, paraphrases, etc. What is the exact role that each plays in BERT? To answer
this question, Akkalyoncu Yilmaz [2019] performed an ablation experiment where all sentences that
contain at least one query term were discarded; this had the effect of eliminating all exact match
signals and forced BERT to rely only on semantic match signals. As expected, effectiveness was much
lower, reaching only 0.3101 MAP on Robust04 in the best model configuration, but the improvement
over the BM25 + RM3 baseline (0.2903 MAP) remained statistically significant. This result suggests
that with BERT, semantic match signals make important contributions to relevance matching.
As an anecdotal example, for the query “international art crime”, in one relevant document, the
following sentence was identified as the most relevant: “Three armed robbers take 21 Renaissance
paintings worth more than $5 million from a gallery in Zurich, Switzerland.” Clearly, this sentence
contains no terms from the query, yet provides information relevant to the information need. An
analysis of the attention patterns shows strong associations between “art” and “paintings” and between
“crime” and “robbers” in the different transformer encoder layers. Here, we see that BERT accurately
captures semantically important matches for the purposes of modeling query–document relevance,
providing qualitative evidence supporting the conclusion above.
To provide some broader context for the level of effectiveness achieved by Birch: Akkalyoncu Yilmaz
et al. [2019b] claimed to have reported the highest known MAP at the time of publication on the
Robust04 test collection. This assertion appears to be supported by the meta-analysis of Yang et al.
[2019b], who analyzed over 100 papers up until early 2019 and placed the best neural model at
0.3124 [Dehghani et al., 2018]. These results also exceeded the previous best known score of 0.3686,
a non-neural method based on ensembles [Cormack et al., 2009] reported in 2009. On the same
dataset, CEDR [MacAvaney et al., 2019a] (which we discuss in Section 3.3.3) achieved a slightly
higher nDCG@20 of 0.5381, but the authors did not report MAP. BERT–MaxP (which we discuss
next in Section 3.3.2) reported 0.529 nDCG@20. It seems clear that the “first wave” of text ranking
70
models based on BERT was able to outperform pre-BERT models and at least match the best nonneural techniques known at the time.96 These scores, in turn, have been bested by even newer ranking
models such as PARADE [Li et al., 2020a] (Section 3.3.4) and monoT5 (Section 3.5.3). The best
Birch model also achieved a higher MAP than the best TREC submissions that did not use past
labels or involve human intervention for both Core17 and Core18, although both test collections were
relatively new at the time and thus had yet to receive much attention from researchers.
Additional Studies. Li et al. [2020a] introduced a Birch variant called Birch–Passage, which differs
in four ways: (1) the model is trained end-to-end, (2) it is fine-tuned with relevance judgments on
the target corpus (with pre–fine-tuning on the MS MARCO passage ranking test collection) rather
than being used in a zero-shot setting, (3) it takes passages rather than sentences as input, and (4) it
does not combine retrieval scores from the first-stage ranker. In more detail: Passages are formed
by taking sequences of 225 tokens with a stride of 200 tokens. As with the original Birch design,
Birch–Passage combines relevance scores from the top three passages. To train the model end-to-end,
a fully-connected layer with all weights initially set to one is used to combine the three scores; this is
equivalent to a weighted summation. Instead of BERTLarge as in the original work, Li et al. [2020a]
experimented with BERTBase as well as the ELECTRABase variant.
ELECTRA [Clark et al., 2020b] can be described as a BERT variant that attempts to improve
pretraining by substituting its masked language model pretraining task with a replaced token detection
task, in which the model predicts whether a given token has been replaced with a token produced by
a separate generator model. The contextual representations learned by ELECTRA were empirically
shown to outperform those from BERT on various natural language processing tasks given the same
model size, data, and compute.
Results copied from Li et al. [2020a] are shown in row (4) in Table 11. The “Title” and “Description”
columns denote the effectiveness of using different parts of a TREC topic in the input template
fed to the model for reranking; the original Birch model only experimented with topic titles. The
effectiveness differences between these two conditions were first observed by Dai and Callan [2019b]
in the context of MaxP, and thus we defer our discussions until there. Comparing these results to the
original Birch experiments, repeated in row (3) from row (4c) in Table 10, it seems that one or more
of the changes in Birch–Passage increased effectiveness. However, due to differences in experimental
design, it is difficult to isolate the source of the improvement.
To better understand the impact of various design decisions made in Li et al. [2020a] and Akkalyoncu Yilmaz et al. [2019b], we conducted additional experiments with Birch–Passage using the
Capreolus toolkit [Yates et al., 2020]; to date, these results have not be reported elsewhere. In addition
to the various conditions examined by Li et al., we also considered the impact of linear interpolation
with first-stage retrieval scores and the impact of pre–fine-tuning. These experiments used the same
first-stage ranking, folds, hyperparameters, and codebase as Li et al. [2020a], thus enabling a fair and
meaningful comparison.
Results are shown in Table 11, grouped into “no interpolation” and interpolation “with BM25 + RM3”
columns. These model configurations provide a bridge that allows us to compare the results of Li
et al. [2020a] and Akkalyoncu Yilmaz et al. [2019b] in a way that lets us better attribute the impact
of different design choices. Rows (5a) and (5b) represent Birch–Passage using either BERTBase
or ELECTRABase, without pre–fine-tuning in both cases. It seems clear that a straightforward
substitution of BERTBase for ELECTRABase yields a gain in effectiveness. Here, model improvements
on general NLP tasks reported by Clark et al. [2020b] do appear to translate into effective gains in
document ranking.
Comparing the interpolated results on title (keyword) queries, we see that Birch–Passage performs
slightly worse than the original Birch model, row (3), using BERTBase, row (5a), and slightly better
than the original Birch model using ELECTRABase, row (5b). While ELECTRABase is about onethird the size of BERTLarge, it is worth noting that Birch–Passage has the advantage of being fine-tuned
on Robust04. These results can be viewed as a replication (i.e., independent implementation) of the
main ideas behind Birch, as well as their generalizability, since we see that a number of different
design choices leads to comparable levels of effectiveness.
96The comparison to Cormack et al. [2009], however, is not completely fair due to its use of ensembles, whereas
Birch, BERT–MaxP, and CEDR are all individual ranking models.
71
Robust04
No interpolation with BM25 + RM3
nDCG@20 nDCG@20
Method Title Desc Title Desc
(1) BM25 0.4240 0.4058 - -
(2) BM25 + RM3 0.4514 0.4307 - -
(3) Birch (MS→MB, BERTLarge) = Table 10, row (4c) - - 0.5325 -
(4) Birch–Passage (ELECTRABase w/ MSM pFT) [Li et al., 2020a] 0.5454 0.5931 - -
(5a) Birch–Passage (BERTBase, no pFT) 0.4959† 0.5502 0.5260† 0.5723
(5b) Birch–Passage (ELECTRABase, no pFT) 0.5259 0.5611 0.5479 0.5872
Table 11: The effectiveness of Birch variants on the Robust04 test collection using title and description queries with and without BM25 + RM3 interpolation. Statistically significant decreases
in effectiveness from Birch–Passage (ELECTRABase) are indicated with the symbol † (two-tailed
paired t-test, p < 0.05, with Bonferroni correction).
Also from rows (5a) and (5b), we can see that both Birch–Passage variants benefit from linear
interpolation with BM25 + RM3 as the first-stage ranker. Comparing title and description queries,
Birch–Passage performs better with description queries regardless of the interpolation setting and
which BERT variant is used (more discussion next, in the context of MaxP). Row (5b) vs. row (4)
illustrates the effects of pre–fine-tuning, which is the only difference between those two conditions.
It should be no surprise that first fine-tuning with a very large, albeit out-of-domain, dataset has a
beneficial impact on effectiveness. In Section 3.3.2, we present additional experimental evidence
supporting the effectiveness of this technique.
Takeaway Lessons. Summarizing, there are two important takeaways from Birch:
1. BERT exhibits strong zero-shot cross-domain relevance classification capabilities when used in
a similar way as monoBERT. That is, we can train a BERT model using relevance judgments
from one domain (e.g., tweets) and directly apply the model to relevance classification in a
different domain (e.g., newswire articles) and achieve a high-level of effectiveness.
2. The relevance score of the highest-scoring sentence in a document is a good proxy for the
relevance of the entire document. In other words, it appears that document-level relevance can
be accurately estimated by considering only a few top sentences.
The first point illustrates the power of BERT, likely attributable to the wonders of pretraining. The
finding with Birch is consistent with other demonstrations of BERT’s zero-shot capabilities, for
example, in question answering [Petroni et al., 2019]. We return to elaborate on this observation in
Section 3.5.3 in the context of ranking with sequence-to-sequence models and also in Section 6.2 in
the context of domain-specific applications.
The second point is consistent with previous findings in the information retrieval literature as well as
the BERT–MaxP model that we describe next. We defer a more detailed discussion of this takeaway
after presenting that model.
3.3.2 Passage Score Aggregation: BERT–MaxP and Variants
Another solution to the length limitations of BERT is offered by Dai and Callan [2019b], which can
be summarized as follows:
• For training, don’t worry about it! Segment documents into overlapping passages: treat all
segments from a relevant document as relevant and all segments from a non-relevant document
as not relevant.
• For the inference problem, segment documents in the same way, estimate the relevance of
each passage, and then perform simple aggregation of the passage relevance scores (taking the
maximum, for example; see more details below) to arrive at the document relevance score.
In more detail, documents are segmented into passages using a 150-word sliding window with a stride
of 75 words. Window width and stride length are hyperparameters, but Dai and Callan [2019b] did
72
Robust04 ClueWeb09b
nDCG@20 nDCG@20
Model Title Desc Title Desc
(1) BoW 0.417 0.409 0.268 0.234
(2) SDM 0.427 0.427 0.279 0.235
(3) LTR 0.427 0.441 0.295 0.251
(4a) BERT–FirstP 0.444† 0.491† 0.286 0.272†
(4b) BERT–MaxP 0.469† 0.529† 0.293 0.262†
(4c) BERT–SumP 0.467† 0.524† 0.289 0.261
(5) BERT–FirstP (Bing pFT) - - 0.333† 0.300†
Table 12: The effectiveness of different passage score aggregation approaches on the Robust04 and
ClueWeb09b test collections. The symbol † denotes significant improvements over LTR (p < 0.05).
not report experimental results exploring the effects of different settings. Inference on the passages is
the same as in Birch and in monoBERT, where for each passage pi ∈ D, the following sequence is
constructed and fed to BERT as the input template:
[[CLS], q, [SEP], pi
, [SEP]] (20)
where q is the query. The [CLS] token is then fed into a fully-connected layer (exactly as in
monoBERT) to produce a score si for passage pi
.
97 The passage relevance scores {si} are then
aggregated to produce the document relevance score sd according to one of three approaches:
• BERT–MaxP: take the maximum passage score as the document score, i.e., sd = max si
• BERT–FirstP: take the score of the first passage as the document score, i.e., sd = s1.
• BERT–SumP: take the sum of all passage scores as the document score, i.e., sd =
P
i
si
.
Another interesting aspect of this work is an exploration of different query representations that are fed
into BERT, which is the first study of its type that we are aware of. Recall that in Birch, BERT input is
composed from the “title” portion of TREC topics, which typically comprises a few keywords, akin to
queries posed to web search engines today (see Section 2.2). In addition to using these as queries, Dai
and Callan [2019b] also investigated using the sentence-long natural language “description” fields as
query representations fed to BERT. As the experimental results show, this choice has a large impact
on effectiveness.
Results and Analysis. Main results, in terms of nDCG@20, copied from Dai and Callan [2019b]
on Robust04 and test collections on ClueWeb09b (see Section 2.7) are presented in Table 12. Just
like in Birch and monoBERT, the retrieve-and-rerank strategy was used—in this case, the candidate
documents were supplied by bag-of-words default ranking with the Indri search engine.98 These
results are shown in row (1) as “BoW”. The top k = 100 results, with either title or description
queries were reranked with BERTBase; for comparison, note that Birch reranked with k = 1000.
Different aggregation techniques were compared against two baselines: SDM, shown in row (2),
refers to the sequential dependence model [Metzler and Croft, 2005]. On top of bag-of-words queries
(i.e., treating all terms as independent unigrams), SDM contributes evidence from query bigrams that
occur in the documents (both ordered and unordered). Previous studies have validated the empirical
effectiveness of this technique, and in this context SDM illustrates how keyword queries can take
advantage of simple “structure” present in the query (based purely on linear word order). As another
point of comparison, the effectiveness of a simple learning-to-rank approach was also examined,
shown in row (3) as “LTR”. The symbol † denotes improvements over LTR that are statistically
significant (p < 0.05).
Without pre–fine-tuning, the overall gains coming from BERT on ranking web pages (ClueWeb09b)
are modest at best, and for title queries none of the aggregation techniques even beat the LTR baseline.
97According to the original paper, this was accomplished with a multi-layer perceptron; however, our description
is more accurate, based on personal communications with the first author.
98The ranking model used was query-likelihood with Dirichlet smoothing (µ = 2500); this detail was omitted
from the original paper, filled in here based on personal communications with the authors.
73
Robust04
nDCG@20
Method Avg. Length SDM MaxP
(1) Title 3 0.427 0.469
(2a) Description 14 0.404 0.529
(2b) Description, keywords 7 0.427 0.503
(3a) Narrative 40 0.278 0.487
(3b) Narrative, keywords 18 0.332 0.471
(3c) Narrative, negative logic removed 31 0.272 0.489
Table 13: The effectiveness of SDM and BERT–MaxP using different query types on the Robust04
test collection.
Pre–fine-tuning BERT–FirstP on a Bing query log significantly improves effectiveness, row (5),
demonstrating that BERT can be effective in this setting with sufficient training data.99 Since it is
unclear what conclusions can be drawn from the web test collections, we focus the remainder of our
analysis on Robust04. Comparing the different aggregation techniques, the MaxP approach appears
to yield the highest effectiveness. The low effectiveness of FirstP on Robust04 is not very surprising,
since it is not always the case that relevant material appears at the beginning of a news article. Results
show that SumP is almost as effective as MaxP, despite having the weakness that it performs no
length normalization; longer documents will tend to have higher scores, thus creating a systematic
bias against shorter documents.
Looking at the bag-of-words baseline, row (1), the results are generally consistent with the literature:
We see that short title queries are more effective than sentence-length description queries; the drop is
bigger for ClueWeb09b (web pages) than Robust04 (newswire articles). However, reranking with the
descriptions as input to BERT is significantly more effective that reranking with titles, at least for
Robust04. This means that BERT is able to take advantage of richer natural language descriptions of
the information need. This finding appears to be robust, as the Birch–Passage experimental results
shown in Table 11 confirm the higher effectiveness of description queries over title queries as well.
Dai and Callan [2019b] further investigated the intriguing finding that reranking documents using
description queries is more effective than title queries, as shown in Table 12. In addition to considering
the description and narrative fields from the Robust04 topics, they also explored a “keyword” version
of those fields, stripped of punctuation as well as stopwords. For the narrative, they also discarded
“negative logic” that may be present in the prose. For example, consider topic 697:
Title: air traffic controller
Description: What are working conditions and pay for U.S. air traffic controllers?
Narrative: Relevant documents tell something about working conditions or pay
for American controllers. Documents about foreign controllers or individuals are
not relevant.
In this topic, the second sentence in the narrative states relevance in a negative way, i.e., what makes
a document not relevant. These are removed in the “negative logic removed” condition.
Results of these experiments are shown in Table 13, where the rows show the different query
conditions described above.100 For each of the conditions, the average length of the query is provided:
as expected, descriptions are longer than titles, and narratives are even longer. It is also not surprising
that removing stopwords reduces the average length substantially. In these experiments, SDM (see
above) is taken as a point of comparison, since it represents a simple attempt to exploit “structure”
that is present in the query representations. Comparing the “title” query under SDM and the BoW
results in Table 12, we can confirm that SDM does indeed improve effectiveness.
The MaxP figures in the first two rows of Table 13 are identical to the numbers presented in Table 12
(same experimental conditions, just arranged differently). For SDM, we see that using description
99As a historical note, although Dai and Callan [2019b] did not use the terminology of pre–fine-tuning, this work
represents one of the earliest example of the technique, as articulated in Section 3.2.4.
100For these experiments, stopwords filtering in Indri (used for first-stage retrieval) was disabled (personal
communication with the authors).
74
queries decreases effectiveness compared to the title queries, row (2a). In contrast, BERT is able to
take advantage of the linguistically richer description field to improve ranking effectiveness, also
row (2a). If we use only the keywords that are present in the description (only about half of the
terms), SDM is able to “gain back” its lost effectiveness, row (2b). We also see from row (2b) that
removing stopwords and punctuation from the description decreases effectiveness with BERT–MaxP.
This is worth restating in another way: stopwords (that is, non-content words) contribute to ranking
effectiveness in the input sequence fed to BERT for inference. These terms, by definition, do not
contribute content; instead, they provide the linguistic structure to help the model estimate relevance.
This behavior makes sense because BERT was pretrained on well-formed natural language text, and
thus removing non-content words during fine-tuning and inference creates distributional mismatches
that degrade model effectiveness.
Looking at the narratives, which on average are over ten times longer than the title queries, we
see the same general pattern.101 SDM is not effective with long narrative queries, as it becomes
“confused” by extraneous words present that are not central to the information need, row (3a). By
focusing only on the keywords, SDM performs much better, but still worse than title queries, row (3b).
Removing negative logic has minimal impact on effectiveness compared to the full narrative queries,
as the queries are still quite long, row (3c). For BERT–MaxP, reranking with full topic narratives
beats reranking with only topic titles, but this is still worse than reranking with topic descriptions,
row (3a). As is consistent with the descriptions case, retaining only keywords hurts effectiveness,
demonstrating the important role that non-content words play. For BERT, removing the negative
logic has negligible effect overall, just as with SDM; there doesn’t seem to be sufficient evidence to
draw conclusions about each model’s ability to handle negations.
To further explore these findings, Dai and Callan [2019b] conducted some analyses of attention
patterns in their model, similar to some of the studies discussed in Section 3.2.2, although not in a
systematic manner. Nevertheless, they reported a few intriguing observations: for the description
query “Where are wind power installations located?”, a high-scoring passage contains the sentence
“There were 1,200 wind power installations in Germany.” Here, the preposition in the document
“in” received the strongest attention from the term “where” in the topic description. The preposition
appears in the phrase “in Germany”, which precisely answers a “where” question. This represents a
concrete example where non-content words play an important role in relevance matching: these are
exactly the types of terms that would be discarded with exact match techniques!
Are we able to make meaningful comparisons between Birch and BERT–MaxP based on available
experimental evidence? Given that they both present evaluation results on Robust04, there is
a common point for comparison. However, there are several crucial differences that make this
comparison difficult: Birch uses BERTLarge whereas BERT–MaxP uses BERTBase. All things being
equal, a larger (deeper) transformer model will be more effective. There are more differences:
BERT–MaxP only reranks the top k = 100 results from first-stage retrieval, whereas Birch reranks
the top k = 1000 hits. For this reason, computing MAP (at the standard cutoff of rank 1000) for
BERT–MaxP would not yield a fair comparison to Birch; however, as nDCG is an early-precision
metric, it is less affected by reranking depth. Additionally, Birch combines evidence from the original
BM25 document scores, whereas BERT–MaxP does not consider scores from first-stage retrieval
(cf. results of interpolation experiments in Section 3.2.2).
Finally, there is the issue of training. Birch operates in a zero-shot transfer setting, since it was
fine-tuned on the MS MARCO passage ranking test collection and TREC Microblog Track data;
Robust04 data was used only to learn the sentence weight parameters. In contrast, the BERT–MaxP
results come from fine-tuning directly on Robust04 data in a cross-validation setting. Obviously,
in-domain training data should yield higher effectiveness, but the heuristic of constructing overlapping
passages and simply assuming that they are relevant leads inevitably to noisy training examples. In
contrast, Birch benefits from far more training examples from MS MARCO (albeit out of domain). It
is unclear how to weigh the effects of these different training approaches.
In short, there are too many differences between Birch and BERT–MaxP to properly isolate and
attribute effectiveness differences to specific design choices, although as a side effect of evaluating
PARADE, a model we discuss in Section 3.3.4, Li et al. [2020a] presented experiment results that try
to factor away these differences. Nevertheless, on the whole, the effectiveness of the two approaches
is quite comparable: in terms of nDCG@20, 0.529 for BERT–MaxP with description, 0.533 for Birch
101Here, BERT is reranking results from title queries in first-stage retrieval.
75
with three sentences (MS MARCO → MB fine-tuning) reported in row (4c) of Table 10. Nevertheless,
at a high level, the success of these two models demonstrates the robustness and simplicity of
BERT-based approaches to text ranking. This also explains the rapid rise in the popularity of such
models—they are simple, effective, and easy to replicate.
Additional Studies. Padaki et al. [2020] followed up the work of Dai and Callan [2019b] to explore
the potential of using query expansion techniques (which we cover in Section 4.2) to generate better
queries for BERT-based rankers. In one experiment, they scraped Google’s query reformulation
suggestions based on the topic titles, which were then manually filtered to retain only those that were
well-formulated natural language questions semantically similar to the original topic descriptions.
While reranking using these suggestions was not as effective as reranking using the original topic
descriptions, they still improved over reranking with titles (keywords) only. This offers additional
supporting evidence that BERT not only exploits relevance signals in well-formed natural language
questions, but critically depends on them to achieve maximal effectiveness.
The work of Dai and Callan [2019b] was successfully replicated by Zhang et al. [2021] on Robust04
starting from an independent codebase. They performed additional experiments evaluating BERT–
MaxP on another dataset (Gov2) and investigated the effects of using a simple BERT variant in
place of BERTBase (see Section 3.2.2). The authors largely followed the experimental setup used
in the original work, with two different design choices intended to examine the generalizability
of the original results: a different set of folds was used and the first-stage retrieval results were
obtained using BM25 with RM3 expansion rather than using query likelihood. Results on Gov2 are
in agreement with those on Robust04 using both title and description queries: MaxP aggregation
outperformed FirstP and SumP, as well as a newly introduced AvgP variant that takes the mean of
document scores.
In terms of BERT variants, Zhang et al. [2021] experimented with RoBERTa (introduced in Section 3.2.2), ELECTRA (introduced in Section 3.3.1), and another model called ALBERT [Lan et al.,
2020]. ALBERT reduces the memory footprint of BERT by tying the weights in its transformer
layers together (i.e., it uses the same weights in every layer).
Results of Zhang et al. [2021] combining MaxP aggregation with different BERT variants are shown in
Table 14, copied directly from their paper. For convenience, we repeat the reference MaxP condition
of Dai and Callan [2019b] from row (4b) in Table 12 as row (1). Row group (2) shows the effect
of replacing BERT with one of its variants; none of these conditions used pre–fine-tuning. While
these model variants sometimes outperform BERTBase, Zhang et al. [2021] found that none of the
improvements were statistically significant according to a two-tailed t-test (p < 0.01) with Bonferroni
correction. It is worth noting that this includes the comparison between BERTBase and BERTLarge;
BERTBase appears to be more effective on Gov2 (although the difference is not statistically significant
either). Rows (3a) and (3b) focus on the comparison between BERTBase and ELECTRABase with
pre–fine-tuning on the MS MARCO passage ranking task (denoted “MSM pFT”). Zhang et al. [2021]
reported that the improvement in this case of ELECTRABase over BERTBase is statistically significant
in three of the four settings based on two-tailed t-test (p < 0.01) with Bonferroni correction. If we
combine this finding with the Birch–Passage results presented in Table 11, row (5b), there appears to
be multiple sources of evidence suggesting that ELECTRABase is more effective than BERTBase for
text ranking tasks.
Takeaway Lessons. There are two important takeaways from the work from Dai and Callan [2019b]:
• Simple maximum passage score aggregation—taking the maximum of all the passage relevance
scores as the document relevance score—works well. This is a robust finding that has been
replicated and independently verified.
• BERT can exploit linguistically rich descriptions of information needs that include non-content
words to estimate relevance, which appears to be a departure from previous keyword search
techniques.
The first takeaway is consistent with Birch results. Conceptually, MaxP is quite similar to the “1S”
condition of Birch, where the score of the top sentence is taken as the score of the document. Birch
reported at most small improvements, if any, when multiple sentences are taken into account, and no
improvements beyond the top three sentences. The effectiveness of both techniques is also consistent
with previous results reported in the information retrieval literature. There is a long thread of work,
76
Robust04 Gov2
nDCG@20 nDCG@20
Model Title Desc Title Desc
(1) BERT–MaxP = Table 12, row (4b) 0.469 0.529 - -
(2a) BERTBase 0.4767 0.5303 0.5175 0.5480
(2b) BERTLarge 0.4875 0.5448 0.5161 0.5420
(2c) ELECTRABase 0.4959 0.5480 0.4841 0.5152
(2d) RoBERTaBase 0.4938 0.5489 0.4679 0.5370
(2e) ALBERTBase 0.4632 0.5400 0.5354 0.5459
(3a) BERTBase (MSM pFT) 0.4857 0.5476 0.5473 0.5788
(3b) ELECTRABase (MSM pFT) 0.5225† 0.5741† 0.5624 0.6062†
Table 14: The effectiveness of different BERT variants using MaxP passage score aggregation on
the Robust04 and Gov2 test collections. Statistically significant increases in effectiveness over the
corresponding BERTBase model are indicated with the symbol † (two-tailed t-test, p < 0.01, with
Bonferroni correction).
dating back to the 1990s, that leverages passage retrieval techniques for document ranking [Salton
et al., 1993, Hearst and Plaunt, 1993, Callan, 1994, Wilkinson, 1994, Kaszkiel and Zobel, 1997, Clarke
et al., 2000]—that is, aggregating passage-level evidence to estimate the relevance of a document.
In fact, both the “Max” and “Sum” aggregation techniques were already explored over a quarter
of a century ago in Hearst and Plaunt [1993] and Callan [1994], albeit the source of passage-level
evidence was far less sophisticated than the transformer models of today.
Additional evidence from user studies suggest why BERT–MaxP and Birch work well: it has been
shown that providing users concise summaries of documents can shorten the amount of time required
to make relevance judgments, without adversely affecting quality (compared to providing users with
the full text) [Mani et al., 2002]. This finding was recently replicated and expanded upon by Zhang
et al. [2018], who found that showing users only document extracts reduced both assessment time
and effort in the context of a high-recall retrieval task. In a relevance feedback setting, presenting
users with sentence extracts in isolation led to comparable accuracy but reduced effort compared
to showing full documents [Zhang et al., 2020b]. Not only from the perspective of ranking models,
but also from the perspective of users, well-selected short extracts serve as good proxies for entire
documents for the purpose of assessing relevance. There are caveats, however: results presented
later in Section 3.3.5 suggest that larger portions of documents need to be considered to differentiate
between different grades of relevance (e.g., relevant vs. highly relevant).
3.3.3 Leveraging Contextual Embeddings: CEDR
Just as in applications of BERT to classification tasks in NLP (see Section 3.1), monoBERT, Birch,
and BERT–MaxP use only the final representation of the [CLS] token to compute query–document
relevance scores. Specifically, all of these models discard the contextual embeddings that BERT
produces for both the query and the candidate text. Surely, representations of these terms can also be
useful for ranking? Starting from this question, MacAvaney et al. [2019a] were the first to explore
the use of contextual embeddings from BERT for text ranking by incorporating them into pre-BERT
interaction-based neural ranking models. Their approach, Contextualized Embeddings for Document
Ranking (CEDR), addressed BERT’s input length limitation by performing chunk-by-chunk inference
over the document and then assembling relevance signals from each chunk.
From the scientific perspective, MacAvaney et al. [2019a] investigated whether BERT’s contextual
embeddings outperform static embeddings when used in a pre-BERT neural ranking model and
whether they are complementary to the more commonly used [CLS] representation. They hypothesized that since interaction-based models rely on the ability of the underlying embeddings to capture
semantic term matches, using richer contextual embeddings to construct the similarity matrix should
improve the effectiveness of interaction-based neural ranking models.
Specifically, CEDR uses one of three neural ranking models as a “base”: DRMM [Guo et al., 2016],
KNRM [Xiong et al., 2017], and PACRR [Hui et al., 2017]. Instead of static embeddings (e.g., from
GloVe), the embeddings that feed these models now come from BERT. In addition, the aggregate
[CLS] representation from BERT is concatenated to the other signals consumed by the feedforward
77
E1 E
2
E
3
F1 F2 Fm
…
…
E
… 1 E
2
E
3
F1 F2 Fm
…
…
…
E[CLS]
T[CLS]
[CLS]
E1
q1
E2
q2
E3
q3
E[SEP1]
[SEP]
F1
d1
F2
d2
Fm
dm
E[SEP2]
T[SEP2]
… [SEP]
… … … … … … … … … …
s
E1 E
2
E
3
F1 F2 Fm
…
…
…
U1 U2 U3 T[SEP1] V1 V2 … Vm
…
Figure 12: The architecture of CEDR, which comprises two main sources of relevance signals: the
[CLS] representation and the similarity matrix computed from the contextual embeddings of the
query and the candidate text. This illustration contains a number of intentional simplifications in
order to clearly convey the model’s high-level design.
network of each base model. Thus, query–document relevance scores are derived from two main
sources: the [CLS] token (as in monoBERT, Birch, and BERT–MaxP) and from signals derived from
query–document term similarities (as in pre-BERT interaction-based models). This overall design is
illustrated in Figure 12. The model is more complex than can be accurately captured in a diagram,
and thus we only attempt to highlight high-level aspects of the design.
To handle inputs longer than 512 tokens, CEDR splits documents into smaller chunks, as evenly as
possible, such that the length of each input sequence (complete with the query and special delimiter
tokens) is not longer than the 512 token maximum. BERT processes each chunk independently and
the output from each chunk is retained. Once all of a document’s chunks have been processed, CEDR
creates a document-level [CLS] representation by averaging the [CLS] representations from each
chunk (i.e., average pooling). The document-level [CLS] representation is then concatenated to
the relevance signals that are fed to the underlying interaction-based neural ranking model. Unlike
in monoBERT, Birch, and BERT–MaxP, which discard the contextual embeddings of the query
and candidate texts, CEDR concatenates the contextual embeddings of the document terms from
each chunk to form the complete sequence of contextual term embeddings for the entire document.
Similarity matrices are then constructed by computing the cosine similarity between each document
term embedding and each query term embedding from the first document chunk. Note that in this
design, BERT is incorporated into interaction-based neural ranking models in a way that retains the
differentiability of the overall model. This allows end-to-end training with relevance judgments and
provides the solution to the length limitations of BERT.
Given that the input size in a transformer encoder is equal to its output size, each layer in BERT can be
viewed as producing some (intermediate) contextual representation. Rather than using only the term
embeddings generated by BERT’s final transformer encoder layer, CEDR constructs one similarity
matrix for each layer. Analogously to how the [CLS] representation is handled, the relevance signals
from each matrix are concatenated together. Unlike the contextual embeddings, though, only the final
[CLS] representation is used. With the [CLS] representation and similarity matrix signals, CEDR
produces a final document relevance score by using the same series of fully-connected layers that is
used by the underlying base neural ranking model. In more detail:
78
Robust04 Web
Method Input Representation nDCG@20 nDCG@20
(1) BM25 n/a 0.4140 0.1970
(2) Vanilla BERT BERT (fine-tuned) [B] 0.4541 [B] 0.2895
(3a) PACRR GloVe 0.4043 0.2101
(3b) PACRR BERT 0.4200 0.2225
(3c) PACRR BERT (fine-tuned) [BVG] 0.5135 [BG] 0.3080
(3d) CEDR–PACRR BERT (fine-tuned) [BVG] 0.5150 [BVGN] 0.3373
(4a) KNRM GloVe 0.3871 [B] 0.2448
(4b) KNRM BERT [G] 0.4318 [B] 0.2525
(4c) KNRM BERT (fine-tuned) [BVG] 0.4858 [BVG] 0.3287
(4d) CEDR–KNRM BERT (fine-tuned) [BVGN] 0.5381 [BVG] 0.3469
(5a) DRMM GloVe 0.3040 0.2215
(5b) DRMM BERT 0.3194 [BG] 0.2459
(5c) DRMM BERT (fine-tuned) [G] 0.4135 [BG] 0.2598
(5d) CEDR–DRMM BERT (fine-tuned) [BVGN] 0.5259 [BVGN] 0.3497
Table 15: The effectiveness of CEDR variants on Robust04 and the test collections from the TREC
2012–2014 Web Tracks. Significant improvements (paired t-tests, p < 0.05) are indicated in brackets,
over BM25, Vanilla BERT, the corresponding model trained with GloVe embeddings, and the
corresponding Non-CEDR model (i.e., excluding [CLS] signals).
• CEDR–DRMM uses a fully-connected layer with five output nodes and a ReLU non-linearity
followed by a fully-connected layer with a single output node.
• CEDR–KNRM uses one fully-connected layer with a single output node.
• CEDR–PACRR uses two fully-connected layers with 32 output nodes and ReLU non-linearities
followed by a fully-connected layer with a single output node.
All variants are trained using a pairwise hinge loss and initialized with BERTBase. The final query–
document relevance scores are then used to rerank a list of candidate documents.
As a baseline model for comparison, MacAvaney et al. [2019a] proposed what they called “Vanilla
BERT”, which is an ablated version of CEDR that uses only the signals from the [CLS] representations. Specifically, documents are split into chunks in exactly the same way as the full CEDR model
and the [CLS] representations from each chunk are averaged before feeding a standard relevance
classifier (as in monoBERT, Birch, and BERT–MaxP). This ablated model quantifies the effectiveness
impact of the query–document term interactions.
Results and Analysis. CEDR was evaluated using Robust04 and a non-standard combination of
datasets from the TREC 2012–2014 Web Tracks that we simply denote as “Web” (see Section 2.7
and the original paper for details). Results in terms of nDCG@20 are shown in Table 15, with figures
copied directly from MacAvaney et al. [2019a]. CEDR was deployed as a reranker over BM25 results
from Anserini, the same as Birch. However, since CEDR only reranks the top k = 100 hits (as
opposed to k = 1000 hits in Birch), the authors did not report MAP. Nevertheless, since nDCG@20
is an early-precision metric, the scores can be meaningfully compared. Copying the conventions used
by the authors, the prefix before each result in brackets denotes significant improvements over BM25,
Vanilla BERT, the corresponding model trained with GloVe embeddings, and the corresponding
Non-CEDR model (i.e., excluding [CLS] signals), based on paired t-tests (p < 0.05).
In Table 15, each row group represents a particular “base” interaction-based neural ranking model,
where the rows with the “CEDR–” prefix denote the incorporation of the [CLS] representations. The
“Input Representation” column indicates whether static GloVe embeddings [Pennington et al., 2014]
or BERT’s contextual embeddings are used. When using contextual embeddings, the original versions
from BERT may be used or the embeddings may be fine-tuned on the ranking task along with the
underlying neural ranking model. When BERT is fine-tuned on the ranking task, a Vanilla BERT
model is first fine-tuned before training the underlying neural ranking model. That is, BERT is first
fine-tuned in the Vanilla BERT configuration for relevance classification, and then it is fine-tuned
further in conjunction with a particular interaction-based neural ranking model. This is another
example of the multi-step fine-tuning strategy discussed in Section 3.2.4.
79
Robust04
Method Configuration Reference nDCG@20
Birch 3S: BERT(MS MARCO → MB) Table 10, row (4c) 0.533
BERT–MaxP Description Table 12, row (4b) 0.529
CEDR–KNRM BERT (fine-tuned) Table 15, row (4d) 0.538
Table 16: The effectiveness of the best Birch, BERT–MaxP, and CEDR configurations on the
Robust04 test collection.
Let us examine these results. First, consider whether contextual embeddings improve over static
GloVe embeddings: the answer is clearly yes.
102 Even without fine-tuning on the ranking task, BERT
embeddings are more effective than GloVe embeddings across all models and datasets, which is likely
attributable to their ability to better capture term context. This contrast is shown in the (b) rows vs.
the (a) rows. Fine-tuning BERT yields additional large improvements for most configurations, with
the exception of DRMM on the Web data. These results are shown in the (c) rows vs. the (b) rows.
Next, consider the effectiveness of using only contextual embeddings in an interaction-based neural
ranking model compared to the effectiveness of using only the [CLS] representation, represented
by Vanilla BERT in row (2). When using contextual embeddings, the PACRR and KNRM models
perform substantially better than Vanilla BERT; see the (c) rows vs. row (2). DRMM does not appear
to be effective in this configuration, however, as shown in row (5c). This may be caused by the fact
that DRMM’s histograms are not differentiable, which means that BERT is fine-tuned using only the
relevance classification task (i.e., BERT weights are updated when Vanilla BERT is first fine-tuned,
but the weights are not updated when DRMM is further fine-tuned). Nevertheless, there is some
reason to suspect that the effectiveness of Vanilla BERT is under-reported, perhaps due to some
training issue, because an equivalent approach by Li et al. [2020a] is much more effective (more
details below).
Finally, consider whether the [CLS] representation from BERT is complementary to the contextual
embeddings from the remaining tokens in the input sequence. The comparison is shown in the (d)
rows vs. the (c) rows, where CEDR–PACRR, CEDR–KNRM, and CEDR–DRMM represent the full
CEDR model that incorporates the [CLS] representations on top of the models that use fine-tuned
contextual embeddings. In all cases, incorporating the [CLS] representations improve effectiveness
and the gains are significant in the majority of cases.
A natural question that arises is how CEDR compares to Birch (Section 3.3.1) and BERT–MaxP
(Section 3.3.2), the two other contemporaneous models in the development of BERT for ranking
full documents. Fortunately, all three models were evaluated on Robust04 and nDCG@20 was
reported for those experiments, which offers a common reference point. Table 16 summarizes the
best configuration of each model. While the experimental setups are different, which prevents a fair
direct comparison, we can see that the effectiveness scores all appear to be in the same ballpark. This
point has already been mentioned in Section 3.3.2 but is worth repeating: it is quite remarkable that
three ranking models with different designs, by three different research groups with experiments
conducted on independent implementations, all produce similar results. This provides robust evidence
that BERT does really “work” for text ranking.
The connection between Birch and BERT–MaxP has already been discussed in the previous section,
but both models are quite different from CEDR, which has its design more firmly rooted in pre-BERT
interaction-based neural ranking models. Specifically, Birch and BERT–MaxP are both entirely
missing the explicit similarity matrix between query–document terms that forms a central component
in CEDR, and instead depend entirely on the [CLS] representations. The CEDR experiments
unequivocally show that contextual embeddings from BERT improve the quality of the relevance
signals extracted from interaction-based neural ranking models and increase ranking effectiveness,
but the experiments are not quite so clear on whether the explicit interactions are necessary to begin
with. In fact, there is evidence to suggest that with BERT, explicit interactions are not necessary:
from the discussion in Section 3.2, it might be the case that BERT’s all-to-all attention patterns at
each transformer layer, in effect, already capture all possible term interactions.
102Apart from contextualization, GloVe embeddings also differ in that some terms may be out-of-vocabulary.
MacAvaney et al. [2019a] attempted to mitigate this issue by ensuring that terms always have a similarity of
one with themselves.
80
Robust04
No interpolation with BM25 + RM3
nDCG@20 nDCG@20
Method Title Desc Title Desc
(1) BM25 0.4240 0.4058 - -
(2) BM25 + RM3 0.4514 0.4307 - -
(3a) KNRM w/ FT BERTBase (no pFT) = Table 15, row (3c) 0.4858 - - -
(3b) CEDR–KNRM w/ FT BERTBase (no pFT) = Table 15, row (3d) 0.5381 - - -
(4a) KNRM w/ FT ELECTRABase (MSM pFT) 0.5470 0.6113 - -
(4b) CEDR–KNRM w/ FT ELECTRABase (MSM pFT) 0.5475 0.5983 - -
(5a) KNRM w/ FT BERTBase (no pFT) 0.5027†‡ 0.5409†‡ 0.5183†‡ 0.5532†‡
(5b) KNRM w/ FT ELECTRABase (no pFT) 0.5505 0.5954 0.5454 0.6016
(6a) CEDR–KNRM w/ FT BERTBase (no pFT) 0.5060†‡ 0.5661†‡ 0.5235†‡ 0.5798†‡
(6b) CEDR–KNRM w/ FT ELECTRABase (no pFT) 0.5326 0.5905 0.5536 0.6010
Table 17: The effectiveness of CEDR variants on the Robust04 test collection using title and
description queries with and without BM25 + RM3 interpolation. In rows (5a) and (6a), statistically
significant decreases in effectiveness from row (5b) and row (6b) are indicated with the symbol † and
the symbol ‡, respectively (two-tailed paired t-test, p < 0.05, with Bonferroni correction).
Additional Studies. As already noted above, the Vanilla BERT experimental results by MacAvaney
et al. [2019a] are not consistent with follow-up work reported by Li et al. [2020a] (more details in
the next section). Researchers have also reported difficulties reproducing results for CEDR–KNRM
and ablated variants using the authors’ open-source code with BERTBase.
103 In response, the CEDR
authors have recommended resolving these issues by replacing BERTBase with ELECTRABase and
also adopting the Capreolus toolkit [Yates et al., 2020] as the reference implementation of CEDR.
Further experiments by Li et al. [2020a] with Capreolus have confirmed that CEDR is effective when
combined with ELECTRABase, but they have not affirmed the finding by MacAvaney et al. [2019a]
that the [CLS] token is complementary to the contextual embeddings.
Experimental results copied from Li et al. [2020a] are shown in rows (4a) and (4b) of Table 17.
Comparing these rows, there does not appear to be any benefit to using the [CLS] token with title
queries, and using the [CLS] token actually reduces effectiveness with description queries. Note that
the results in row groups (3) and (4) are not comparable because the latter configurations have the
additional benefit of pre–fine-tuning on the MS MARCO passage dataset, indicated by “MSM pFT”.
To better understand the reproduction difficulties with the CEDR codebase, we replicated some of
the important model configurations using the Capreolus toolkit [Yates et al., 2020] to obtain new
results with the different CEDR–KNRM conditions; these results have to date not been reported
elsewhere. In particular, we consider the impact of linear interpolation with the first-stage retrieval
scores, the impact of using different BERT variants, and the impact of using title vs. description
queries. These experiments used the same first-stage ranking, folds, hyperparameters, and codebase
as Li et al. [2020a], allowing meaningful comparisons. Results are shown in row groups (5) and (6)
in Table 17 and are directly comparable to the results in row group (4), but note that these results do
not benefit from pre–fine-tuning.
We can view row (5a) as a replication attempt of the CEDR results in row (3a), and row (6a) as a
replication attempt of the CEDR results in row (3b), since the latter in each pair of comparisons is
based on an independent implementation. The results do appear to confirm the reported issues with
reproducing CEDR using the original codebase by MacAvaney et al. However, this concern also
appears to be assuaged by the authors’ recommendation of replacing BERT with ELECTRA. While
the original CEDR paper found that including the [CLS] token improved over using only contextual
embeddings, row (3a) vs. (3b), the improvement is inconsistent in our replication, as seen in row (5a)
vs. (6a) and row (5b) vs. (6b).
Thus, to be clear, our results here support the finding by MacAvaney et al. that incorporating contextual
embeddings in a pre-BERT interaction-based model can be effective (i.e., outperforms non-contextual
embeddings), but our experiments do not appear to support the finding that incorporating the [CLS]
token further improves effectiveness. Comparing row (5a) with (5b) and row (6a) with (6b) in
103See https://github.com/Georgetown-IR-Lab/cedr/issues/22.
81
Table 17, we see that variants using ELECTRABase consistently outperform those using BERTBase.
Moreover, considering the results reported by Li et al. [2020a], in row group (4), we see that the
improvements from pre–fine-tuning are less consistent than those reported by Zhang et al. [2021]
(see Section 3.3.2). Pre–fine-tuning ELECTRA–KNRM slightly reduces effectiveness on descripton
queries but improves effectiveness on title queries, row (4a) vs. row (5b). CEDR–KNRM benefits
from pre–fine-tuning with both query types, but the improvement is larger for title queries, rows (4b)
and (6b). With the exception of row (5b), interpolating the reranker’s retrieval scores with scores
from first-stage retrieval improves effectiveness.
Takeaway Lessons. Despite some lack of clarity in the experimental results presented by MacAvaney
et al. [2019a] in being able to unequivocally attribute effectiveness gains to different architectural
components of the overall ranking model, CEDR to our knowledge is the first end-to-end differentiable
BERT-based ranking model for full-length documents. While Birch and BERT–MaxP could have
been modified to be end-to-end differentiable—for example, as Li et al. [2020a] have done with
Birch–Passage, presented in Section 3.3.1—neither Akkalyoncu Yilmaz et al. [2019b] nor Dai and
Callan [2019b] made this important leap. This strategy of handling long documents by aggregating
contextual term embeddings was later adopted by Boytsov and Kolter [2021]. The CEDR design has
two important advantages: the model presents a principled solution to the length limitations of BERT
and allows uniform treatment of both training and inference (reranking). Our replication experiments
confirm the effectiveness of using contextual embeddings to handle ranking long texts, but the role of
the [CLS] token in the complete CEDR architecture is not quite clear.
3.3.4 Passage Representation Aggregation: PARADE
PARADE [Li et al., 2020a], which stands for Passage Representation Aggregation for Document
Reranking, is a direct descendant of CEDR that also incorporates lessons learned from Birch and
BERT–MaxP. The key insight of PARADE, building on CEDR, is to aggregate the representations of
passages from a long text rather than aggregating the scores of individual passages, as in Birch and
BERT–MaxP. As in CEDR, this design yields an end-to-end differentiable model that can consider
multiple passages in unison, which also unifies training and inference. However, PARADE abandons
CEDR’s connection to pre-BERT neural ranking models by discarding explicit term-interaction
similarity matrices. The result is a ranking model that is simpler than CEDR and generally more
effective.
More precisely, PARADE is a family of models that splits a long text into passages and performs
representation aggregation on the [CLS] representation from each passage. Specifically, PARADE
splits a long text into a fixed number of fixed-length passages. When texts contain fewer passages,
the passages are padded and masked out during representation aggregation. When texts contain more
passages, the first and last passages are always retained, but the remaining passages are randomly
sampled. Consecutive passages partially overlap to minimize the chance of separating relevant
information from its context.
A passage representation p

queries. Statistically significant differences in effectiveness between a given method and the full
PARADE model are indicated with the symbol † (two-tailed paired t-test, p < 0.05).
Compared to the baselines, the full PARADE model and PARADECNN consistently outperforms
ELECTRA–MaxP, row (4), and almost always outperforms Birch, row (3), and CEDR, row (5).
In addition to providing a point of comparison for PARADE, these experiments also shed additional
insight about differences between Birch, ELECTRA–MaxP, and CEDR in the same experimental
setting. Here, it is worth spending some time discussing these results, independent of PARADE.
Confirming the findings reported by Dai and Callan [2019b], the effectiveness of all models increases
when moving from Robust04 title queries to description queries. However, the results are more mixed
on Gov2, and description queries do not consistently improve across metrics. ELECTRA–KNRM
(5a) and CEDR–KNRM (5b) are comparable in terms of effectiveness to Birch–Passage on Robust04
but generally better on Gov2. All Birch and CEDR variants are substantially more effective than
ELECTRA–MaxP, providing further support for the claim that considering multiple passages from a
single document passage can improve relevance predictions.
Takeaway Lessons. We see two main takeaways from PARADE, both building on insights initially
demonstrated by CEDR: First, aggregating passage representations appears to be more effective than
aggregating passages scores. By the time a passage score is computed, a lot of the relevance signal
has already been “lost”. In contrast, passage representations are richer and thus allow higher-level
components to make better decisions about document relevance. Second, chunking a long text and
performing chunk-level inference can be an effective strategy to addressing the length restrictions of
85
BERT. In our opinion, this approach is preferable to alternative solutions that try to directly increase
the maximum length of input sequences to BERT [Tay et al., 2020] (see next section). The key to
chunk-wise inference lies in properly aggregating representations that emerge from inference over the
individual chunks. Pooling, particularly max pooling, is a simple and effective technique, but using
another transformer to aggregate the individual representations appears to be even more effective,
suggesting that there are rich signals present in the sequence of chunk-level representations. This
hierarchical approach to relevance modeling retains the important model property of differentiability,
enabling the unification of training and inference.
3.3.5 Alternatives for Tackling Long Texts
In addition to aggregating passage scores or representations, two alternative strategies have been
proposed for ranking long texts: making use of passage-level relevance labels and modifying the
transformer architecture to consume long texts more efficiently. We discuss both approaches below.
Passage-level relevance labels. As an example of the first strategy, Wu et al. [2020b] considered
whether having graded passage-level relevance judgments at training time can lead to a more effective ranking model. This approach avoids the label mismatch at training time (for example,
with MaxP) since passage-level judgments are used. To evaluate whether this approach improves
effectiveness, the authors annotated a corpus of Chinese news articles with passage-level cumulative
gain, defined as the amount of relevant information a reader would encounter after having read a
document up to a given passage. Here, the authors operationalized passages as paragraphs. The
document-level cumulative gain is then, by definition, the highest passage-level cumulative gain,
which is the cumulative gain reached after processing the entire document. Based on these human
annotations, Wu et al. [2020b] made the following two observations:
• On average, highly-relevant documents are longer than other types of documents, measured
both in terms of the number of passages and the number of words.
• The higher the document-level cumulative gain, the more passages that need to be read by a
user before the passage-level cumulative gain reaches the document-level cumulative gain.
These findings suggest that whether a document is relevant can be accurately predicted from its
most relevant passage—which is consistent with BERT–MaxP and Birch, as well as the user studies
discussed in Section 3.3.2. However, to accurately distinguish between different relevance grades (e.g.,
relevant vs. highly-relevant), a model might need to accumulate evidence from multiple passages,
which suggests that BERT–MaxP might not be sufficient. Intuitively, the importance of observing
multiple passages is related to how much relevance information accumulates across the full document.
To make use of their passage-level relevance labels, Wu et al. [2020b] proposed the Passage-level
Cumulative Gain model (PCGM), which begins by applying BERT to obtain individual query–passage
representations (i.e., the final representation of the [CLS] token). The sequence of query–passage
representations is then aggregated with an LSTM, and the model is trained to predict the cumulative
gain after each passage. An embedding of the previous passage’s predicted gain is concatenated to
the query–passage representation to complete the model. At inference time, the gain of a document’s
final passage is used as the document-level gain. One can think of PCGM as a principled approach to
aggregating evidence from multiple passages, much like PARADE, but adds the requirement that
passage-level gain labels are available. PCGM has two main advantages: the LSTM is able to model
and extract signal from the sequence of passages, and the model is differentiable and thus amenable
to end-to-end training.
The PCGM model was evaluated on two Chinese test collections. While experimental results
demonstrate some increase in effectiveness over BERT–MaxP, the increase was not statistically
significant. Unfortunately, the authors did not evaluate on Robust04, and thus a comparison to other
score and passage aggregation approaches is difficult. However, it is unclear whether the lack of
significant improvements is due to the design of the model, the relatively small dataset, or some
issue with the underlying observations about passage-level gains. Nevertheless, the intuitions of Wu
et al. [2020b] in recognizing the need to aggregate passage representations do appear to be valid, as
supported by the experiments with PARADE in Section 3.3.4.
Transformer architectures for long texts. Researchers have proposed a variety of techniques to
directly apply the transformer architecture to long documents by reducing the computational cost of
86
MS MARCO Doc (Dev) TREC 2019 DL Doc
Method MRR@10 nDCG@10 MAP
(1) Birch (BM25 + RM3) - 0.640 0.328
(2) Sparse-Transformer 0.328 0.634 0.257
(3) Longformer-QA 0.326 0.627 0.255
(4) QDS-Transformer 0.360 0.667 0.278
Table 20: The effectiveness of efficient transformer variants on the development set of the MS
MARCO document ranking task and the TREC 2019 Deep Learning Track document ranking test
collection.
its attention mechanism, which is quadratic with respect to the sequence length (see discussion in
Section 3.3).
Kitaev et al. [2020] proposed the Reformer, which replaces standard dot-product attention by a design
based on locality-sensitive hashing to efficiently compute attention only against the most similar
tokens, thus reducing model complexity from O(L
2
) to O(Llog L), where L is the length of the
sequence. Another solution, dubbed Longformer by Beltagy et al. [2020], addressed the blow-up in
computational costs by sparsifying the all-to-all attention patterns in the basic transformer design
through the use of a sliding window to capture local context and global attention tokens that can be
specified for a given task. Researchers have begun to apply Longformer-based models to ranking
long texts [Sekulic et al., 2020, Jiang et al., 2020]. ´
Jiang et al. [2020] proposed the QDS-Transformer, which is a Longformer model where the query
tokens are global attention tokens (i.e., each query term attends to all query and document terms). The
authors evaluated the QDS-Transformer on the MS MARCO document ranking test collection and on
the TREC 2019 Deep Learning Track document ranking test collection where they reranked the BM25
results provided by the track organizers. QDS-Transformer was compared against Longformer-QA,
which adds a special token to the query and document for global attention, as proposed by Beltagy
et al. [2020], and Sparse-Transformer [Child et al., 2019], which uses local attention windows with
no global attention.
Experimental results are shown in Table 20. The Sparse-Transformer and Longformer-QA models
perform similarly, rows (2) and (3), suggesting that the global token approach used by LongformerQA does not represent an improvement over the local windows used by Sparse-Transformer. QDSTransformer, row (4), outperforms both approaches, which suggests that treating the query tokens as
global attention tokens is important. For context, we present the closest comparable Birch condition
we could find in row (1); this corresponds to run bm25_marcomb submitted to the TREC 2019 Deep
Learning Track [Craswell et al., 2020], which reranked the top 1000 hits from BM25 + RM3 as
first-stage retrieval. The higher MAP of Birch is likely due to a deeper reranking depth, but the
effectiveness of QDS-Transformer is only a little bit higher. For Robust04, Jiang et al. [2020] reported
an nDCG@20 of 0.457, which is far lower than many of the figures reported in this section. Although
there aren’t sufficient common reference points, taken as a whole, it is unclear if QDS-Transformer is
truly competitive compared to many of the models discussed earlier.
Takeaway Lessons. While replacing all-to-all attention lowers the computational complexity in the
alternative transformer architectures discussed in this section, it is not clear whether they can match
the effectiveness of reranking methods based either on score or representation aggregation. Note that
the strategy of sparsifying attention patterns leads down the road to an architecture that looks quite
like PARADE. In PARADE’s hierarchical model, a second lightweight transformer is applied to the
[CLS] representations from the individual passages, but this design is operationally identical to a
deeper transformer architecture where the top few layers adopt a special attention pattern (e.g., via
masking). In fact, we might go as far to say that hierarchical transformers and selective sparsification
of attention are two ways of describing the same idea.
3.4 From Single-Stage to Multi-Stage Rerankers
The applications of BERT to text ranking that we have covered so far operate as rerankers in a
retrieve-and-rerank setup, which as we have noted dates back to at least the 1960s [Simmons, 1965].
87
Inverted
Index
Initial
Retrieval
Texts
Ranked List
Candidate
Texts
Queries
Reranker
Inverted
Index
Initial
Retrieval
Texts
Candidate
Texts
Queries
Reranker
Reranked
Candidates
Reranker … Reranker
Ranked List
Figure 14: A retrieve-and-rerank design (top) is the simplest instantiation of a multi-stage ranking
architecture (bottom). In multi-stage ranking, the candidate generation stage (also called initial
retrieval or first-stage retrieval) is followed by more than one reranking stages.
An obvious extension of this design is to incorporate multiple reranking stages as part of a multi-stage
ranking architecture, as shown in Figure 14. That is, following candidate generation or first-stage
retrieval, instead of having just a single reranker, a system could have an arbitrary number of reranking
stages, where the output of each reranker feeds the input to the next. This basic design goes by a few
other names as well: reranking pipelines, ranking cascades, or “telescoping”.
We formalize the design as follows: a multi-stage ranking architecture comprises N reranking stages,
denoted H1 to HN . We refer to the candidate generation stage (also called initial retrieval or first-stage
retrieval) as H0, which retrieves k0 texts from the corpus to feed the rerankers. Candidate generation
is typically accomplished using an inverted index, but may exploit dense retrieval techniques or
dense–sparse hybrids as well (see Section 5). Each stage Hn, n ∈ {1, . . . N} receives a ranked list
Rn−1 comprising kn−1 candidates from the previous stage. Each stage, in turn, provides a ranked
list Rn comprising kn candidates to the subsequent stage, with the requirement that kn ≤ kn−1.
105
The ranked list generated by the final stage HN is the output of the multi-stage ranking architecture.
This description intentionally leaves unspecified the implementation of each reranking stage, which
could be anything ranging from decisions made based on the value of a single hand-crafted feature
(known as a “decision stump”) to a sophisticated machine-learned model (for example, based on
BERT). Furthermore, each stage could decide how to take advantage of scores from the previous
stage: one common design is that scores from each stage are additive, or a reranker can decide to
completely ignore previous scores, treating the previous candidate texts as an unordered set.
One practical motivation for the development of multi-stage ranking is to better balance tradeoffs
between effectiveness (most of the time, referring to the quality of the ranked lists) and efficiency
(for example, retrieval latency or query throughput). Users, of course, demand systems that are both
“good” and “fast”, but in general, there is a natural tradeoff between these two desirable characteristics.
Multi-stage ranking evolved in the context of learning to rank (see Section 1.2.3): For example,
compared to unigram features (i.e., of individual terms) such as BM25 scores, many n-gram features
are better signals of relevance, but also more computationally expensive to compute, in both time and
space. To illustrate: one helpful feature is the count of query n-grams that occur in a text (that is, the
ranking model checks whether matching query terms are contiguous). This is typically accomplished
by storing the positions of terms in the text (which consumes space) and intersecting lists of term
positions (within individual documents) to determine whether the terms appear contiguously (which
takes time). Thus, we see a common tradeoff between feature cost and output quality, and more
generally, between effectiveness and efficiency.
105We leave aside a minor detail here in that a stage can return a ranked list of a particular length, and the next
stage may choose to truncate that list prior to processing. The net effect is the same; a single parameter kn is
sufficient to characterize such a design.
88
Thus, a ranking model (e.g., learning to rank) that takes advantage of “expensive” features will often
be slow, since inference must be performed on every candidate. Latency increases linearly with the
number of candidates considered and can be managed by varying the depth of first-stage retrieval,
much like the experiments presented in Section 3.2.2 in the context of monoBERT. However, it is
desirable that the candidate pool contains as many relevant texts as possible (i.e., have high recall), to
maximize the opportunities for a reranker to identify relevant texts; obviously, rerankers are useless
if there are no relevant texts in the output of first-stage retrieval to process. Thus, designers of
production real-world systems are faced with an effectiveness/efficiency tradeoff.
The intuition behind the multi-stage design is to exploit expensive features only when necessary:
earlier stages in the reranking pipeline can use “cheap” features to discard candidates that are easy to
distinguish as not relevant; “expensive” features can then be brought to bear after the “easy” nonrelevant candidates have been discarded. Latency can be managed because increasingly expensive
features are computed on fewer and fewer candidates. Furthermore, reranking pipelines can exploit
“early exits” that bypass later stages if the results are “good enough” [Cambazoglu et al., 2010]. In
general, the multi-stage design provides system designers with tools to balance effectiveness and
efficiency, often leading to systems that are both “good” and “fast”.106
The development of this idea in modern times has an interesting history. It had been informally known
by many in the information retrieval community since at least the mid-2000s that Microsoft’s Bing
search engine adopted a multi-stage design; for one, it was the most plausible approach for deploying
the learning-to-rank models they were developing at the time [Burges et al., 2005]. However, the
earliest “official” public acknowledgment we are aware of appears to be in a SIGIR 2010 Industry
Track keynote by Jan Pedersen, whose presentation included a slide that explicitly showed this
multi-stage architecture. Bing named these stages “L0” through “L4”, with “L0” being “Boolean
logic” (understood to be conjunctive query processing, i.e., the “ANDing” of query terms), “L1”
being “IR score” (understood to be BM25), and “L2/L3/L4” being machine-learned models. Earlier
that year, a team of authors from Yahoo! [Cambazoglu et al., 2010] described a multi-stage ranking
architecture in the form of with additive ensembles (the score of each stage is added to the score of
the previous stages). However, the paper did not establish a clear connection to production systems.
In the academic literature, Matveeva et al. [2006] described the first known instance of multi-stage
ranking (“nested” rankers, as the authors called it). The term “telescoping” was used to describe
the pruning process where candidates were discarded between stages. Interestingly, the paper was
motivated by high-accuracy retrieval and did not discuss the implications of their techniques on
system latency. Furthermore, while four of the five co-authors were affiliated with Bing, the paper
provided no indications of or connections to the design of the production search engine. One of
the earliest academic papers to include efficiency objectives in learning to rank was by Wang et al.
[2010], who explicitly modeled feature costs in a framework to jointly optimize effectiveness and
efficiency; cf. [Xu et al., 2012]. In a follow-up, Wang et al. [2011] proposed a boosting algorithm
for learning ranking cascades to directly optimize this quality/speed tradeoff. Within the academic
literature, this is the first instance we are aware of that describes learning the stages in a multi-stage
ranking architecture. Wang et al. coined the term “learning to efficiently rank” to describe this thread
of research. Nevertheless, it is clear that industry led the way in explorations of this design, but since
there is paucity of published material about production systems, we have no public record of when
various important innovations occurred and when they were deployed.
Since the early 2010s, multi-stage ranking architectures have received substantial interest in the
academic literature [Tonellotto et al., 2013, Asadi and Lin, 2013, Capannini et al., 2016, Clarke
et al., 2016, Chen et al., 2017c, Mackenzie et al., 2018] as well as industry. Beyond Bing, publicly
documented production deployments of such an architecture at scale include Alibaba’s e-commerce
search engine [Liu et al., 2017] and elsewhere within Alibaba as well [Yan et al., 2021], Baidu’s web
search engine [Zou et al., 2021], and Facebook search [Huang et al., 2020]. In fact, Facebook writes:
Facebook search ranking is a complex multi-stage ranking system where each stage
progressively refines the results from the preceding stage. At the very bottom of
106Note an important caveat here is the assumption that users only desire a few relevant documents, as is typical
in web search and operationalized in terms of early-precision metrics. Multi-stage architectures might not
be as useful if users desire high recall, which is important for many scenarios in the medical domain (for
example, systematic reviews) or the legal domain (for example, patent search).
89
this stack is the retrieval layer, where embedding based retrieval is applied. Results
from the retrieval layer are then sorted and filtered by a stack of ranking layers.
We see that multi-stage ranking remains very much relevant in the neural age. While keyword-based
retrieval has been replaced with retrieval using learned dense representations (see Section 5) as the
first stage in this case, and subsequent reranking stages are now primarily driven by neural models,
the general multi-stage design has not changed.
Having provided sufficient background, the remainder of this section presents a few multi-stage
ranking architectures specifically designed around transformer models. Section 3.4.1 describes a
reranking approach that explicitly compares the relevance of pairs of texts in a single inference step,
which can be logically extended to assessing the relevance of lists of texts, which we describe in
Section 3.4.2. We then present cascade transformers in Section 3.4.3, which treat transformer layers
as reranking stages.
3.4.1 Reranking Pairs of Texts
The first application of transformers in a multi-stage ranking architecture was described by Nogueira
et al. [2019a] as a solution for mitigating the quadratic computational costs associated with a ranking
model that applies inference on an input template that incorporates pairs of texts, as we explain below.
Recall that monoBERT turns ranking into a relevance classification problem, where we sort texts by
P(Relevant = 1|di
, q) given a query q and candidates {di}. In the terminology of learning to rank,
this model is best described as a “pointwise” approach since each text is considered in isolation during
training [Liu, 2009, Li, 2011]. An alternative is a “pairwise” approach, which focuses on comparisons
between pairs of documents. Intuitively, pairwise ranking has the advantage of harnessing signals
present in other candidate texts to decide if a text is relevant to a given query; these comparisons are
also consonant with the notion of graded relevance judgments (see Section 2.5).
The “duoBERT” model proposed by Nogueira et al. [2019a] operationalizes this intuition by explicitly
considering pairs of text. In this ranking model, BERT is trained to estimate the following:
P(di  dj |di
, dj , q), (30)
where di  dj is a commonly adopted notation for stating that di
is more relevant than dj (with
respect to the query q).
Before going into details, there are two conceptual challenges to realizing this ranking strategy:
1. The result of model inferences comprises a set of pairwise comparisons between candidate texts.
Evidence from these pairs still need to be aggregated to produce a final ranked list.
2. One simple implementation is to compare each candidate to every other candidate (e.g., from
first-stage retrieval), and thus the computational costs increase quadratically with the size of the
candidate set. Since monoBERT’s effectiveness increases with the size of the candidates set
(see Section 3.2), there emerges an effectiveness/efficiency tradeoff that needs to be controlled.
Nogueira et al. [2019a] proposed a number of evidence aggregation strategies (described below) to
tackle the first challenge and adopts a multi-stage ranking architecture to address the second challenge.
In summary, in a multi-stage design, a relevance classifier can be used to select a smaller set of
candidates from first-stage retrieval to be fed to the pairwise reranker.
The duoBERT model is trained to estimate pi,j , the probability that di  dj , i.e., candidate di
is
more relevant than dj . It takes as input a sequence comprised of a query and two texts, comprising
the input template:
[[CLS], q, [SEP], di
, [SEP], dj , [SEP]], (31)
Similar to the implementation of monoBERT, each input token in q, di
, and dj is represented by the
element-wise sum of the token, segment type, and position embeddings. In the duoBERT model,
there are three segment types: type A for q tokens, and types B and C for the di and dj tokens,
respectively. Type embeddings A and B are learned during pretraining, but the new type segment
C embedding is learned from scratch during fine-tuning. Due to the length limitations of BERT,
the query, candidates di and dj are truncated to 62, 223, and 223 tokens, respectively, so that the
entire sequence has at most 512 tokens when concatenated with the [CLS] token and the three [SEP]
90
MS MARCO Passage
Development Test
Method MRR@10 MRR@10
(1) Anserini BM25 = Table 5, row (3a) 0.187 0.190
(2) + monoBERT (k0 = 1000) = Table 5, row (3b) 0.372 0.365
+ monoBERT (k0 = 1000)
(3a) + duoBERTMAX (k1 = 50) 0.326 -
(3b) + duoBERTMIN (k1 = 50) 0.379 -
(3c) + duoBERTSUM (k1 = 50) 0.382 0.370
(3d) + duoBERTBINARY (k1 = 50) 0.383 -
(4a) + monoBERT + TCP 0.379 -
(4b) + monoBERT + duoBERTSUM + TCP 0.390 0.379
Table 21: The effectiveness of the monoBERT/duoBERT pipeline on the MS MARCO passage
ranking test collection. TCP refers to target corpus pretraining.
tokens. Using the above length limits, for the MS MARCO passage ranking test collection, Nogueira
et al. [2019a] did not have to truncate any of the queries and less than 1% of the candidate texts were
truncated. Similar to monoBERT, the final representation of the [CLS] token is used as input to a
fully-connected layer to obtain the probability pi,j . For k candidates, |k| × (|k| − 1) probabilities are
computed.
The model is trained end-to-end with the following loss:
Lduo = −
X
i∈Jpos,j∈Jneg
log(pi,j ) −
X
i∈Jneg,j∈Jpos
log(1 − pi,j ), (32)
Note that in the equation above, candidates di and dj are never both relevant or not relevant. Since
this loss function considers pairs of candidate texts, it can be characterized as belonging to the family
of pairwise learning-to-rank methods [Liu, 2009, Li, 2011] (but see additional discussions below).
For details about the training procedure, including hyperparameter settings, we refer the reader to the
original paper.
At inference time, the pairwise scores pi,j are aggregated so that each document receives a single
score si
. Nogueira et al. [2019a] investigated a number of different aggregation methods:
MAX : si = max
j∈Ji
pi,j , (33)
MIN : si = min
j∈Ji
pi,j , (34)
SUM : si =
X
j∈Ji
pi,j , (35)
BINARY : si =
X
j∈Ji
1pi,j>0.5. (36)
where Ji = {0 ≤ j < |D|, j 6= i} and m is the number of samples drawn without replacement
from the set Ji
. The SUM method measures the pairwise agreement that candidate di
is more
relevant than the rest of the candidates {dj}j6=i
. The BINARY method is inspired by the Condorcet
method [Montague and Aslam, 2002], which serves as a strong aggregation baseline [Cormack et al.,
2009]. The MIN (MAX) method measures the relevance of di only against its strongest (weakest)
“competitor”. The final ranked list (for evaluation) is obtained by reranking the candidates according
to their scores si
.
Before presenting experimental results, it is worthwhile to clarify a possible point of confusion. In
“traditional” (i.e., pre-neural) learning to rank, “pairwise” and “pointwise” refer to the form of the
loss, not the form of the inference mechanism. For example, RankNet [Burges et al., 2005] is trained
in a pairwise manner (i.e., loss is computed with respect to pairs of texts), but inference (i.e., at query
time) is still performed on individual texts. In duoBERT, both training and inference are performed
on pairs of texts in a cross-encoder design where all three inputs (the query and the two texts to be
compared) are “packed” into the input template fed to BERT.
91
Results on the MS MARCO passage ranking test collection are shown in Table 21, organized in
the same manner as Table 5; the experimental conditions are directly comparable. Row (1) reports
the effectiveness of Anserini’s initial candidates using BM25 scoring. In row (2), BM25 results
reranked with monoBERT using BERTLarge (k0 = 1000) are shown, which is exactly the same as row
(3b) in Table 5. Rows (3a)–(3d) report results from reranking the top 50 results from the output of
monoBERT (i.e., k1 = 50) using the various aggregation techniques presented above. Effectiveness
in terms of the official metric MRR@10 is reported on the development set for all aggregation
methods (i.e., duoBERT using BERTLarge), but Nogueira et al. [2019a] only submitted results from
the SUM condition for evaluation on the test set. We see that MAX aggregation is not as effective as
the other three techniques, but the difference between MIN, SUM, and BINARY are all quite small.
In the same paper, Nogueira et al. [2019a] also introduced the target corpus pretraining (TCP)
technique presented in Section 3.2.4. Rows (4a) and (4b) in Table 5 report results of applying TCP
with monoBERT and monoBERT + duoBERT. Here, we see that the gains are relatively modest, but
as discussed earlier, unsupervised pretraining can be viewed as a source of “free” improvements in
that these gains do not require any additional labeled data.
In all the experimental conditions above, duoBERT considers the top 50 candidates from monoBERT
(i.e., k1 = 50), and thus requires an additional 50 × 49 BERT inferences to compute the final ranking
(the time required for aggregation is negligible). For simplicity, Nogueira et al. [2019a] used the total
number of BERT inferences as a proxy to capture overall query latency. Based on this metric, since
monoBERT with k0 = 1000 requires 1000 BERT inferences, a monoBERT + duoBERT pipeline
represents a 3.5× increase in latency. While it is true that each pair of texts in duoBERT takes longer
to process than a single text in monoBERT due to the longer input length, this detail does not change
the argument qualitatively (although the actual tradeoff point in our analysis below might change if
we were to measure wall-clock latency; there are GPU batching effects to consider as well).
From this perspective, duoBERT does not seem compelling because the gain from monoBERT +
duoBERT vs. monoBERT alone is far more modest than the gain from monoBERT vs. BM25 (at the
k0 and k1 settings shown in Table 21). However, the more pertinent question is as follows: Given
a fixed budget for neural inference, how should we allocate resources between monoBERT and
duoBERT? In this scenario, the pairwise reranking approach becomes much more compelling. We
demonstrate this below:
In general, a two-stage configuration provides a richer design space for selecting a desirable operating
point to balance effectiveness and efficiency under a certain computational budget. With a single
reranking stage (monoBERT), the only choice is to vary the k0 parameter, but with two rerankers,
it is possible to simultaneously tune k0 and k1. These tradeoff curves are shown in Figure 15, with
duoBERTSUM for aggregation. This experiment was not reported in Nogueira et al. [2019a] and here
we present results that have not yet been published anywhere else. In the plot, the gray line shows
effectiveness with different values of k0 for monoBERT in a single-stage setup (this is the same as the
curve in Figure 9, just across a narrower range). The other lines show settings of k1 ∈ {10, 30, 50},
and with each k1 setting, points in each tradeoff curve represent k0 = {50, 100, 200, 500, 1000}. In
the two-stage configuration, the number of inferences per query is calculated as k0 + k1(k1 − 1).
Thus, the x axis is a reasonable proxy of the total computational budget.
Hypothetical vertical lines intersecting with each curve denote the best effectiveness that can be
achieved with a particular computational budget: these results suggest that if a system designer were
willing to expend more than couple of hundred BERT inferences, then a two-stage configuration
is more effective overall. That is, rather than simply increasing the reranking depth of single-stage
monoBERT, it is better to reallocate some of the computational budget to a pairwise approach that
examines pairs of candidate texts. The Pareto frontier in the effectiveness/efficiency tradeoff space
is shown in Figure 15 as the dotted black line. For each point on the frontier, there exists no other
setting that achieves both higher MRR@10 while requiring fewer inferences. This frontier serves as
a guide for system designers in choosing desirable operating points in the effectiveness/efficiency
design space.
Takeaway Lessons. Multi-stage ranking architectures represent a straightforward generalization of
the retrieve-and-rerank approach adopted in monoBERT. Introducing multiple rerankers in a pipeline
greatly expands the possible operating points of an end-to-end system in the effectiveness/efficiency
tradeoff space, potentially leading to settings that are both better and faster than what can be achieved
with a single-stage reranker. On potential downside, however, is that multi-stage pipelines introduce
92
50 100 200 500 1,000 2,000 3,450 10,000
0.320
0.340
0.360
0.380
0.400
BERT Inferences/Query
MRR@10
duoBERT, k1 = 10
duoBERT, k1 = 30
duoBERT, k1 = 50
monoBERT
Pareto frontier
Effectiveness/Efficiency Tradeoffs on MS MARCO Passage
Figure 15: Effectiveness/efficiency tradeoff curves for different monoBERT and monoBERT +
duoBERTSUM settings on the development set of the MS MARCO passage ranking test collection.
Efficiency is measured in the number of BERT inferences per query. For monoBERT, the tradeoff
curve plots different values of k0 (the same as in Figure 9). For monoBERT + duoBERTSUM, each
curve plots a different k1, and points on each curve correspond to k0 = {50, 100, 200, 500, 1000};
the number of inferences per query is calculated as k0 + k1(k1 − 1). The Pareto frontier is shown as
the dotted black line.
additional “tuning knobs” that need to be properly adjusted to achieve a desired tradeoff. In the
monoBERT/duoBERT design, these parameter settings (k0, k1) are difficult to learn as the pipeline is
not differentiable end-to-end. Thus, the impact of different parameter settings must be empirically
determined from a test collection.
3.4.2 Reranking Lists of Texts
Given a query, the duoBERT model described in the previous section estimates the relevance of a
text relative to another text, where both texts are directly fed into BERT for consideration in a single
inference pass. This pairwise approach can be more effective than pointwise rerankers based on
relevance classification such as monoBERT because the pairwise approach allows the reranker to “see”
what else is in the set of candidates. One natural extension of the pairwise approach is the “listwise”
approach, in which the relevance of a text is estimated jointly with multiple other candidates. Here
we describe two proposed listwise reranking methods.
Before proceeding, two important caveats: First, the labels “pairwise” and “listwise” here explicitly
refer to the form of the input template for inference (which necessitates, naturally, modifications to the
loss function during model training). Thus, our usage of these terms diverges from “traditional” (i.e.,
pre-neural) learning to rank, which describes only the form of the loss; see, for example, ListNet [Cao
et al., 2007]. We do not cover these listwise learning-to-rank methods here and instead refer the
reader to existing surveys [Liu, 2009, Li, 2011]. Second, while listwise approaches may not have
been proposed explicitly in the context of multi-stage ranking architectures, they are a natural fit for
the same reasons as duoBERT. Given the length limitations of many neural models and the blow-up
in terms of input permutations that need to be considered, a stage-wise reranking approach makes a
lot of sense.
We begin with Ai et al. [2019], who proposed a listwise reranking approach based on learning what
they called a groupwise multivariate scoring function. In their approach, each text di
is represented by
a hand-crafted feature vector xi
, which can include signals designed to capture query–text interactions.
The concatenation of n such feature vectors is fed to a fully-connected neural network that outputs n
relevance scores, one for each text. Depending on the query, the number of candidate texts k can be
93
quite large (e.g., k = 1000). Consequently, it is not practical to feed all candidates to the model at
once since the input sequence would become prohibitively long, thus making the model difficult to
effectively train. Instead, the authors proposed to compute size-n permutations of k candidate texts
and independently feed each group of n feature vectors to the model. At inference time, the final
score of each text is the sum of the scores in each group it was part of.
The model is trained with the following cross-entropy loss:
L = −
X
k
i=1
wiyi
log pi
, (37)
where wi
is the Inverse Propensity Weight [Joachims et al., 2017, Liu, 2009] of the i-th results and
yi = 1 if the text is relevant and zero otherwise. The probability pi
is obtained by applying a softmax
to all logits t of the candidate texts:
pi =
e
ti
Pk
j=1 e
tj
(38)
Results on publicly available datasets are encouraging, but the effectiveness of this approach is not
clearly superior to pointwise or pairwise approaches. The authors identified possible improvements,
including the design of the feedforward network and a better way to organize model input than a
simple concatenation of features from the candidate texts.
Instead of feeding hand-crafted features to a fully-connected neural network as in Ai et al. [2019],
Zhang et al. [2020f] proposed to directly feed raw candidate texts into pretrained transformers. Due
to model length limitations, however, candidate texts are truncated until they fit into a 512 token
sequence. The resulting listwise reranker showed small improvements over its pairwise counterpart
on two ranking datasets: the first is a non-public dataset in Chinese, while the second is a modified
version of the MS MARCO passage ranking test collection. Unfortunately, modifications to the latter
render the results not comparable to other papers, so we lack meaningful points of comparison.
Takeaway Lessons. Listwise rerankers represent a natural extension of pairwise rerankers and are
intuitively appealing because relevance scores can be estimated jointly. However, the necessity of
feeding multiple candidate texts into a neural model in each inference pass leads to potentially long
input sequences and thus presents a major technical challenge, for all the reasons already discussed
throughout this section. For the problem of label prediction in a fact verification setting, Pradeep et al.
[2021a] demonstrated the effectiveness of a listwise approach in which multiple claims are presented
to a pretrained transformer model in a single input template. In this case, the candidate sentences
are shorter than typical texts to be ranked, and thus the work highlights the potential of the listwise
approach, as long as we can overcome the model length limitations. This remains an open problem
in the general case, and despite encouraging results, in our opinion, ranking models that consider
lists of candidates have not been conclusively demonstrated to be more effective than models that
consider pairs of candidates.
3.4.3 Efficient Multi-Stage Rerankers: Cascade Transformers
Multi-stage ranking pipelines exploit faster (and possibly less effective) models in earlier stages
to discard likely non-relevant documents so there are fewer candidates under consideration by
more expensive models in later stages. In the case of the mono/duoBERT architecture described
above, the primary goal was to make a more inference-heavy model (i.e., duoBERT) more practical.
Indeed, experimental results in the previous section offer a guide for how to optimally allocate
resources to monoBERT and duoBERT inference given a computational budget. In other words, the
goal is to improve the quality of a single-stage monoBERT design while maintaining acceptable
effectiveness/efficiency tradeoffs.
However, the mono/duoBERT architecture isn’t particularly useful if we desire a system that is even
faster (but perhaps less effective) than the baseline (single-stage) monoBERT design. In this case,
one possibility is to use a standard telescoping pipeline that potentially include pre-BERT neural
ranking methods, as suggested by Matsubara et al. [2020]. Given monoBERT as a starting point,
another obvious solution is to leverage the large body of research on model pruning and compression,
which is not specific to text ranking or even natural language processing. In Section 3.5, we cover
knowledge distillation and other threads of research in this broad space. Here, we discuss a solution
that shares similar motivations, but is clearly inspired by multi-stage ranking architectures.
94
Soldaini and Moschitti [2020] began with the observation that a model like monoBERT is already
like a multi-stage ranking architecture if we consider each layer of the transformer encoder as a
separate ranking stage. In the monoBERT design, inference is applied to all input texts (for example,
k0 = 1000). This seems like a “waste”, and we could accelerate inference if the model could
somehow predict that a particular text was not likely to be relevant partway through the layers.
Therefore, a sketch of the solution might look like the following: start with a pool of candidate texts,
apply inference on the entire batch using the first few layers, discard the least promising candidates,
continue inference with the next few layers, discard the least promising candidates, and so on, until
the end, when only the most promising candidates have made it all the way through the layers. With
cascade transformers, Soldaini and Moschitti [2020] did exactly this.
More formally, with cascade transformers, intermediate classification decision points (which we’ll call
“early exits” for reasons that will become clear in a bit) are built in at layers j = λ0 +λ1 ·(i−1), ∀i ∈
{1, 2, . . .}, where λ0, λ1 ∈ N are hyperparameters. Specifically, Soldaini and Moschitti [2020] build
on the base version of RoBERTa [Liu et al., 2019c], which has 12 layers; they used a setting of λ0 = 4
and λ1 = 2, which yields five rerankers, with decision points at layers 4, 6, 8, 10, and 12.107 The
rationale for skipping the first λ0 layers is that relevance classification effectiveness is too poor for the
model to be useful; this observation is consistent with findings across many NLP tasks [Houlsby et al.,
2019, Lee et al., 2019a, Xin et al., 2020]. The [CLS] vector representation at each of the j layers
(i.e., each of the cascade rerankers) is then fed to a fully-connected classification layer that computes
the probability of relevance for the candidate text; this remains a pointwise relevance classification
design. At inference time, at each of the j layers, the model will score P candidate documents and
retain only the top (1 − α) · P scoring candidates, where α ∈ [0 . . . 1] is a hyperparameter, typically
between 0.3 and 0.5. That is, α · P candidates are discarded at each stage.
In practice, neural network inference is typically conducted on GPUs in batches. Soldaini and
Moschitti [2020] worked through a concrete example of how these settings play out in practice:
Consider a setting of α = 0.3 with a batch size b = 128. With the five cascade reranker design
described above, after layer 4, the size of the batch is reduced to 90, i.e., b0.3 · 128c = 38 candidates
are discarded after the first classifier. At layer 6, after the second classification, 27 additional
candidates are discarded, with only 63 remaining. At the end, only 31 candidates are left. Thus,
cascade transformers have the effect of reducing the average batch size, which increases throughput on
GPUs compared to a monolithic design, where inference must be applied to all input instances. In the
example above, suppose that based on a particular hardware configuration we can process a maximum
batch size of 84 using a monolithic model. With cascade transformers, we can instead process batches
of 128 instances within the same memory constraints, since (4·128+2·90+2·63+2·44+2·28)/12 =
80.2 < 84. This represents a throughput increase of 52%.
The cascade transformer architecture requires training all the classifiers at each of the individual
rerankers (i.e., early exit points). The authors described a procedure wherein for each training batch,
one of the rerankers is sampled (including the final output reranker): its loss against the target labels
is computed and back-propagated through the entire model, down to the embedding layers. This
simple uniform sampling strategy was found to be more effective than alternative techniques such as
round-robin selection and biasing the early rerankers.
Soldaini and Moschitti [2020] evaluated their cascade transformers on the answer selection task in
question answering, where the goal is to select from a pool of candidate sentences the ones that
contain the answer to a given natural language question. This is essentially a text ranking task on
sentences, where the ranked output provides the input to downstream modules that identify answer
spans. The authors reported results on multiple answer selection datasets, but here we focus on two:
Answer Sentence Natural Questions (ASNQ) [Garg et al., 2020], which is a large dataset constructed
by extracting sentence candidates from the Google Natural Question (NQ) dataset [Kwiatkowski
et al., 2019], and General Purpose Dataset (GPD), which is a proprietary dataset comprising questions
submitted to Amazon Alexa with answers annotated by humans. In both cases, the datasets include
the candidates to be reranked (i.e., first-stage retrieval is fixed and part of the test collection itself).
107In truth, Soldaini and Moschitti [2020] describe their architecture in terms of reranking with multiple
transformer stacks, e.g., first with a 4-layer transformer, then a 6-layer transformers, then a 8-layer transformer,
etc. However, since in their design, all common transformer layers have shared weights, it is entirely equivalent
to a monolithic 12-layer transformer with five intermediate classification decision points (or early exits).
We find this explanation more intuitive and better aligned with the terminology used by other researchers.
Nevertheless, we retain the authors’ original description of calling this design a five-reranker cascade.
95
ASNQ GDP
Method MAP nDCG@10 MRR MAP nDCG@10 MRR Cost Reduction
(1) TANDABASE 0.655 0.651 0.647 0.580 0.722 0.768
(2a) CT (α = 0.0) 0.663 0.661 0.654 0.578 0.719 0.769
(2b) CT (α = 0.3) 0.653 0.653 0.653 0.557 0.698 0.751 −37%
(2c) CT (α = 0.4) 0.648 0.650 0.648 0.528 0.686 0.743 −45%
(2d) CT (α = 0.5) 0.641 0.650 0.645 0.502 0.661 0.729 −51%
Table 22: The effectiveness and cost reduction of cascade transformers on the ASNQ and GPD
datasets. The parameter α controls the proportion of candidates discarded at each pipeline stage.
Results copied from the authors’ paper are shown in Table 22. The baseline is TANDABASE [Garg
et al., 2020], which is monoBERT with a multi-stage fine-tuning procedure that uses multiple
datasets—what we introduced as pre–fine-tuning in Section 3.2.4. For each dataset, effectiveness
results in terms of standard metrics are shown; the final column denotes an analytically computed cost
reduction per batch. The cascade transformer architecture is denoted CT, in row group (2). In row (2a),
with α = 0.0, all candidate sentences are scored using all layers of the model (i.e., no candidates are
discarded). This model performs slightly better than the baseline, and these gains can be attributed to
the training of the intermediate classification layers, since the rest of the CT architecture is exactly the
same as the TANDA baseline. Rows (2b), (2c), and (2d) report effectiveness with different α settings.
On the ASNQ dataset, CT with α = 0.5 is able to decrease inference cost per batch by around half
with a small decrease in effectiveness. On the GPD dataset, inference cost can be reduced by 37%
(α = 0.3) with a similarly modest decrease in effectiveness. These experiments clearly demonstrated
that cascade transformers provide a way for system designers to control effectiveness/efficiency
tradeoffs in multi-stage ranking architectures. As with the mono/duoBERT design, the actual
operating point depends on many considerations, but the main takeaway is that these designs provide
the knobs for system designers to express their desired tradeoffs.
At the intersection of model design and the practical realities of GPU-based inference, Soldaini and
Moschitti [2020] discussed a point that is worth repeating here. In their design, a fixed α is crucial to
obtaining the performance gains observed, although in theory one could devise other approaches to
pruning. For example, candidates could be discarded based on a score threshold (that is, discard all
candidates with scores below a given threshold). Alternatively, it might even be possible to separately
learn a lightweight classifier that dynamically decides the candidates to discard. The challenge with
these alternatives, however, is that it becomes difficult to determine batch sizes a priori, and therefore
to efficiently exploit GPU resources (which depend critically on regular computations).
It is worth noting that cascade transformers were designed to rank candidate sentences in a question
answering task, and cannot be directly applied to document ranking, even with relatively simple
architectures like Birch and BERT–MaxP. There is the practical problem of packing sentences (from
Birch) or passages (from BERT–MaxP) into batches for GPU processing. As we can see from the
discussion above, cascade transformers derive their throughput gains from the ability to more densely
pack instances into the same batch for efficient inference. However, for document ranking, it is
important to distinguish between scores of segments within documents as well as across documents.
The simple filtering decision in terms of α cannot preserve both relationships at the same time if
segments from multiple documents are mixed together, but since documents have variable numbers
of sentences or passages, strictly segregating batches by document will reduce the regularity of the
computations and hence the overall efficiency. To our knowledge, these issues have not been tackled,
and cascade transformers have not been extended for ranking texts that are longer than BERT’s 512
token length limit. Such extensions would be interesting future work.
To gain a better understanding of cascade transformers, it is helpful to situate this work within the
broader context of other research in NLP. The insight that not all layers of BERT are necessary for
effectively performing a task (e.g., classification) was shared independently and contemporaneously
by a number of different research teams. While Soldaini and Moschitti [2020] operationalized this
idea for text ranking in cascade transformers, other researchers applied the same intuition for other
natural language processing tasks. For example, DeeBERT [Xin et al., 2020] proposed building early
exit “off ramps” in BERT to accelerate inference for test instances based on an entropy threshold;
two additional papers, Schwartz et al. [2020] and Liu et al. [2020] implemented the same idea with
96
only minor difference in details. Quite amazingly, these three papers, along with the work of Soldaini
and Moschitti, were all published at the same conference, ACL 2020!
Although this remarkable coincidence suggests early exit was an idea “whose time had come”, it is
important to recognize that, in truth, the idea had been around for a while—just not in the modern
context of neural networks. Over a decade ago, Cambazoglu et al. [2010] proposed early exits in
additive ensembles for ranking, but in the context gradient-boosted decision trees, which exhibit the
same regular, repeating structure (at the “block” level) as transformer layers. Of course, BERT and
pretrained transformers offer a “fresh take” that opens up new design choices, but many of the lessons
and ideas from (much older) previous work remain applicable.
A final concluding thought before moving on: the above discussion suggests that the distinction
between monolithic ranking models and multi-stage ranking is not clear cut. For example, is the
cascade transformer a multi-stage ranking pipeline or a monolithic ranker with early exits? Both
seem apt descriptions, depending on one’s perspective. However, the mono/duoBERT combination
can only be accurately described as multi-stage ranking, since the two rerankers are quite different.
Perhaps the distinction lies in the “end-to-end” differentiability of the model (and hence how it is
trained)? But differentiability stops at the initial candidate generation stage since all the architectures
discussed in this section still rely on keyword search. Learned dense representations, which we cover
in Section 5, can be used for single-stage direct ranking, but can also replace keyword search for
candidate generation, further muddling these distinctions. Indeed, the relationship between these
various architectures remains an open question and the focus of much ongoing research activity,
which we discuss in Section 6.
Takeaway Lessons. Cascade transformers represent another example of a multi-stage ranking
pipeline. Compared to the mono/duoBERT design, the approach is very different, which illustrates
the versatility of the overall architecture. Researchers have only begun to explore this vast and
interesting design space, and we expect more interesting future work to emerge.
3.5 Beyond BERT
All of the ranking models discussed so far in this section are still primarily built around BERT or a
simple BERT variant, even if they incorporate other architectural components, such as interaction
matrices in CEDR (see Section 3.3.3) or another stack of transformers in PARADE (see Section 3.3.4).
There are, however, many attempts to move beyond BERT to explore other transformer models,
which is the focus of this section.
At a high level, efforts to improve ranking models can be characterized as attempts to make ranking
better, attempts to make ranking faster, attempts to accomplish both, or attempts to find other operating
points in the effectiveness/efficiency tradeoff space. Improved ranking effectiveness is, of course,
a perpetual quest and needs no elaboration. Attempts to make text ranking models faster can be
motivated by many sources. Here, we present results by Hofstätter and Hanbury [2019], shown in
Figure 16. The plot captures the effectiveness vs. query latency (millisecond per query) of different
neural ranking models on the development set of the MS MARCO passage ranking test collection.
Note that the x axis is in log scale! Pre-BERT models can be deployed for real-world applications
with minimal modifications, but it is clear that naïve production deployments of BERT are impractical
or hugely expensive in terms of required hardware resources. In other words, BERT is good but slow:
Can we trade off a bit of quality for better performance?
This section is organized roughly in increasing “distance from BERT”. Admittedly, what’s BERT and
what’s “beyond BERT” is somewhat an arbitrary distinction. These classifications represent primarily
our judgment for expository purposes and shouldn’t be taken as any sort of definitive categorization.
Building on our previous discussion of simple BERT variants in Section 3.2.2, we begin by discussing
efforts to distill BERT into smaller models in Section 3.5.1. Distilled models are similar to the simple
BERT variants in that they can easily be “swapped in” as a replacement for BERT “classic”. Attempts
to design transformer-based architectures specifically for text ranking from the ground up—the
Transformer Kernel (TK) and Conformer Kernel (CK) models—are discussed next in Section 3.5.2.
Finally, we turn our attention to ranking with pretrained sequence-to-sequence transformers in
Section 3.5.3 and Section 3.5.4, which are very different from the transformer encoder design of
BERT and BERT variants.
97
1 10 100 1,000 2,000
0.2
0.25
0.3
0.35
0.4
Query Latency (milliseconds)
MRR@10
BERT
KNRM
MatchP.
PACRR
DUET
C-KNRM
Effectiveness/Efficiency Tradeoffs on MS MARCO Passage
Figure 16: Effectiveness/efficiency tradeoffs comparing BERT with pre-BERT models (using FastText
embeddings) on the development set of the MS MARCO passage ranking test collection, taken
from Hofstätter and Hanbury [2019]. Note that the x-axis is in log scale.
3.5.1 Knowledge Distillation
Knowledge distillation refers to a general set of techniques where a smaller student model learns to
mimic the behavior of a larger teacher model [Ba and Caruana, 2014, Hinton et al., 2015]. The goal
is for the student model to achieve comparable effectiveness on a particular task but more efficiently
(e.g., lower inference latencies, fewer model parameters, etc.). While knowledge distillation is model
agnostic and researchers have explored this approach for many years, to our knowledge Tang et al.
[2019] were the first to apply the idea to BERT, demonstrating knowledge transfer between BERT
and much simpler models such as single-layer BiLSTMs. A much simpler RNN-based student model,
of course, cannot hope to achieve the same level of effectiveness as BERT, but if the degradation is
acceptable, inference can be accelerated by an order of magnitude or more. These ideas have been
extended by many others [Sun et al., 2019a, Liu et al., 2019b, Sanh et al., 2019, Hofstätter et al.,
2020], with a range of different student models, including smaller versions of BERT.
Unsurprisingly, knowledge distillation has been applied to text ranking. Researchers have investigated
whether the efficiency of BERT can be improved by distilling a larger trained (BERT) model into a
smaller (but still BERT-based) one [Gao et al., 2020c, Li et al., 2020a, Chen et al., 2021, Zhang et al.,
2020f]. To encourage the student model to mimic the behavior of the teacher model, one common
distillation objective is the mean squared error between the student’s and teacher’s logits [Tang et al.,
2019, Tahami et al., 2020]. The student model can be fine-tuned with the linear combination of the
student model’s cross-entropy loss and the distillation objective as the overall loss:
L = α · LCE + (1 − α) · ||r
t − r
s
||2
(39)
where LCE is the cross-entropy loss, r
t
and r
s
are the logits from the teacher and student models,
respectively, and α is a hyperparameter. As another approach, TinyBERT proposed a distillation
objective that additionally considers the mean squared error between the two models’ embedding
layers, transformer hidden states, and transformer attention matrices [Jiao et al., 2019]. In the
context of text ranking, Chen et al. [2021] reported that this more complicated objective can improve
effectiveness.
Gao et al. [2020c] observed that distillation can be applied to both a BERT model that has already been
fine-tuned for relevance classification (“ranker distillation”) and to pretrained but not yet fine-tuned
BERT itself (“LM distillation”). Concretely, this yields three possibilities:
1. apply distillation so that a (randomly initialized) student model learns to directly mimic an
already fine-tuned teacher model using the distillation objective above (“ranker distillation”),
2. apply LM distillation into a student model followed by fine-tuning the student model for the
relevance classification task (“LM distillation + fine-tuning”), or
98
MS MARCO Passage TREC 2019 DL Passage Latency
Method Layers MRR@10 MRR nDCG@10 (ms / doc)
(1) monoBERTBase 12 0.353 0.935 0.703 2.97
(2a) Ranker distillation 6 0.338 0.927 0.686 1.50
(2b) LM Distillation + Fine-Tuning 6 0.356 0.965 0.719 1.50
(2c) LM + Ranker Distillation 6 0.360 0.952 0.692 1.50
(3a) Ranker distillation 4 0.329 0.935 0.669 0.33
(3b) LM Distillation + Fine-Tuning 4 0.332 0.950 0.681 0.33
(3c) LM + Ranker Distillation 4 0.350 0.929 0.683 0.33
Table 23: The effectiveness of distilled monoBERT variants on the development set of the MS
MARCO passage ranking test collection and the TREC 2019 Deep Learning Track passage ranking
test collection. Inference times were measured on an NVIDIA RTX 2080 Ti GPU.
3. apply LM distillation followed by ranker distillation (“LM + ranker distillation”).
Operationally, the third approach is equivalent to the first approach, except with a better initialization
of the student model. The relative effectiveness of these three approaches is an empirical question.
To answer this question, Gao et al. [2020c] used the TinyBERT distillation objective to distill a
BERTBase model into smaller transformers: a six-layer model with a hidden dimension of 768 or a
four-layer model with a hidden dimension of 312. Both the student and teacher models are designed
as relevance classifiers (i.e., monoBERT).
Evaluation on the development set of the MS MARCO passage ranking test collection and TREC
2019 Deep Learning Track passage ranking test collection are shown in Table 23, with results copied
Gao et al. [2020c]. The six-layer and four-layer student models are shown in row groups (2) and (3),
respectively, and the monoBERTBase teacher model is shown in row (1). The (a), (b), (c) rows of
row groups (2) and (3) correspond to the three approaches presented above. The final column shows
inference latency measured on an NVIDIA RTX 2080Ti GPU.
We see that ranker distillation alone performs the worst; the authors reported a statistically significant
decrease in effectiveness from the teacher model across all metrics and both test collections. Both
LM distillation followed by fine-tuning and LM distillation followed by ranker distillation led to
student models comparable to the teacher in effectiveness. We see that in terms of MRR, “LM +
ranker distillation” outperforms “LM distillation + fine-tuning” on the MS MARCO passage ranking
test collection, but the other way around for the TREC 2019 Deep Learning Track document ranking
test collection; note though, that the first has far more queries than the second and thus might provide
a more stable characterization of effectiveness. Overall, the six-layer distilled model can perform
slightly better than the teacher model while being twice as fast,108 whereas the four-layer distilled
model gains a 9× speedup in exchange for a small decrease in effectiveness.
As another example of explorations in knowledge distillation, Li et al. [2020a] investigated how well
their PARADE model performs when distilled into student models that range in size. Specifically,
they examined two approaches:
1. train the full PARADE model using a smaller BERT variant distilled from BERTLarge by Turc
et al. [2019] in place of BERTBase, and
2. apply ranker distillation with the MSE distillation objective, where PARADE trained with
BERTBase is used as the teacher model, and the student model is PARADE with a smaller BERT
variant (i.e., one of the pre-distilled models provided by Turc et al. [2019]).
Experimental results for Robust04 title queries are shown in Table 24, with figures copied from Li
et al. [2020a]. Row (1) presents the effectiveness of the teacher model, which is the same model
shown in row (6f) in Table 18. However, in order to reduce the computational requirements, the
experimental setup here differs from that used in Table 18 in two ways: fewer terms per document
are considered (1650 rather than 3250) and fewer documents are being reranked (100 rather than
1000); thus, the starting effectiveness is lower. Rows (2–8) present the distillation results: The
108We suspect that the slightly higher effectiveness is due to a regularization effect, but this finding needs more
detcailed investigation.
99
